<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-learning on Isaac Changhau</title>
    <link>https://isaacchanghau.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine-learning on Isaac Changhau</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017-2018</copyright>
    <lastBuildDate>Fri, 27 Jul 2018 14:20:36 +0800</lastBuildDate>
    
	<atom:link href="https://isaacchanghau.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Variational Autoencoders Explained</title>
      <link>https://isaacchanghau.github.io/post/vae_explained/</link>
      <pubDate>Fri, 27 Jul 2018 14:20:36 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/vae_explained/</guid>
      <description>Note: This blog article is borrowed from kevin frans blog Â· Variational Autoencoders Explained.
In my previous post about generative adversarial networks, I went over a simple method to training a network that could generate realistic-looking images.
However, there were a couple of downsides to using a plain GAN.
First, the images are generated off some arbitrary noise. If you wanted to generate a picture with specific features, there&amp;rsquo;s no way of determining which initial noise values would produce that picture, other than searching over the entire distribution.</description>
    </item>
    
    <item>
      <title>Autoencoder and Sparsity</title>
      <link>https://isaacchanghau.github.io/post/autoencoder_and_sparsity/</link>
      <pubDate>Sat, 19 Aug 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/autoencoder_and_sparsity/</guid>
      <description>The article is excerpted from Andrew Ng&amp;rsquo;s CS294A Lecture notes: Sparse Autoencoder with some personal understanding. Before going through this article, you would better get some information from the Backpropagation Algorithm.
Suppose we have only unlabeled training examples set $\{x^{(1)},x^{(2)},x^{(3)},\dots\}$, where $x^{(i)}\in\mathbb{R}^{n}$. An autoencoder neural network is an unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. i.e., it uses $y^{(i)}=x^{(i)}$. Below is an autoencoder: The autoencoder tries to learn a function $h_{W,b}(x)\approx x$.</description>
    </item>
    
    <item>
      <title>Backpropagation in Neural Networks</title>
      <link>https://isaacchanghau.github.io/post/backpropagation/</link>
      <pubDate>Thu, 17 Aug 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/backpropagation/</guid>
      <description>The article is excerpted from Andrew Ng&amp;rsquo;s CS294A Lecture notes: Sparse Autoencoder, then I add some personal understanding.
In this article, we will let $n_{l}$ denote the number of layers in our network, label $l$ as $L_{l}$, so layer $L_{1}$ is the input layer, and layer $L_{n_{l}}$ the output layer. Neural network has parameters $(W,b)=(W^{(1)},b^{(1)},\dots,W^{(n_{l}-1)},b^{(n_{l}-1)})$, where we write $W_{ij}^{(l)}$ to denote the parameter (or weight) associated with the connection between unit $j$ in layer $l$ and unit $i$ in layer $l+1$.</description>
    </item>
    
    <item>
      <title>Plain Stock Price Prediction via LSTM</title>
      <link>https://isaacchanghau.github.io/post/stock_price_predict/</link>
      <pubDate>Wed, 26 Jul 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/stock_price_predict/</guid>
      <description>This is a practice of using LSTM to do the one day ahead prediction of the stock close price. The dataset I used here is the New York Stock Exchange from Kaggle, which consists of following files:
 prices.csv: raw, as-is daily prices. Most of data spans from 2010 to the end 2016, for companies new on stock market date range is shorter. There have been approx. 140 stock splits in that time, this set doesn&amp;rsquo;t account for that.</description>
    </item>
    
    <item>
      <title>LSTM and GRU -- Formula Summary</title>
      <link>https://isaacchanghau.github.io/post/lstm-gru-formula/</link>
      <pubDate>Sat, 22 Jul 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/lstm-gru-formula/</guid>
      <description>Introduction Long Short-Term Memory (LSTM) unit and Gated Recurrent Unit (GRU) RNNs are among the most widely used models in Deep Learning for NLP today. Both LSTM (1997) and GRU (2014) are designed to combat the vanishing gradient problem prevents standard RNNs from learning long-term dependencies through gating mechanism.
Note that, this article heavily rely on the following to articles, Understanding LSTM Networks and Recurrent Neural Network Tutorial, I summary the formula definition and explanation from them to enhance my understanding of LSTM and GRU as well as their similarity and difference.</description>
    </item>
    
    <item>
      <title>House Prices Advanced Regression Techniques -- Modeling and Prediction</title>
      <link>https://isaacchanghau.github.io/post/house_price_modeling/</link>
      <pubDate>Mon, 10 Jul 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/house_price_modeling/</guid>
      <description>After cleaning and transforming processes in House Prices Advanced Regression Techniques &amp;ndash; Data Analysis. Here we continue to build machine learning model to train the dataset and make a prediction based on the trained model. We use Elastic Net and Gradient Boosting models to train the dataset and make predictions separately, then average the results of the two models to generate the final output.
Elastic Net Regression In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.</description>
    </item>
    
    <item>
      <title>House Prices Advanced Regression Techniques -- Data Analysis</title>
      <link>https://isaacchanghau.github.io/post/house_price_data_analysis/</link>
      <pubDate>Sat, 08 Jul 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/house_price_data_analysis/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&#34; target=&#34;_blank&#34;&gt;House Prices: Advanced Regression Techniques&lt;/a&gt; is a &lt;a href=&#34;https://www.kaggle.com/&#34; target=&#34;_blank&#34;&gt;kaggle&lt;/a&gt; competition to predict the house prices, which aims to practice feature engineering, RFs, and gradient boosting. After exploring and referring others&amp;rsquo; methods, I decide to do it by myself to improve my python skill in data science and data analysis ability.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Understanding LSTM Networks</title>
      <link>https://isaacchanghau.github.io/post/understand_lstm/</link>
      <pubDate>Tue, 13 Jun 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/understand_lstm/</guid>
      <description>Declaration: This blog article is totaly not my original article. I just reproduce it from colah&amp;rsquo;s blog &amp;ndash; Understanding LSTM Networks, since it is really an excellent article of explanation of LSTM and I am afraid that I may lose the link of this article or the author may change her blog address. So I put this article into my own blog&amp;hellip; Moreover, the Chinese version of this article, translated by Not_GOD, are available here.</description>
    </item>
    
    <item>
      <title>Loss Functions in Neural Networks</title>
      <link>https://isaacchanghau.github.io/post/loss_functions/</link>
      <pubDate>Wed, 07 Jun 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/loss_functions/</guid>
      <description>Loss function is an important part in artificial neural networks, which is used to measure the inconsistency between predicted value ($\hat{y}$) and actual label ($y$). It is a non-negative value, where the robustness of model increases along with the decrease of the value of loss function. Loss function is the hard core of empirical risk function as well as a significant component of structural risk function. Generally, the structural risk function of a model is consist of empirical risk term and regularization term, which can be represented as $$ \begin{aligned} \boldsymbol{\theta}^{*} &amp;amp; =\arg\min_{\boldsymbol{\theta}}\boldsymbol{\mathcal{L}}(\boldsymbol{\theta})+\lambda\cdot\Phi(\boldsymbol{\theta})\newline &amp;amp; =\arg\min_{\boldsymbol{\theta}}\frac{1}{n}\sum_{i=1}^{n}L\big(y^{(i)},\hat{y}^{(i)}\big)+\lambda\cdot\Phi(\boldsymbol{\theta})\newline &amp;amp; =\arg\min_{\boldsymbol{\theta}}\frac{1}{n}\sum_{i=1}^{n}L\big(y^{(i)},f(\mathbf{x}^{(i)},\boldsymbol{\theta})\big)+\lambda\cdot\Phi(\boldsymbol{\theta}) \end{aligned} $$ where $\Phi(\boldsymbol{\theta})$ is the regularization term or penalty term, $\boldsymbol{\theta}$ is the parameters of model to be learned, $f(\cdot)$ represents the activation function and $\mathbf{x}^{(i)}=\{x_{1}^{(i)},x_{2}^{(i)},\dots ,x_{m}^{(i)}\}\in\mathbb{R}^{m}$ denotes the a training sample.</description>
    </item>
    
    <item>
      <title>Parameter Update Methods in Neural Networks</title>
      <link>https://isaacchanghau.github.io/post/parameters_update/</link>
      <pubDate>Mon, 29 May 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/parameters_update/</guid>
      <description>Gradient descent optimization algorithms are very popular to perform optimization and by far the most common way to optimize neural networks. However, there are also many other algorithms, like Hessian Free, Conjugate Gradient, BFGS, L-BFGS and etc., are proposed to deal with optimization tasks, here we only take those gradient descent methods into account. And we will discuss the drawbacks among those gradient algorithms and the methods to solve these blemishes.</description>
    </item>
    
    <item>
      <title>Weight Initialization Methods in Neural Networks</title>
      <link>https://isaacchanghau.github.io/post/weight_initialization/</link>
      <pubDate>Wed, 24 May 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/weight_initialization/</guid>
      <description>This is a summary of weight initialization in aritifical neural networks. All Zero Initialization (Pitfall): Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the âbest guessâ in expectation.</description>
    </item>
    
    <item>
      <title>Activation Functions in Neural Networks</title>
      <link>https://isaacchanghau.github.io/post/activation_functions/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/activation_functions/</guid>
      <description>By definition, activation function is a function used to transform the activation level of a unit (neuron) into an output signal. Typically, activation function has a &amp;ldquo;squashing&amp;rdquo; effect. An activation function serves as a threshold, alternatively called classification or a partition. Bengio et al. refers to this as &amp;ldquo;Space Folding&amp;rdquo;. It essentially divides the original space into typically two partitions. Activation functions are usually introduced as requiring to be a non-linear function, that is, the role of activation function is made neural networks non-linear.</description>
    </item>
    
    <item>
      <title>Machine Learning Note (4): Ensemble Learning</title>
      <link>https://isaacchanghau.github.io/post/ml_zzh_note_4/</link>
      <pubDate>Thu, 18 May 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/ml_zzh_note_4/</guid>
      <description>æèªå¨å¿åãæºå¨å­¦ä¹ ãç¬¬å«ç« -éæå­¦ä¹ ã
ä¸ªä½ä¸éæ éæå­¦ä¹  (ensemble learning) éè¿æå»ºå¹¶ç»åå¤ä¸ªå­¦ä¹ å¨æ¥å®æå­¦ä¹ ä»»å¡ï¼ä¹è¢«ç§°ä¸ºå¤åç±»å¨ç³»ç» (multi-classifier system)ãåºäºå§åä¼çå­¦ä¹  (committee-based learning) ç­ã ä¸å¾å±ç¤ºäºéæå­¦ä¹ çä¸è¬ç»æï¼åäº§çä¸ç»âä¸ªä½å­¦ä¹ å¨â (individual learner)ï¼åç¨æç§ç­ç¥å°å®ä»¬ç»åãè¥éæä¸­åªåå«åç§ç±»åçä¸ªä½å­¦ä¹ å¨ï¼è¿æ ·çéææ¯âåè´¨çâ (homogeneous)ï¼åè´¨éæä¸­çä¸ªä½å­¦ä¹ å¨ç§°ä¸ºâåºå­¦ä¹ å¨â (base learner)ï¼ç¸åºçå­¦ä¹ ç®æ³ç§°ä¸ºâåºå­¦ä¹ ç®æ³â (base learning algorithm)ãéæä¹å¯ä»¥åå«ä¸åç±»åçä¸ªä½å­¦ä¹ å¨ï¼å³âå¼è´¨çâ (heterogenous)ãå¼è´¨éæä¸­çä¸ªä½å­¦ä¹ å¨ç±ä¸åçå­¦ä¹ ç®æ³çæï¼æ­¤æ¶ä¸åæåºå­¦ä¹ ç®æ³ï¼èä¸ªä½å­¦ä¹ å¨ä¹å¸¸ç§°ä¸ºâç»ä»¶å­¦ä¹ å¨â (component learner)ã éæå­¦ä¹ éè¿å°å¤ä¸ªå­¦ä¹ å¨è¿è¡ç»åï¼éå¸¸å¯ä»¥è·å¾æ¯åä¸å­¦ä¹ å¨æ¾èä¼è¶çæ³åæ§è½ãè¿å¯¹âå¼±å­¦ä¹ å¨â (weak learner) å°¤ä¸ºææ¾ãå®éä¸­ï¼è¦è·å¾å¥½çéæï¼ä¸ªä½å­¦ä¹ å¨éå¸¸åºè¯¥âå¥½äºä¸åâï¼å³ä¸ªä½å­¦ä¹ å¨è¦æä¸å®çâåç¡®æ§âï¼å¹¶ä¸è¦æâå¤æ ·æ§âï¼å³å­¦ä¹ å¨é´å·æå·®å¼ãå¦ä¸å¾æç¤º ä¸¾ä¸ªä¾å­ï¼èèäºåç±»é®é¢ $y\in\{-1,+1\}$ åçå®å½æ° $\boldsymbol{f}$ï¼åè®¾åºåç±»å¨çéè¯¯çä¸º $\epsilon$ï¼å³å¯¹æ¯ä¸ªåºåç±»å¨ $h_{i}$ æ $$ P(h_{i}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\epsilon\tag{1} $$ åè®¾éæéè¿ç®åæç¥¨æ³ç»å $T$ ä¸ªåºåç±»å¨ï¼è¥æè¶è¿åæ°çåºåç±»å¨æ­£ç¡®ï¼åéæåç±»å°±æ­£ç¡®ï¼ $$ H(\boldsymbol{x})=sign\bigg(\sum_{i=1}^{T}h_{i}(\boldsymbol{x})\bigg)\tag{2} $$ åè®¾åºåç±»å¨çéè¯¯çç¸äºç¬ç«ï¼åç± Hoeffding ä¸ç­å¼å¯ç¥ï¼éæçéè¯¯çä¸º $$ P(H(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\sum_{k=0}^{\lfloor T/2\rfloor}{T\choose k}(1-\epsilon)^{k}\epsilon^{T-k}\leq\exp\big(-\frac{1}{2}T(1-2\epsilon)^{2}\big)\tag{3} $$ ç±ä¸å¼å¯å¾ï¼éçä¸ªä½éæä¸­ä¸ªä½åç±»å¨æ°ç® $T$ çå¢å¤§ï¼éæçéè¯¯çå°ææ°çº§ä¸éï¼æç»è¶åäºé¶ãä¸é¢çåææä¸ä¸ªå³é®åè®¾ï¼åºå­¦ä¹ å¨çè¯¯å·®ç¸äºç¬ç«ãå®éä¸ï¼ä¸ªä½å­¦ä¹ å¨æ¯ä¸ºè§£å³åä¸ä¸ªé®é¢è®­ç»åºæ¥çï¼æ¾ç¶ä¸è½ç¸äºç¬ç«ãèâåç¡®æ§âåâå¤æ ·æ§âæ¬èº«å°±å­å¨å²çªï¼å½åç¡®æ§å¾é«ä¹åï¼å¢å å¤æ ·æ§å°±éè¦çºç²åç¡®æ§ã
æ ¹æ®ä¸ªä½å­¦ä¹ å¨ççææ¹å¼ï¼éæå­¦ä¹ å¤§è´åä¸ºä¸¤ç±»ï¼
 ä¸ªä½å­¦ä¹ å¨é´å­å¨å¼ºä¾èµå³ç³»ãå¿é¡»ä¸²è¡çæçåºååæ¹æ³ï¼å¦ Boostingã ä¸ªä½å­¦ä¹ å¨ä¸å­å¨å¼ºä¾èµå³ç³»ãå¯åæ¶çæçå¹¶è¡åæ¹æ³ï¼å¦ Bagging åâéå³æ£®æâ (Random Forest)ã  Boosting Boosting æ¯ä¸æå¯å°å¼±å­¦ä¹ å¨æåä¸ºå¼ºå­¦ä¹ å¨çç®æ³ï¼å¶å·¥ä½æºå¶ç±»ä¼¼ï¼åä»åå§è®­ç»éè®­ç»åºä¸ä¸ªåºå­¦ä¹ å¨ï¼åæ ¹æ®åºå­¦ä¹ å¨çè¡¨ç°å¯¹è®­ç»æ ·æ¬è¿è¡è°æ´ï¼ä½¿å¾åååºå­¦ä¹ å¨åéçè®­ç»æ ·æ¬å¨åç»­åå°æ´å¤çå³æ³¨ï¼ç¶ååºäºè°æ´åçæ ·æ¬è®­ç»ä¸ä¸ä¸ªåºå­¦ä¹ å¨ï¼å¦æ­¤éå¤è¿è¡ï¼ç´è³åºå­¦ä¹ å¨è¾¾å°äºåæå®çå¼ $T$ï¼æç»å°è¿ $T$ ä¸ªåºå­¦ä¹ å¨è¿è¡å æç»åãå¦ AdaBoost ç®æ³ï¼</description>
    </item>
    
    <item>
      <title>Machine Learning Note (3): Support Vector Machine</title>
      <link>https://isaacchanghau.github.io/post/ml_zzh_note_3/</link>
      <pubDate>Tue, 16 May 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/ml_zzh_note_3/</guid>
      <description>æèªå¨å¿åãæºå¨å­¦ä¹ ãç¬¬å­ç« âæ¯æåéæºã
é´éä¸æ¯æåé ç»å®æ°æ®é$D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$ï¼$y_{i}\in\{-1,+1\}$ï¼åç±»å­¦ä¹ æåºæ¬çææ³æ¯åºäºè®­ç»é$D$å¨æ ·æ¬ç©ºé´ä¸­æ¾å°ä¸ä¸ªååè¶å¹³é¢ï¼å°ä¸åç±»å«çæ ·æ¬åå¼ãä½è½å°è®­ç»æ ·æ¬ç²æ¥çååè¶å¹³é¢å¯è½æå¾å¤ï¼ç´è§ä¸ï¼åºè¯¥å»æ¾ä¸¤ç±»è®­ç»æ ·æ¬âæ­£ä¸­é´âçååè¶å¹³é¢ï¼å³ä¸å¾ä¸­ç²çº¿è¡¨ç¤ºçååè¶å¹³é¢ï¼å ä¸ºè¯¥ååå¯¹è®­ç»æ ·æ¬å±é¨æ°å¨ç*âå®¹å¿æ§â*æå¥½ã å¨æ ·æ¬ç©ºé´ä¸­ï¼ååè¶å¹³é¢ç±ä¸è¿°çº¿æ§æ¹ç¨è¡¨ç¤ºï¼ $$ \boldsymbol{w}^{T}\boldsymbol{x}+b=0\tag{1} $$ å¶ä¸­$\boldsymbol{w}=(w_{1},\dots,w_{d})$ä¸ºæ³åéï¼å³å®äºè¶å¹³é¢çæ¹åï¼$b$ä¸ºä½ç§»é¡¹ï¼å³å®äºè¶å¹³é¢ä¸åç¹ä¹é´çè·ç¦»ãè®°è¶å¹³é¢ä¸º$(\boldsymbol{w},b)$ï¼æ ·æ¬ç©ºé´ä¸­ä»»æç¹$\boldsymbol{x}$å°è¶å¹³é¢$(\boldsymbol{w},b)$çè·ç¦»å¯åä¸º $$ r=\frac{\vert\boldsymbol{w}^{T}\boldsymbol{x}+b\vert}{\Vert\boldsymbol{w}\Vert}\tag{2} $$ åè®¾$(\boldsymbol{w},b)$è½å°è®­ç»æ ·æ¬æ­£ç¡®åç±»ï¼å³å¯¹äº$(\boldsymbol{x}_{i},y_{i})\in D$ï¼è¥$y_{i}=+1$ï¼åæ$\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b&amp;gt;0$ï¼è¥$y_{i}=-1$ï¼å$\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b&amp;lt;0$ãä»¤ $$ \begin{cases} \boldsymbol{w}^{T}\boldsymbol{x}_{i}+b\geq+1,&amp;amp;y_{i}=+1;\newline \boldsymbol{w}^{T}\boldsymbol{x}_{i}+b\leq-1,&amp;amp;y_{i}=-1. \end{cases}\tag{3} $$ å¦ä¸å¾ï¼è·ç¦»è¶å¹³é¢æè¿çå ä¸ªè®­ç»æ ·æ¬ç¹ä½¿å¬å¼(3)æç«ï¼å®ä»¬è¢«ç§°ä¸ºâæ¯æåéâ (support vector)ï¼ä¸¤ä¸ªå¼ç±»æ¯æåéå°è¶å¹³é¢çè·ç¦»ä¹åä¸º$$ \gamma=\frac{2}{\Vert\boldsymbol{w}\Vert}\tag{4}$$å®è¢«ç§°ä¸ºâé´éâ (margin)ã æ±è§£âæå¤§é´éâ (maximum margin)çååè¶å¹³é¢ï¼å³æ¾å°æ»¡è¶³å¬å¼(3)ä¸­ççº¦æåæ°$\boldsymbol{w}$å$b$ï¼ä½¿å¾$\gamma$æå¤§ï¼ $$ \begin{aligned} &amp;amp; \max_{\boldsymbol{w},b}\frac{2}{\Vert\boldsymbol{w}\Vert}\newline &amp;amp; s.t.\quad y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1,\quad i=1,2,\dots,m \end{aligned}\tag{5} $$ æå¤§åé´éï¼ä»éè¦æå¤§å$\Vert\boldsymbol{w}\Vert^{-1}$ï¼ç­ä»·äºæå°å$\Vert\boldsymbol{w}\Vert^{2}$ï¼äºæ¯æ $$ \begin{aligned} &amp;amp; \min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}\newline &amp;amp; s.t.\quad y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1,\quad i=1,2,\dots,m \end{aligned}\tag{6} $$ ä¸å¼ä¸ºæ¯æåéæº(Support Vector Machineï¼SVM)çåºæ¬åã
å¯¹å¶é®é¢ æä»¬å¸ææ±è§£å¼(6)æ¥å¾å°æå¤§é´éååè¶å¹³é¢æå¯¹åºçæ¨¡å $$ f(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}+b\tag{7} $$ å¬å¼(6)æ¬èº«ä¸ºä¸ä¸ªå¸äºæ¬¡è§å (convex quadratic programming)é®é¢ï¼ä¸ºæ±è§£(6)å¼ï¼å¯¹å¶ä½¿ç¨ææ ¼ææ¥ä¹å­æ³å¯å¾å¶âå¯¹å¶é®é¢â (dual problem)ï¼ $$ L(\boldsymbol{w},b,\boldsymbol{\alpha})=\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+ \sum_{i=1}^{m}\alpha_{i}\big(1-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)\tag{8} $$ å¶ä¸­$\boldsymbol{\alpha}=(\alpha_{1};\dots;\alpha_{m})$ãä»¤$L(\boldsymbol{w},b,\boldsymbol{\alpha})$å¯¹$\boldsymbol{w}$å$b$çåå¯¼ä¸ºé¶å¯å¾ $$ \boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}\tag{9} $$ $$ 0=\sum_{i=1}^{m}\alpha_{i}y_{i}\tag{10} $$ å°(9)ä»£å¥(8)ï¼å³å¯å°$L(\boldsymbol{w},b,\boldsymbol{\alpha})$ä¸­ç$\boldsymbol{w}$å$b$æ¶å»ï¼åèèå¼(10)ççº¦æï¼å¯å¾å°å¼(6)çå¯¹å¶é®é¢ $$ \begin{aligned} &amp;amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\newline &amp;amp; s.</description>
    </item>
    
    <item>
      <title>Machine Learning Note (2): Linear Regression</title>
      <link>https://isaacchanghau.github.io/post/ml_zzh_note_2/</link>
      <pubDate>Thu, 04 May 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/ml_zzh_note_2/</guid>
      <description>æèªå¨å¿åãæºå¨å­¦ä¹ ãç¬¬ä¸ç« &amp;ndash;çº¿æ§æ¨¡åã
çº¿æ§æ¨¡å ç»å®ç±$d$ä¸ªå±æ§æè¿°çç¤ºä¾$\mathbf{x}=(x_{1};x_{2};\dots ;x_{d})$ï¼å¶ä¸­$x_{i}$æ¯$\mathbf{x}$å¨ç¬¬$i$ä¸ªå±æ§ä¸çåå¼ï¼çº¿æ§æ¨¡å(linear model)è¯å¾å­¦çä¸ä¸ªéè¿å±æ§ççº¿æ§ç»åæ¥è¿è¡é¢æµçå½æ°ï¼å³ $$ f(\mathbf{x})=\mathbf{w}^{T}\mathbf{x}+b $$ å¶ä¸­$\mathbf{w}=(w_{1};w_{2};\dots ;w_{d})$ã$\mathbf{w}$å$b$å­¦å¾ä¹åï¼æ¨¡åå°±å¯ä»¥ç¡®å®ã
çº¿æ§åå½ ç»å®æ°æ®é$D={(\mathbf{x}_{1},y_{1}),(\mathbf{x}_{2},y_{2}),\dots ,(\mathbf{x}_{m},y_{m})}$ï¼å¶ä¸­$\mathbf{x}_{i}=(x_{i1};x_{i2};\dots ;x_{id}), y_{i}\in \mathbb{R}$ãâçº¿æ§åå½â (linear regression)è¯å¾å­¦å¾ $$ f(\mathbf{x}_{i})=\mathbf{w}^{T}\mathbf{x}_{i}+b, ä½¿å¾f(\mathbf{x}_{i})\simeq y_{i} $$ æ¥ä¸æ¥çä»»å¡æ¯ç¡®å®$\mathbf{w}$å$b$ï¼å¶å³é®å¨äºè¡¡é$f(x)$å$y$ä¹é´çå·®å«ï¼èåæ¹è¯¯å·®æ¯åå½ä»»å¡ä¸­æå¸¸ç¨çæ§è½åº¦éï¼å®å¯¹åºäºå¸¸ç¨çâæ¬§å¼è·ç¦»â (Euclidean distance)ï¼å æ­¤å¯ä»¥è¯å¾è®©åæ¹è¯¯å·®æå°åãåæ¹è¯¯å·®æå°åè¿è¡æ¨¡åæ±è§£çæ¹æ³ç§°ä¸ºâæå°äºä¹æ³â (least square method)ãå¨çº¿æ§åå½ä¸­ï¼æå°äºä¹æ³å°±æ¯è¯å¾æ¾å°ä¸æ¡ç´çº¿ï¼ä½¿æææ ·æ¬å°ç´çº¿ä¸çæ¬§å¼è·ç¦»ä¹åæå°ãè¿éï¼æä»¬å°$\mathbf{w}$å$b$å¸æ¶å¥åéå½¢å¼$\mathbf{\hat{w}}=(b;\mathbf{w})=(b;w_{1};w_{2};\dots ;w_{d})$ï¼ç¸åºçï¼å°æ°æ®é$D$ä¸­ç$\mathbf{x}_{i}$è¡¨ç¤ºä¸º$\mathbf{x}_{i}=(1;x_{i1};x_{i2};\dots ;x_{id})$ï¼å æ­¤æ°æ®é$D$å¯ä»¥è¡¨ç¤ºä¸ºä¸ä¸ª$m\times (d+1)$çç©éµ$\mathbf{X}=(\mathbf{x}_{1};\mathbf{x}_{2};\dots ;\mathbf{x}_{m})^{T}$ï¼åææ è®°ä¹åæåéå½¢å¼$\mathbf{y}=(y_{1};y_{2};\dots ;y_{m})$ãäºæ¯æ $$ f(\mathbf{X})=\mathbf{X}\mathbf{\hat{w}}, ä½¿å¾f(\mathbf{X})\simeq\mathbf{y} $$ æå°ååæ¹è¯¯å·®ï¼æ $$ \mathbf{\hat{w}}^{*}=\arg\min_{\mathbf{\hat{w}}}(f(\mathbf{X})-\mathbf{y})^{2}=\arg\min_{\mathbf{\hat{w}}}(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})^{T}(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}}) $$ ä»¤$E_{\mathbf{\hat{w}}}=(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})^{T}(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})$ï¼å¯¹$\mathbf{\hat{w}}$æ±å¯¼å¾å° $$\frac{\partial E_{\mathbf{\hat{w}}}}{\partial \mathbf{\hat{w}}}=2\mathbf{X}^{T}(\mathbf{X}\mathbf{\hat{w}}-\mathbf{y}) $$ ä»¤ä¸å¼ä¸ºé¶å¯å¾$\mathbf{\hat{w}}$æä¼è§£çå°é­å¼ãå½$\mathbf{X}^{T}\mathbf{X}$ä¸ºæ»¡ç§©ç©éµ (full-rank matrix)ææ­£å®ç©éµ (positive definite matrix)æ¶ï¼å¯å¾å° $$ \mathbf{\hat{w}}^{*}=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y} $$ å¶ä¸­$(\mathbf{X}^{T}\mathbf{X})^{-1}$æ¯$\mathbf{X}^{T}\mathbf{X}$çéç©éµãç¶èç°å®ä»»å¡ä¸­ï¼$\mathbf{X}^{T}\mathbf{X}$å¾å¾ä¸æ¯æ»¡ç§©ç©éµï¼æ­¤æ¶å¯è§£åºå¤ä¸ª$\mathbf{\hat{w}}$ï¼é½è½ä½¿åæ¹è¯¯å·®æå°åï¼èéæ©åªä¸ä¸ªè§£ä½ä¸ºè¾åºå°ç±ç®æ³çå½çº³åå¥½å³å®ï¼å¸¸è§çåæ³æ¯å¼å¥æ­£åå(regularization)é¡¹ã
çº¿æ§æ¨¡åé¢æµå¼ä¸ä»å¯ä»¥é¼è¿$y$ï¼ä¹å¯ä»¥ä½¿å¶é¼è¿$y$çè¡çç©ãæ¯å¦ï¼å¯å°è¾åºæ è®°çå¯¹æ°ä½ä¸ºçº¿æ§æ¨¡åé¼è¿çç®æ ï¼å³âå¯¹æ°çº¿æ§åå½â (log-linear regression) $$ \ln y=\mathbf{w}^{T}\mathbf{x}+b $$ å®å®éä¸æ¯è®©$e^{\mathbf{w}^{T}\mathbf{x}+b}$é¼è¿$y$ï¼å½¢å¼ä¸å®ä»æ¯çº¿æ§åå½ï¼ä½å®è´¨ä¸å·²æ¯å¨æ±åè¾å¥ç©ºé´å°è¾åºç©ºé´çéçº¿æ§å½æ°æ å°ï¼å¦å¾ æ´ä¸è¬å°ï¼èèåè°å¯å¾®å½æ°$g(\cdot)$ï¼ä»¤$$y=g^{-1}(\mathbf{w}^{T}\mathbf{x}+b)$$è¿æ ·å¾å°çæ¨¡åç§°ä¸ºâå¹¿ä¹çº¿æ§æ¨¡åâ (generalized linear model)ï¼å¶ä¸­å½æ°$g(\cdot)$ç§°ä¸ºâèç³»å½æ°â (link function)ãå¯¹æ°çº¿æ§åå½æ¯å¹¿ä¹çº¿æ§æ¨¡åå¨$g(\cdot)=\ln (\cdot)$æ¶çç¹ä¾ã</description>
    </item>
    
    <item>
      <title>Machine Learning Note (1): Model Evaluation and Selection</title>
      <link>https://isaacchanghau.github.io/post/ml_zzh_note_1/</link>
      <pubDate>Fri, 28 Apr 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/ml_zzh_note_1/</guid>
      <description>æèªå¨å¿åãæºå¨å­¦ä¹ ãç¬¬äºç« &amp;ndash;æ¨¡åè¯ä¼°ä¸éæ©ã
æ ¹æ®è®­ç»æ°æ®æ¯å¦æ¥ææ è®°ä¿¡æ¯ï¼å­¦ä¹ ä»»å¡å¯å¤§è´åä¸ºä¸¤å¤§ç±»: âçç£å­¦ä¹ â (supervised learning) åâæ çç£å­¦ä¹ â (unsupervised learning)ï¼åç±» (classification) ååå½ (regression) æ¯åèçä»£è¡¨ï¼èèç±» (clustering) æ¯åèçä»£è¡¨ã æºå¨å­¦ä¹ çç®æ æ¯ä½¿å­¦å¾çæ¨¡åè½å¾å¥½çéç¨äºâæ°æ ·æ¬âï¼èä¸æ¯ä»ä»å¨è®­ç»æ ·æ¬ä¸å·¥ä½å¾å¾å¥½ï¼å³ä¾¿å¯¹èç±»è¿æ ·çæ çç£å­¦ä¹ ä»»å¡ï¼æä»¬ä¹å¸æå­¦å¾çç°ååè½éç¨äºæ²¡å¨è®­ç»éä¸­åºç°çæ ·æ¬ãå­¦å¾æ¨¡åéç¨äºæ°æ ·æ¬çè½åï¼ç§°ä¸ºæ³å (generalization) è½åãå·æå¼ºæ³åè½åçæ¨¡åè½å¾å¥½çéç¨äºæ´ä¸ªæ ·æ¬ç©ºé´ã
ç»éªè¯¯å·®ä¸è¿æå éè¯¯ç (error rate): ä¸ºåç±»éè¯¯çæ ·æ¬æ°å æ ·æ¬æ»æ°çæ¯ä¾ï¼å³å¨$m$ä¸ªæ ·æ¬ä¸­æ$a$ä¸ªæ ·æ¬åç±»éè¯¯ï¼åéè¯¯ç$E=\frac{a}{m}$ï¼ç¸åºç$1-\frac{a}{m}$ç§°ä¸ºç²¾åº¦ (accuracy)ï¼å³âç²¾åº¦=1-éè¯¯çâã
è¯¯å·® (error): å­¦ä¹ å¨å®éé¢æµè¾åºä¸æ ·æ¬ççå®è¾åºä¹é´çå·®å¼ã
è®­ç»è¯¯å·® (training error)æâç»éªè¯¯å·®â(empirical error): å­¦ä¹ å¨å¨è®­ç»éä¸çè¯¯å·®ã
æ³åè¯¯å·® (generalization error): å­¦ä¹ å¨å¨æ°æ ·æ¬ä¸çè¯¯å·®ã è¿æå (overfitting): æå¨æåä¸ä¸ªç»è®¡æ¨¡åæ¶ï¼ä½¿ç¨è¿å¤åæ°ãå¯¹æ¯äºå¯è·åçæ°æ®æ»éæ¥è¯´ï¼ä¸ä¸ªèè°¬çæ¨¡ååªè¦è¶³å¤å¤æï¼æ¯å¯ä»¥å®ç¾å°éåºæ°æ®ãè¿æåä¸è¬å¯ä»¥è§ä¸ºè¿åå¥¥å¡å§ååååãå½å¯éæ©çåæ°çèªç±åº¦è¶è¿æ°æ®æåå«ä¿¡æ¯åå®¹æ¶ï¼è¿ä¼å¯¼è´æåï¼æååï¼æ¨¡åä½¿ç¨ä»»æçåæ°ï¼è¿ä¼åå°æç ´åæ¨¡åä¸è¬åçè½åæ´çäºéåºæ°æ®ãè¿æåçå¯è½æ§ä¸åªåå³äºåæ°ä¸ªæ°åæ°æ®ï¼ä¹è·æ¨¡åæ¶æä¸æ°æ®çä¸è´æ§æå³ãæ­¤å¤å¯¹æ¯äºæ°æ®ä¸­é¢æçåªå£°æéè¯¯æ°éï¼è·æ¨¡åéè¯¯çæ°éä¹æå³ã
æ¬ æå (underfitting): ææ¨¡åæ²¡æå¾å¥½å°ææå°æ°æ®ç¹å¾ï¼ä¸è½å¤å¾å¥½å°æåæ°æ®ï¼ä¸è¿æåç¸åã
è¯ä¼°æ¹æ³ çåºæ³ (hold-out): ç´æ¥å°æ°æ®éDååä¸ºä¸¤ä¸ªäºæ¥çéåï¼å¶ä¸­ä¸ä¸ªéåä½ä¸ºè®­ç»é$S$ï¼å¦ä¸ä¸ªä½ä¸ºæµè¯é$T$ï¼å³$D=S\cup T$ï¼$S\cap T=\varnothing$ãå¨$S$ä¸è®­ç»åºæ¨¡ååï¼ç¨$T$æ¥è¯ä¼°å¨æµè¯è¯¯å·®ï¼ä½ä¸ºæ³åè¯¯å·®çä¼°è®¡ãæåæ°æ®é$D$ä¸è¬éç¨âåå±éæ ·â (stratified sampling)ã
äº¤åéªè¯æ³ (cross validation): åå°æ°æ®é$D$ååä¸º$k$ä¸ªå¤§å°ç¸ä¼¼çäºæ¥å­éï¼å³$D=D_{1}\cup D_{2} \cup &amp;hellip;\cup D_{k}$ï¼$D_{i}\cap D_{j}=\varnothing (i\neq j)$ãæ¯ä¸ªå­é$D_{i}$é½å°½å¯è½ä¿ææ°æ®åå¸çä¸è´æ§ï¼å³ä»$D$ä¸­éè¿åå±éæ ·å¾å°ãç¶åï¼æ¯æ¬¡ç¨k-1ä¸ªå­éçå¹¶éä½ä¸ºè®­ç»éï¼ä½ä¸çé£ä¸ªå­éä½ä¸ºæµè¯éï¼è¿æ ·å°±å¯ä»¥è·å¾kç»è®­ç»/æµè¯éï¼ä»èå¯è¿è¡kæ¬¡è®­ç»åæµè¯ï¼æç»è¿åkæ¬¡æµè¯ç»æçåå¼ãè¿ç§æ¹æ³çç¨³å®æ§ (stability) åä¿çæ§ (fidelity) å¾å¤§ç¨åº¦ä¸åå³äºkçåå¼ï¼éå¸¸è¿ç§æ¹æ³åç§°ä¸ºâkæäº¤åéªè¯â (k-fold cross validation)ãéå¸¸kçåå¼ä¸º10ã
èªå©æ³ (bootstrapping): ä»¥èªå©éæ · (bootstrap sampling) ä¸ºåºç¡ï¼ç»å®åå«$m$ä¸ªæ ·æ¬çæ°æ®é$D$ï¼å¯¹å®è¿è¡éæ ·äº§çæ°æ®é$D&amp;rsquo;$ï¼æ¯æ¬¡éæºä»$D$ä¸­æéä¸ä¸ªæ ·æ¬ï¼å°å¶æ·è´å°$D&amp;rsquo;$ï¼ç¶ååå°è¯¥æ ·æ¬æ¾ååå§æ°æ®é$D$ä¸­ï¼ä½¿å¾è¯¥æ ·æ¬å¨ä¸æ¬¡éæ ·æ¶ä»æå¯è½è¢«éå°ï¼è¿ä¸ªè¿ç¨éå¤æ§è¡$m$æ¬¡åï¼å°±å¾å°åå«$m$ä¸ªæ ·æ¬çæ°æ®é$D&amp;rsquo;$ï¼è¿å°±æ¯èªå©éæ ·çç»æãæ¾ç¶$D$ä¸­æä¸é¨åæ ·æ¬ä¼å¨$D&amp;rsquo;$ä¸­å¤æ¬¡åºç°ï¼èå¦ä¸é¨åæ ·æ¬ä¸åºç°ãç²ç¥ä¼°è®¡ï¼æ ·æ¬å¨$m$æ¬¡éæ ·ä¸­å§ç»ä¸è¢«éå°çæ¦çæ¯$(1-\frac{1}{m})^{m}$ï¼åæéå¾å° $$ \lim_{m \to \infty}(1-\frac{1}{m})^{m}\longmapsto\frac{1}{e}\approx0.</description>
    </item>
    
  </channel>
</rss>