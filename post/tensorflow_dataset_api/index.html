<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.40.1" />
  <meta name="author" content="Zhang Hao">

  
  
  
  
    
      
    
  
  <meta name="description" content="Reprinted from Francesco Zuppichini·How to use Dataset in TensorFlow.
Based on TensorFlow 1.6
As you should know, feed-dict is the slowest possible way to pass information to TensorFlow and it must be avoided. The correct way to feed data into your models is to use an input pipeline to ensure that the GPU has never to wait for new stuff to come in.
Fortunately, TensorFlow has a built-in API, called Dataset to make it easier to accomplish this task.">

  
  <link rel="alternate" hreflang="en-us" href="https://isaacchanghau.github.io/post/tensorflow_dataset_api/">

  


  

  
  
  <meta name="theme-color" content="#3f51b5">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  <link rel="feed" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://isaacchanghau.github.io/post/tensorflow_dataset_api/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/IsaacChanghau">
  <meta property="twitter:creator" content="@https://twitter.com/IsaacChanghau">
  
  <meta property="og:site_name" content="Isaac Changhau">
  <meta property="og:url" content="https://isaacchanghau.github.io/post/tensorflow_dataset_api/">
  <meta property="og:title" content="Introduction of TensorFlow Dataset API | Isaac Changhau">
  <meta property="og:description" content="Reprinted from Francesco Zuppichini·How to use Dataset in TensorFlow.
Based on TensorFlow 1.6
As you should know, feed-dict is the slowest possible way to pass information to TensorFlow and it must be avoided. The correct way to feed data into your models is to use an input pipeline to ensure that the GPU has never to wait for new stuff to come in.
Fortunately, TensorFlow has a built-in API, called Dataset to make it easier to accomplish this task.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-02-21T00:00:00&#43;08:00">
  
  <meta property="article:modified_time" content="2018-02-21T00:00:00&#43;08:00">
  

  

  <title>Introduction of TensorFlow Dataset API | Isaac Changhau</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Isaac Changhau</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#tags">
            
            <span>Tags</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#achievements">
            
            <span>Achievements</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Introduction of TensorFlow Dataset API</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-02-21 00:00:00 &#43;0800 &#43;08" itemprop="datePublished dateModified">
      Wed, Feb 21, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhang Hao">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    10 min read
  </span>
  

  
  

  

  
  

  

</div>

    
    <div class="article-style" itemprop="articleBody">
      

<p>Reprinted from <a href="https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428" target="_blank">Francesco Zuppichini·How to use Dataset in TensorFlow</a>.</p>

<p><strong>Based on TensorFlow 1.6</strong></p>

<p>As you should know, <code>feed-dict</code> is the slowest possible way to pass information to TensorFlow and it must be avoided. The correct way to feed data into your models is to use an input pipeline to ensure that the GPU has never to wait for new stuff to come in.</p>

<p>Fortunately, TensorFlow has a built-in API, called <a href="https://www.tensorflow.org/programmers_guide/datasets" target="_blank">Dataset</a> to make it easier to accomplish this task. In this tutorial, we are going to see how we can create an input pipeline using it and how to feed the data into the model efficiently.</p>

<p>This article will explain the basic mechanics of the Dataset, covering the most common use cases. You can found all the code as a jupyter notebook here: <a href="https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb" target="_blank">[link]</a>.</p>

<h3 id="generic-overview">Generic Overview</h3>

<p>In order to use a Dataset we need three steps:</p>

<ol>
<li><strong>Importing Data</strong>. Create a Dataset instance from some data</li>
<li><strong>Create an Iterator</strong>. By using the created dataset to make an Iterator instance to iterate thought the dataset</li>
<li><strong>Consuming Data</strong>. By using the created iterator we can get the elements from the dataset to feed the model</li>
</ol>

<h3 id="importing-data">Importing Data</h3>

<p>We first need some data to put inside our dataset</p>

<p><strong>From <code>Numpy</code></strong>：This is the common case, we have a numpy array and we want to pass it to tensorflow.</p>

<pre><code class="language-python"># create a random vector of shape (100,2)
x = np.random.sample((100,2))
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x)
</code></pre>

<p>We can also pass more than one <code>numpy</code> array, one classic example is when we have a couple of data divided into features and labels</p>

<pre><code class="language-python">features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))
dataset = tf.data.Dataset.from_tensor_slices((features,labels))
</code></pre>

<p><strong>From <code>tensors</code></strong>: We can, of course, initialise our dataset with some tensor</p>

<pre><code class="language-python"># using a tensor
dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))
</code></pre>

<p><strong>From a <code>placeholder</code></strong>: This is useful when we want to dynamic change the data inside the Dataset, we will se later how.</p>

<pre><code class="language-python">x = tf.placeholder(tf.float32, shape=[None,2])
dataset = tf.data.Dataset.from_tensor_slices(x)
</code></pre>

<p><strong>From <code>generator</code></strong>: We can also initialise a Dataset from a <code>generator</code>, this is useful when we have an array of different elements lenght (e.g a sequence):</p>

<pre><code class="language-python">sequence = np.array([[1],[2,3],[3,4]])


def generator():
    for el in sequence:
        yield el


dataset = tf.data.Dataset().from_generator(generator, output_types=tf.float32, output_shapes=[tf.float32])
</code></pre>

<p>In this case you also need specify the types and the shapes of your data that will be used to create the correct <code>tensors</code>.</p>

<h3 id="create-an-iterator">Create an Iterator</h3>

<p>We have seen how to create a dataset, but how to get our data back? We have to use an <code>Iterator</code>, that will give us the ability to iterate through the dataset and retrieve the real values of the data. There exist four types of iterators.</p>

<p><strong>One shot Iterator</strong>: This is the easiest iterator. Using the first example</p>

<pre><code class="language-python">x = np.random.sample((100,2))
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x)
# create the iterator
iter = dataset.make_one_shot_iterator()
</code></pre>

<p>Then you need to call <code>get_next()</code> to get the tensor that will contain your data</p>

<pre><code class="language-python">...
# create the iterator
iter = dataset.make_one_shot_iterator()
el = iter.get_next()
</code></pre>

<p>We can run <code>el</code> in order to see its value.</p>

<pre><code class="language-python">with tf.Session() as sess:
    print(sess.run(el)) # output: [ 0.42116176  0.40666069]
</code></pre>

<p><strong>Initializable Iterator</strong>: In case we want to build a dynamic dataset in which we can change the data sourceat runtime, we can create a dataset with a <code>placeholder</code>. Then we can initialize the <code>placeholder</code> using the common <code>feed-dict</code> mechanism. This is done with an initializable iterator. Using example three from last section</p>

<pre><code class="language-python"># using a placeholder
x = tf.placeholder(tf.float32, shape=[None,2])
dataset = tf.data.Dataset.from_tensor_slices(x)
data = np.random.sample((100,2))
iter = dataset.make_initializable_iterator() # create the iterator
el = iter.get_next()
with tf.Session() as sess:
    # feed the placeholder with data
    sess.run(iter.initializer, feed_dict={ x: data }) 
    print(sess.run(el)) # output [ 0.52374458  0.71968478]
</code></pre>

<p>This time we call <code>make_initializable_iterator</code>. Then, inside the <code>sess</code> scope, we run the initializer operation in order to pass our data, in this case a random <code>numpy</code> array.</p>

<p>Imagine that now we have a train set and a test set, a real common scenario:</p>

<pre><code class="language-python">train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.array([[1,2]]), np.array([[0]]))
</code></pre>

<p>Then we would like to train the model and then evaluate it on the test dataset, this can be done by initialising the iterator again after training</p>

<pre><code class="language-python"># initializable iterator to switch between dataset
EPOCHS = 10
x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
dataset = tf.data.Dataset.from_tensor_slices((x, y))
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.array([[1,2]]), np.array([[0]]))
iter = dataset.make_initializable_iterator()
features, labels = iter.get_next()
with tf.Session() as sess:
    # initialise iterator with train data
    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})
    for _ in range(EPOCHS):
        sess.run([features, labels])
    # switch to test data
    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})
    print(sess.run([features, labels]))

</code></pre>

<p><strong>Reinitializable Iterator</strong>: The concept is similar to before, we want to dynamic switch between data. But instead of feed new data to the same dataset, we switch dataset. As before, we want to have a train dataset and a test dataset</p>

<pre><code class="language-python"># making fake data using numpy
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.random.sample((10,2)), np.random.sample((10,1)))
</code></pre>

<p>We can create two Datasets</p>

<pre><code class="language-python"># create two datasets, one for training and one for test
train_dataset = tf.data.Dataset.from_tensor_slices(train_data)
test_dataset = tf.data.Dataset.from_tensor_slices(test_data)
</code></pre>

<p>Now this is the trick, we create a generic <code>Iterator</code></p>

<pre><code class="language-python"># create a iterator of the correct shape and type
iter = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)
</code></pre>

<p>and then two initialisation operations:</p>

<pre><code class="language-python"># create the initialisation operations
train_init_op = iter.make_initializer(train_dataset)
test_init_op = iter.make_initializer(test_dataset)
</code></pre>

<p>We get the next element as before</p>

<pre><code class="language-python">features, labels = iter.get_next()
</code></pre>

<p>Now, we can directly run the two initialisation operation using our <code>session</code>. Putting all together we get:</p>

<pre><code class="language-python"># Reinitializable iterator to switch between Datasets
EPOCHS = 10
# making fake data using numpy
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.random.sample((10,2)), np.random.sample((10,1)))
# create two datasets, one for training and one for test
train_dataset = tf.data.Dataset.from_tensor_slices(train_data)
test_dataset = tf.data.Dataset.from_tensor_slices(test_data)
# create a iterator of the correct shape and type
iter = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)
features, labels = iter.get_next()
# create the initialisation operations
train_init_op = iter.make_initializer(train_dataset)
test_init_op = iter.make_initializer(test_dataset)
with tf.Session() as sess:
    sess.run(train_init_op) # switch to train dataset
    for _ in range(EPOCHS):
        sess.run([features, labels])
    sess.run(test_init_op) # switch to val dataset
    print(sess.run([features, labels]))
</code></pre>

<p><strong>Feedable Iterator</strong>: Honestly, I don’t think they are useful. Basically instead of switch between datasets they switch between iterators so you can have, for example, one iterator from <code>make_one_shot_iterator()</code> and one from <code>make_initializable_iterator()</code>.</p>

<h3 id="consuming-data">Consuming data</h3>

<p>In the previous example we have used the <code>session</code> to print the value of the <code>next</code> element in the Dataset.</p>

<pre><code class="language-python">...
next_el = iter.get_next()
...
print(sess.run(next_el)) # will output the current element
</code></pre>

<p>In order to pass the data to a model we have to just pass the tensors generated from <code>get_next()</code>.</p>

<p>In the following snippet we have a Dataset that contains two numpy arrays, using the same example from the first section. Notice that we need to wrap the <code>.random.sample</code> in another numpy array to add a dimension that we is needed to batch the data</p>

<pre><code class="language-python"># using two numpy arrays
features, labels = (np.array([np.random.sample((100,2))]), np.array([np.random.sample((100,1))]))
dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)
</code></pre>

<p>Then as always we create an iterator</p>

<pre><code class="language-python">iter = dataset.make_one_shot_iterator()
x, y = iter.get_next()
</code></pre>

<p>We make a model, a simple neural network</p>

<pre><code class="language-python"># make a simple model
net = tf.layers.dense(x, 8) # pass the first value from iter.get_next() as input
net = tf.layers.dense(net, 8)
prediction = tf.layers.dense(net, 1)
loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label
train_op = tf.train.AdamOptimizer().minimize(loss)
</code></pre>

<p>We <strong>directly</strong> use the Tensors from <code>iter.get_next()</code> as input to the first layer and as labels for the loss function. Wrapping all together:</p>

<pre><code class="language-python">EPOCHS = 10
BATCH_SIZE = 16
# using two numpy arrays
features, labels = (np.array([np.random.sample((100,2))]), 
                    np.array([np.random.sample((100,1))]))
dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)
iter = dataset.make_one_shot_iterator()
x, y = iter.get_next()
# make a simple model
net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input
net = tf.layers.dense(net, 8, activation=tf.tanh)
prediction = tf.layers.dense(net, 1, activation=tf.tanh)
loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label
train_op = tf.train.AdamOptimizer().minimize(loss)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(EPOCHS):
        _, loss_value = sess.run([train_op, loss])
        print(&quot;Iter: {}, Loss: {:.4f}&quot;.format(i, loss_value))
</code></pre>

<p>Output:</p>

<pre><code class="language-bash">Iter: 0, Loss: 0.1328 
Iter: 1, Loss: 0.1312 
Iter: 2, Loss: 0.1296 
Iter: 3, Loss: 0.1281 
Iter: 4, Loss: 0.1267 
Iter: 5, Loss: 0.1254 
Iter: 6, Loss: 0.1242 
Iter: 7, Loss: 0.1231 
Iter: 8, Loss: 0.1220 
Iter: 9, Loss: 0.1210
</code></pre>

<h3 id="useful-stuff">Useful Stuff</h3>

<p><strong>Batch</strong>: Usually batching data is a pain in the ass, with the Dataset API we can use the method <code>batch(BATCH_SIZE)</code> that automatically batches the dataset with the provided size. The default value is one. In the following example, we use a batch size of 4</p>

<pre><code class="language-python"># BATCHING
BATCH_SIZE = 4
x = np.random.sample((100,2))
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)
iter = dataset.make_one_shot_iterator()
el = iter.get_next()
with tf.Session() as sess:
    print(sess.run(el)) 
</code></pre>

<p>Output:</p>

<pre><code class="language-bash">[[ 0.65686128  0.99373963]
 [ 0.69690451  0.32446826]
 [ 0.57148422  0.68688242]
 [ 0.20335116  0.82473219]]
</code></pre>

<p><strong>Repeat</strong>: Using <code>.repeat()</code> we can specify the number of times we want the dataset to be iterated. If no parameter is passed it will loop forever, usually is good to just loop forever and directly control the number of epochs with a standard loop.</p>

<p><strong>Shuffle</strong>: We can shuffle the Dataset by using the method <code>shuffle()</code> that shuffles the dataset by default every epoch.</p>

<p><em>Remember: shuffle the dataset is very important to avoid overfitting.</em></p>

<p>We can also set the parameter <code>buffer_size</code>, a fixed size buffer from which the next element will be uniformly chosen from. Example:</p>

<pre><code class="language-python"># BATCHING
BATCH_SIZE = 4
x = np.array([[1],[2],[3],[4]])
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x)
dataset = dataset.shuffle(buffer_size=100)
dataset = dataset.batch(BATCH_SIZE)
iter = dataset.make_one_shot_iterator()
el = iter.get_next()
with tf.Session() as sess:
    print(sess.run(el))
</code></pre>

<p>First run output:</p>

<pre><code class="language-bash">[[4]
 [2]
 [3]
 [1]]
</code></pre>

<p>Second run output:</p>

<pre><code class="language-bash">[[3]
 [1]
 [2]
 [4]]
</code></pre>

<p>Yep. It was shuffled. If you want, you can also set the <code>seed</code> parameter.</p>

<h3 id="mapping">Mapping</h3>

<p>You can apply a custom function to each member of a dataset using the <code>map()</code> method. In the following example we multiply each element by two:</p>

<pre><code class="language-python"># MAP
x = np.array([[1],[2],[3],[4]])
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x)
dataset = dataset.map(lambda x: x*2)
iter = dataset.make_one_shot_iterator()
el = iter.get_next()
with tf.Session() as sess:
    # this will run forever
    for _ in range(len(x)):
        print(sess.run(el))
</code></pre>

<p>Output:</p>

<pre><code class="language-bash">[2]
[4]
[6]
[8]
</code></pre>

<h3 id="full-example">Full example</h3>

<p>In the example below we train a simple model using batching and we switch between train and test dataset</p>

<pre><code class="language-python"># Wrapping all together -&gt; Switch between train and test set
EPOCHS = 10
BATCH_SIZE = 16
x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(BATCH_SIZE).repeat()
# using two numpy arrays
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.random.sample((20,2)), np.random.sample((20,1)))
n_batches = len(train_data[0]) // BATCH_SIZE
iter = dataset.make_initializable_iterator()
features, labels = iter.get_next()

# make a simple model
net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input
net = tf.layers.dense(net, 8, activation=tf.tanh)
prediction = tf.layers.dense(net, 1, activation=tf.tanh)
loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label
train_op = tf.train.AdamOptimizer().minimize(loss)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # initialise iterator with train data
    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})
    print('Training...')
    for i in range(EPOCHS):
        tot_loss = 0
        for _ in range(n_batches):
            _, loss_value = sess.run([train_op, loss])
            tot_loss += loss_value
        print(&quot;Iter: {}, Loss: {:.4f}&quot;.format(i, tot_loss / n_batches))
    # initialise iterator with test data
    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})
    print('Test Loss: {:4f}'.format(sess.run(loss)))
</code></pre>

<p>Output:</p>

<pre><code class="language-bash">Training...
Iter: 0, Loss: 0.5199
Iter: 1, Loss: 0.4504
Iter: 2, Loss: 0.3418
Iter: 3, Loss: 0.2889
Iter: 4, Loss: 0.2352
Iter: 5, Loss: 0.1722
Iter: 6, Loss: 0.1605
Iter: 7, Loss: 0.1234
Iter: 8, Loss: 0.1192
Iter: 9, Loss: 0.0968
Test Loss: 0.115165
</code></pre>

<h3 id="other-resources">Other resources</h3>

<ul>
<li>TensorFlow dataset tutorial: <a href="https://www.tensorflow.org/programmers_guide/datasets" target="_blank">[link]</a></li>
<li>Dataset docs: <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" target="_blank">[link]</a></li>
</ul>

<h3 id="conclusion">Conclusion</h3>

<p>The Dataset API gives us a fast and robust way to create optimized input pipeline to train, evaluate and test our models. In this article, we have seen most of the common operation we can do with them.</p>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/python">python</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/tensorflow">tensorflow</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/tensorflow_scan/">TensorFlow Updates Multiple Tensors Using Scan</a></li>
        
        <li><a href="/post/ptranse/">PTransE -- Modeling Relation Paths for Representation Learning of Knowledge Bases</a></li>
        
        <li><a href="/post/transx/">TransX: Embedding Entities and Relationships of Multi-relational Data</a></li>
        
        <li><a href="/post/wolf_warrior/">Analyzing Wolf Warrior II Movie Comments using Python</a></li>
        
        <li><a href="/post/wechat_analysis/">Analyzing Your Friends Information in WeChat using Python</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2018 &middot; 

      <strong>Isaac Changhau (Zhang Hao)</strong>
      

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/java.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/javascript.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/c.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/html.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/xml.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ 
          tex2jax: { 
            inlineMath: [['$','$'], ['\\(','\\)']] 
          } 
        });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-xAWI9i8WMRLdgksuhaMCYMTw9D+MEc2cYVBApWwGRJ0cdcywTjMovOfJnlGt9LlEQj6QzyMzpIZLMYujetPcQg==" crossorigin="anonymous"></script>
    
    
    

  </body>
</html>

