<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.45" />
  <meta name="author" content="Zhang Hao">

  
  
  
  
    
      
    
  
  <meta name="description" content="The article is excerpted from Andrew Ng&rsquo;s CS294A Lecture notes: Sparse Autoencoder with some personal understanding. Before going through this article, you would better get some information from the Backpropagation Algorithm.
Suppose we have only unlabeled training examples set $\{x^{(1)},x^{(2)},x^{(3)},\dots\}$, where $x^{(i)}\in\mathbb{R}^{n}$. An autoencoder neural network is an unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. i.e., it uses $y^{(i)}=x^{(i)}$. Below is an autoencoder: The autoencoder tries to learn a function $h_{W,b}(x)\approx x$.">

  
  <link rel="alternate" hreflang="en-us" href="https://isaacchanghau.github.io/post/autoencoder_and_sparsity/">

  


  

  
  
  <meta name="theme-color" content="#3f51b5">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  <link rel="feed" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://isaacchanghau.github.io/post/autoencoder_and_sparsity/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/IsaacChanghau">
  <meta property="twitter:creator" content="@https://twitter.com/IsaacChanghau">
  
  <meta property="og:site_name" content="Isaac Changhau">
  <meta property="og:url" content="https://isaacchanghau.github.io/post/autoencoder_and_sparsity/">
  <meta property="og:title" content="Autoencoder and Sparsity | Isaac Changhau">
  <meta property="og:description" content="The article is excerpted from Andrew Ng&rsquo;s CS294A Lecture notes: Sparse Autoencoder with some personal understanding. Before going through this article, you would better get some information from the Backpropagation Algorithm.
Suppose we have only unlabeled training examples set $\{x^{(1)},x^{(2)},x^{(3)},\dots\}$, where $x^{(i)}\in\mathbb{R}^{n}$. An autoencoder neural network is an unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. i.e., it uses $y^{(i)}=x^{(i)}$. Below is an autoencoder: The autoencoder tries to learn a function $h_{W,b}(x)\approx x$.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-08-19T00:00:00&#43;08:00">
  
  <meta property="article:modified_time" content="2017-08-19T00:00:00&#43;08:00">
  

  

  <title>Autoencoder and Sparsity | Isaac Changhau</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Isaac Changhau</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#tags">
            
            <span>Tags</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#achievements">
            
            <span>Achievements</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Autoencoder and Sparsity</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2017-08-19 00:00:00 &#43;0800 &#43;08" itemprop="datePublished dateModified">
      Sat, Aug 19, 2017
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhang Hao">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    9 min read
  </span>
  

  
  

  

  
  

  

</div>

    
    <div class="article-style" itemprop="articleBody">
      <p>The article is excerpted from Andrew Ng&rsquo;s <a href="https://web.stanford.edu/class/cs294a/" target="_blank">CS294A Lecture notes</a>: <a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf" target="_blank">Sparse Autoencoder</a> with some personal understanding. Before going through this article, you would better get some information from the <a href="http://cs229.stanford.edu/notes/cs229-notes-deep_learning.pdf" target="_blank">Backpropagation Algorithm</a>.</p>

<p>Suppose we have only unlabeled training examples set $\{x^{(1)},x^{(2)},x^{(3)},\dots\}$, where $x^{(i)}\in\mathbb{R}^{n}$. An <strong>autoencoder neural network</strong> is an <strong>unsupervised learning algorithm</strong> that applies backpropagation, setting the target values to be equal to the inputs. i.e., it uses $y^{(i)}=x^{(i)}$. Below is an autoencoder:
<img src="/img/deeplearning/autoencoder/1.png" alt="1.png" />
The <strong>autoencoder</strong> tries to learn a function $h_{W,b}(x)\approx x$. In other words, it is trying to learn an approximation to the identity function, so as to output $\hat{x}$ that is similar to $x$. The identity function seems a particularly trivial function to be trying to learn; but <strong>by placing constraints on the network</strong>, such as by <strong>limiting the number of hidden units</strong>, we can <strong>discover interesting structure about the data</strong>.</p>

<p>As a concrete example, suppose the inputs $x$ are the pixel intensity values from a $10\times 10$ image (100 pixels) so $n=100$, and there are $s_{2}=50$ hidden units in layer $L_{2}$. Note that we also have $y\in\mathbb{R}^{100}$. Since there are only 50 hidden units, the network is forced to <strong>learn a compressed representation of the input</strong>. That is, given only the vector of hidden unit activations $a^{(2)}\in\mathbb{R}^{50}$, it must try to <strong>reconstruct</strong> the 100-pixel input $x$.</p>

<p>If the input were completely random, say, each $x_{i}$ comes from an I.I.D. (Independently and Identically Distribute) Gaussian independent of the other features, then this compression task would be very difficult. But if there is structure in the data, for example, if some of the input features are correlated, then this algorithm will be able to discover some of those correlations.</p>

<blockquote>
<p>In fact, this simple autoencoder oftern ends up learning a <strong>low-dimensional representation</strong> very similar to <strong>PCA&rsquo;s</strong>.</p>
</blockquote>

<p>Our argument above relied on the number of hidden units $s_{2}$ being small. But even when the number of hidden units is large (perhaps even greater than the number of input pixels), we can still discover interesting structure, by imposing other constraints on the network. In particular, if we impose a <strong>sparsity constraint</strong> on the hidden units, then the autoencoder will still discover interesting structure in the data, even if the number of hidden units is large.</p>

<p>Informally, we will think of a neuron as being &ldquo;active&rdquo; (or as &ldquo;firing&rdquo;) if its output value is close to 1, or as being &ldquo;inactive&rdquo; if its output value is close to 0. We would like to constrain the neurons to be inactive most of the time.</p>

<blockquote>
<p>This discussion assumes a <code>sigmoid</code> activation function. If you are using a <code>tanh</code> activation function, then we think of a neuron as being inactive when it outputs values close to -1.</p>
</blockquote>

<p>Recall that $a_{j}^{(2)}$ denotes the activation of hidden unit $j$ in the autoencoder. However, this notation does not make explicit what was the input $x$ that led to that activation. Thus, we will write $a_{j}^{(2)}(x)$ to denote the activation of this hidden unit when the network is given a specific input $x$. Further, let
$$
\hat{\rho}_{j}=\frac{1}{m}\sum_{i=1}^{m}\big[a_{j}^{(2)}(x^{(i)})\big]\tag{1}
$$
be the <strong>average activation</strong> of hidden unit $j$ (averaged over the training set). We would like to (approximately) enforce the constraint
$$
\hat{\rho}_{j}=\rho\tag{2}
$$
where $\rho$ is a <strong>sparsity parameter</strong>, typically, a small value close to zero (say $\rho=0.05$). In other words, we would like the average activation of each hidden neuron $j$ to be close to 0.05 (say). To satisfy this constraint, the hidden unit&rsquo;s activations must mostly be near 0.</p>

<p>To achieve this, we will add an extra <strong>penalty term</strong> to our <strong>optimization objective</strong> that penalizes $\hat{\rho}_{j}$ deviating significantly from $\rho$. Many choices of the penalty term will give reasonable results. We will choosr the following:
$$
\sum_{j=1}^{s_{2}}\rho\log\frac{\rho}{\hat{\rho}_{j}}+(1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_{j}}\tag{3}
$$
Here, $s_{2}$ is the number of neurons in the hidden layer, and the index $j$ is summing over the hidden units in our network. If you are familiar with the concept of <strong>KL divergence</strong>, this penalty term is based on it, and can also be written
$$
\sum_{j=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{j})\tag{4}
$$
where $KL(\rho\Vert\hat{\rho}_{j})=\rho\log\frac{\rho}{\hat{\rho}_{j}}+(1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_{j}}$ is the <strong>Kullback-Leibler (KL) divergence</strong> between a <strong>Bernoulli random variable</strong> with mean $\rho$ and a <strong>Bernoulli random variable</strong> with mean $\hat{\rho}_{j}$. <strong>KL-divergence is a standard function for measuring how different two different distributions are</strong>.</p>

<p>This penalty function has the property that $KL(\rho\Vert\hat{\rho}_{j})=0$ if $\hat{\rho}_{j}=\rho$, and
otherwise it increases monotonically as $\hat{\rho}_{j}$ diverges from $\rho$. For example, in the figure below, we have set $\rho=0.2$, and plotted $KL(\rho\Vert\hat{\rho}_{j})$ for a range of values of $\hat{\rho}_{j}$
<img src="/img/deeplearning/autoencoder/2.png" alt="2.png" />
We see that the KL-divergence reaches its minimum of 0 at $\hat{\rho}_{j}=\rho$, and blows up (it actually approaches $\infty$) as $\hat{\rho}_{j}$ approaches 0 or 1. <strong>Thus, minimizing this penalty term has the effect of causing $\hat{\rho}_{j}$ to be close to $\rho$</strong>. Our overall cost function is now
$$
J_{sparse}(W,b)=J(W,b)+\beta\sum_{j=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{j})\tag{5}
$$
where $J(W,b)$ is as defined previously, and <strong>$\beta$ controls the weight of the sparsity penalty term</strong>. The term $\hat{\rho}_{j}$ (implicitly) depends on $W$, $b$ also, because it is the average activation of hidden unit $j$, and the activation of a hidden unit depends on the parameters $W$, $b$.</p>

<p>To incorporate the KL-divergence term into your derivative calculation, there is a simple-to-implement trick involving only a small change to your code. Specifically, where previously for the second layer ($l = 2$), during backpropagation you would have computed
$$
\delta_{i}^{(2)}=\bigg(\sum_{j=1}^{s_{2}}W_{ji}^{(2)}\delta_{j}^{(3)}\bigg)f&rsquo;(z_{i}^{(2)})\tag{6}
$$
now instead compute
$$
\delta_{i}^{(2)}=\bigg(\big(\sum_{j=1}^{s_{2}}W_{ji}^{(2)}\delta_{j}^{(3)}\big)+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\bigg)f&rsquo;(z_{i}^{(2)})\tag{7}
$$</p>

<p>Here is how to derive the $\frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}}$ and $\frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}}$</p>

<p>Denote $S(W,b)=\sum_{j=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{j})$, so (5) can be written as
$$
J_{sparse}(W,b)=J(W,b)+\beta S(W,b)\tag{8}
$$T
hen the two partial derivations in gradient descent algorithm are
$$\begin{cases}
\frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}} &amp;=\frac{\partial J(W,b)}{\partial W_{i,j}^{(l)}}+\beta\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}\newline
\frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}} &amp;=\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}+\beta\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}
\end{cases}\tag{9}
$$
The calculation of $\frac{\partial J(W,b)}{\partial W_{i,j}^{(l)}}$ and $\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}$ have discussed in <a href="https://isaacchanghau.github.io/2017/08/17/Backpropagation-Algorithm/" target="_blank">Backpropagation Algorithm</a>, here we only discuss the computation of $\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}$ and $\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}$.</p>

<p>First, expand $S(W,b)$, we have
$$
S(W,b)=\sum_{t=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{t})=\sum_{t=1}^{s_{2}}\big[\rho\ast\log\frac{\rho}{\hat{\rho}_{t}}+(1-\rho)\ast\log\frac{1-\rho}{1-\hat{\rho}_{t}}\big]\tag{10}
$$
where
$$
\hat{\rho}_{t}=\frac{1}{m}\sum_{k=1}^{m}a_{t}^{(2)}\big(x^{(k)}\big)=\frac{1}{m}\sum_{k=1}^{m}f\big(z_{t}^{(2)}\big)\tag{11}
$$
here
$$
z_{t}^{(2)}=z_{t}^{(2)}\big(x^{(k)}\big)=\bigg(\sum_{s=1}^{s_{1}}W_{t,s}^{(1)}x_{s}^{(k)}\bigg)+b_{t}^{(1)}\tag{12}
$$
according to (11) and (12), we can get the following two points:</p>

<ol>
<li><p>when $l\neq 1$, $\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}=0$, $\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}=0$.</p></li>

<li><p>$\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}=\frac{\partial\sum_{t=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{t})}{\partial W_{i,j}^{(l)}}=\frac{\partial KL(\rho\Vert\hat{\rho}_{i})}{\partial W_{i,j}^{(l)}}$; $\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}=\frac{\partial\sum_{t=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{t})}{\partial b_{i}^{(l)}}=\frac{\partial KL(\rho\Vert\hat{\rho}_{i})}{\partial b_{i}^{(l)}}$.</p></li>
</ol>

<p>According to the above two points, we have
$$\begin{aligned}
\frac{\partial S(W,b)}{\partial W_{i,j}^{(1)}} &amp;=\frac{\partial}{\partial W_{i,j}^{(1)}}\big[\rho\ast\log\frac{\rho}{\hat{\rho}_{i}}+(1-\rho)\ast\log\frac{1-\rho}{1-\hat{\rho}_{i}}\big]\newline
&amp;= \frac{\partial}{\partial W_{i,j}^{(1)}}\big\{\rho(\log\rho-\log\hat{\rho}_{i})+(1-\rho)[\log(1-\rho)-\log(1-\hat{\rho}_{i})]\big\}\newline
&amp;=\rho\big(0-\frac{1}{\hat{\rho}_{i}}\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}\big)+(1-\rho)\big(0+\frac{1}{1-\hat{\rho}_{i}}\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}\big)\quad (\rho\textrm{ is constant})\newline
&amp;=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}
\end{aligned}
$$
Samely, we have
$$
\frac{\partial S(W,b)}{\partial b_{i}^{(1)}}=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\frac{\partial\hat{\rho}_{i}}{\partial b_{i}^{(1)}}
$$
Then, we need to compute $\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}$ and $\frac{\partial\hat{\rho}_{i}}{\partial b_{i}^{(1)}}$, according to (11), we have
$$
\begin{aligned}
\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}f&rsquo;\big(z_{i}^{(2)}\big)\cdot\frac{\partial z_{i}^{(2)}}{\partial W_{i,j}^{(1)}}\newline
&amp;=\frac{1}{m}\sum_{k=1}^{m}f&rsquo;\big(z_{i}^{(2)}\big)\cdot x_{j}^{(k)}\quad (\textrm{according to (12), we have }\frac{\partial z_{i}^{(2)}}{\partial W_{i,j}^{(1)}}=x_{j}^{(k)})
\end{aligned}
$$
Samely, we have
$$
\begin{aligned}
\frac{\partial\hat{\rho}_{i}}{\partial b_{i}^{(1)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}f&rsquo;\big(z_{i}^{(2)}\big)\cdot\frac{\partial z_{i}^{(2)}}{\partial b_{i}^{(1)}}\newline
&amp;=\frac{1}{m}\sum_{k=1}^{m}f&rsquo;\big(z_{i}^{(2)}\big)\quad (\textrm{according to (12), we have }\frac{\partial z_{i}^{(2)}}{\partial b_{i}^{(1)}}=1)
\end{aligned}
$$
In summary, we can obtain
$$\begin{cases}
\frac{\partial S(W,b)}{\partial W_{i,j}^{(1)}} &amp;=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\cdot\frac{1}{m}\sum_{k=1}^{m}f&rsquo;\big(z_{i}^{(2)}\big)\cdot x_{j}^{(k)}\newline
\frac{\partial S(W,b)}{\partial b_{i}^{(1)}}&amp;=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\cdot\frac{1}{m}\sum_{k=1}^{m}f&rsquo;\big(z_{i}^{(2)}\big)
\end{cases}\tag{13}
$$
In <a href="https://isaacchanghau.github.io/2017/08/17/Backpropagation-Algorithm/" target="_blank">Backpropagation Algorithm</a>, we already obtain that
$$\begin{cases}
\frac{\partial J(W,b)}{\partial W_{i,j}^{(1)}}&amp;=\big[\frac{1}{m}\sum_{k=1}^{m}a_{j}^{(1)}\delta_{i}^{(2)}\big]+\lambda W_{i,j}^{(1)}\newline
\frac{\partial J(W,b)}{\partial b_{i}^{(1)}}&amp;=\frac{1}{m}\sum_{k=1}^{m}\delta_{i}^{(2)}
\end{cases}\tag{14}
$$
Thus, put (13), (14) into (9), we will get
$$\begin{cases}
\frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}} &amp;=\big[\frac{1}{m}\sum_{k=1}^{m}a_{j}^{(1)}\delta_{i}^{(2)}\big]+\lambda W_{i,j}^{(1)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\cdot\frac{1}{m}\sum_{k=1}^{m}f&rsquo;\big(z_{i}^{(2)}\big)\cdot x_{j}^{(k)}\newline
\frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}\delta_{i}^{(2)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\cdot\frac{1}{m}\sum_{k=1}^{m}f&rsquo;\big(z_{i}^{(2)}\big)
\end{cases}\tag{15}
$$
Note that $a_{j}^{(1)}$, $\delta_{i}^{(2)}$ and $z_{i}^{(2)}$ are related to $x^{(k)}$ (or $y^{(k)}$), particularly, we have $a_{j}^{(1)}=x_{j}^{(k)}$. After simplify (15), we get
$$\begin{cases}
\frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}\bigg\{a_{j}^{(1)}\big[\delta_{i}^{(2)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)f&rsquo;\big(z_{i}^{(2)}\big)\big]\bigg\}+\lambda W_{i,j}^{(1)}\newline
\frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}\big[\delta_{i}^{(2)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)f&rsquo;\big(z_{i}^{(2)}\big)\big]
\end{cases}$$
Now, we obtain how to change (6) to (7).</p>

<p>One subtlety is that you’ll need to know $\hat{\rho}_{i}$ to compute this term. Thus, you’ll need to compute a forward pass on all the training examples first to compute the average activations on the training set, before computing backpropagation on any example. If your training set is small enough to fit comfortably in computer memory (this will be the case for the programming assignment), you can compute forward passes on all your examples and keep the resulting activations in memory and compute the $\hat{\rho}_{i}$s. Then you can use your precomputed activations to perform backpropagation on all your examples. <strong>If your data is too large to fit in memory, you may have to scan through your examples computing a forward pass on each to accumulate (sum up) the activations and compute $\hat{\rho}_{i}$</strong> (discarding the result of each forward pass after you have taken its activations $a_{i}^{(2)}$ into account for computing $\hat{\rho}_{i}$). Then after having computed $\hat{\rho}_{i}$, you’d have to redo the forward pass for each example so that you can do backpropagation on that example. <strong>In this latter case, you would end up computing a forward pass twice on each example in your training set, making it computationally less efficient</strong>.</p>

<p>The full derivation showing that the algorithm above results in gradient descent is beyond the scope of these notes. But if you implement the autoencoder using backpropagation modified this way, you will be performing gradient descent exactly on the objective $J_{sparse}(W,b)$. Using the derivative checking method, you will be able to verify this for yourself as well.</p>

<p>Below is the <strong>summary of notation</strong>
<style>
table th:first-of-type {
    width: 70px;
}
</style></p>

<table>
<thead>
<tr>
<th align="center">Expression</th>
<th align="left">Explanation</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">$x$</td>
<td align="left">Input features for a training example, $x\in\mathbb{R}^{n}$.</td>
</tr>

<tr>
<td align="center">$y$</td>
<td align="left">Output/target values. Here, $y$ can be vector valued. In the case of an autoencoder, $y = x$.</td>
</tr>

<tr>
<td align="center">$(x^{(i)},y^{(i)})$</td>
<td align="left">The $i$-th training example.</td>
</tr>

<tr>
<td align="center">$h_{W,b}(x)$</td>
<td align="left">Output of our hypothesis on input $x$, using parameters $W$, $b$. This should be a vector of the same dimension as the target value $y$.</td>
</tr>

<tr>
<td align="center">$W_{i,j}^{(l)}$</td>
<td align="left">The parameter associated with the connection between unit $j$ in layer $l$, and unit $i$ in layer $l+1$.</td>
</tr>

<tr>
<td align="center">$b_{i}^{(l)}$</td>
<td align="left">The bias term associated with unit $i$ in layer $l+1$. Can also be thought of as the parameter associated with the connection between the bias unit in layer $l$ and unit $i$ in layer $l+1$.</td>
</tr>

<tr>
<td align="center">$\theta$</td>
<td align="left">Our parameter vector. It is useful to think of this as the result of taking the parameters $W$, $b$ and “unrolling” them into a long column vector.</td>
</tr>

<tr>
<td align="center">$a_{i}^{(l)}$</td>
<td align="left">Activation (output) of unit $i$ in layer $l$ of the network. In addition, since layer $L_{1}$ is the input layer, we also have $a_{i}^{(1)}=x_{i}$.</td>
</tr>

<tr>
<td align="center">$f(\cdot)$</td>
<td align="left">The activation function. Throughout these notes, we used $f(z)=tanh(z)$.</td>
</tr>

<tr>
<td align="center">$z_{i}^{(l)}$</td>
<td align="left">Total weighted sum of inputs to unit $i$ in layer $l$. Thus, $a_{i}^{(l)}=f\big(z_{i}^{(l)}\big)$</td>
</tr>

<tr>
<td align="center">$\alpha$</td>
<td align="left">Learning rate parameter.</td>
</tr>

<tr>
<td align="center">$s_{l}$</td>
<td align="left">Number of units in layer $l$ (not counting the bias unit).</td>
</tr>

<tr>
<td align="center">$n_{l}$</td>
<td align="left">Number layers in the network. Layer $L_{1}$ is usually the input layer, and layer $L_{n_{l}}$ the output layer.</td>
</tr>

<tr>
<td align="center">$\lambda$</td>
<td align="left">Weight decay parameter.</td>
</tr>

<tr>
<td align="center">$\hat{x}$</td>
<td align="left">For an autoencoder, its output; i.e., its reconstruction of the input $x$. Same meaning as $h_{W,b}(x)$.</td>
</tr>

<tr>
<td align="center">$\rho$</td>
<td align="left">Sparsity parameter, which specifies our desired level of sparsity.</td>
</tr>

<tr>
<td align="center">$\hat{\rho}_{i}$</td>
<td align="left">The average activation of hidden unit $i$ (in the sparse autoencoder).</td>
</tr>

<tr>
<td align="center">$\beta$</td>
<td align="left">Weight of the sparsity penalty term (in the sparse autoencoder objective).</td>
</tr>
</tbody>
</table>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/deep-learning">deep-learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/machine-learning">machine-learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/algorithms">algorithms</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/autoencoder">autoencoder</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/backpropagation/">Backpropagation in Neural Networks</a></li>
        
        <li><a href="/post/stock_price_predict/">Plain Stock Price Prediction via LSTM</a></li>
        
        <li><a href="/post/lstm-gru-formula/">LSTM and GRU -- Formula Summary</a></li>
        
        <li><a href="/post/understand_lstm/">Understanding LSTM Networks</a></li>
        
        <li><a href="/post/loss_functions/">Loss Functions in Neural Networks</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2018 &middot; 

      <strong>Isaac Changhau (Zhang Hao)</strong>
      

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/java.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/javascript.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/c.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/html.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/xml.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ 
          tex2jax: { 
            inlineMath: [['$','$'], ['\\(','\\)']] 
          } 
        });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-xAWI9i8WMRLdgksuhaMCYMTw9D+MEc2cYVBApWwGRJ0cdcywTjMovOfJnlGt9LlEQj6QzyMzpIZLMYujetPcQg==" crossorigin="anonymous"></script>
    
    
    

  </body>
</html>

