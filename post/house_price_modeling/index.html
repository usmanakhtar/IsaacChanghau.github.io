<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.41" />
  <meta name="author" content="Zhang Hao">

  
  
  
  
    
      
    
  
  <meta name="description" content="After cleaning and transforming processes in House Prices Advanced Regression Techniques &ndash; Data Analysis. Here we continue to build machine learning model to train the dataset and make a prediction based on the trained model. We use Elastic Net and Gradient Boosting models to train the dataset and make predictions separately, then average the results of the two models to generate the final output.
Elastic Net Regression In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.">

  
  <link rel="alternate" hreflang="en-us" href="https://isaacchanghau.github.io/post/house_price_modeling/">

  


  

  
  
  <meta name="theme-color" content="#3f51b5">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  <link rel="feed" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://isaacchanghau.github.io/post/house_price_modeling/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/IsaacChanghau">
  <meta property="twitter:creator" content="@https://twitter.com/IsaacChanghau">
  
  <meta property="og:site_name" content="Isaac Changhau">
  <meta property="og:url" content="https://isaacchanghau.github.io/post/house_price_modeling/">
  <meta property="og:title" content="House Prices Advanced Regression Techniques -- Modeling and Prediction | Isaac Changhau">
  <meta property="og:description" content="After cleaning and transforming processes in House Prices Advanced Regression Techniques &ndash; Data Analysis. Here we continue to build machine learning model to train the dataset and make a prediction based on the trained model. We use Elastic Net and Gradient Boosting models to train the dataset and make predictions separately, then average the results of the two models to generate the final output.
Elastic Net Regression In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-07-10T00:00:00&#43;08:00">
  
  <meta property="article:modified_time" content="2017-07-10T00:00:00&#43;08:00">
  

  

  <title>House Prices Advanced Regression Techniques -- Modeling and Prediction | Isaac Changhau</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Isaac Changhau</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#tags">
            
            <span>Tags</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#achievements">
            
            <span>Achievements</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">House Prices Advanced Regression Techniques -- Modeling and Prediction</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2017-07-10 00:00:00 &#43;0800 &#43;08" itemprop="datePublished dateModified">
      Mon, Jul 10, 2017
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhang Hao">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  

  

  
  

  

</div>

    
    <div class="article-style" itemprop="articleBody">
      

<p>After cleaning and transforming processes in <strong>House Prices Advanced Regression Techniques &ndash; Data Analysis</strong>. Here we continue to build machine learning model to train the dataset and make a prediction based on the trained model. We use <strong>Elastic Net</strong> and <strong>Gradient Boosting</strong> models to train the dataset and make predictions separately, then average the results of the two models to generate the final output.</p>

<h1 id="elastic-net-regression">Elastic Net Regression</h1>

<p>In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the <a href="https://en.wikipedia.org/wiki/Lasso_%28statistics%29" target="_blank">lasso</a> and <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization" target="_blank">ridge</a> methods.</p>

<p>The target function of Lasso is given as:
$$\min_{\mathbf{w}}\frac{1}{2n}\Vert\mathbf{X}^{T}\mathbf{w}-\mathbf{y}\Vert_{2}^{2}+\alpha\Vert\mathbf{w}\Vert_{1}
$$
where $n$ is number of samples, $\mathbf{w}$ is the coefficient parameters to learn, $\Vert\mathbf{w}\Vert_{1}$ term is L1 penality, which promotes sparsity, reduces the redundancy and improves the accurancy and robustness of regression (it alleviates the overfitting in some degree). Also if there is a group of highly correlated variables, then the Lasso tends to select one variable from a group and ignore the others. However, the L1 norm in Lass has some limitations that, for example, in the &ldquo;large $p$ and small $n$&rdquo; case (high-dimensional data with few examples), the Lasso selects at most n variables before it saturates.</p>

<p>While the target function of Ridge is computed by:
$$
\min_{\mathbf{w}}\frac{1}{2}\Vert\mathbf{X}^{T}\mathbf{w}-\mathbf{y}\Vert_{2}^{2}+\frac{\alpha}{2}\Vert\mathbf{w}\Vert_{2}
$$
where $\Vert\mathbf{w}\Vert_{2}^{2}$ term is L2 penality, which constrains the module of $\mathbf{w}$ in to a L2 ball to alleviate the overfitting problem. However, L2 norm shrinkages the value of approximated parameters, but does not make it zero, which means it does not perform the function of parameter selection.</p>

<p>For Elastic Net, it can be treated as a compromise between Lasso and Ridge, since it integrates the L1 and L2 regularizations. Its target function is described as:
$$
\min_{\mathbf{w}}\frac{1}{2n}\Vert\mathbf{X}^{T}\mathbf{w}-\mathbf{y}\Vert_{2}^{2}+\alpha\rho\Vert\mathbf{w}\Vert_{1}+\frac{\alpha(1-\rho)}{2}\Vert\mathbf{w}\Vert_{2}^{2}
$$
where $\rho$ is an adaptive hyper-parameter to control the contributions of L1 norm and L2 norm. With this compromise, Elastic Net remains part of the parameter selection function in Lasso and part of rotary stability property in Ridge. Meanwhile, compare to Lasso, Elastic Net not only randomly select one of the correlated variables, but also tends to obtain all of them and enahnces their group effect.</p>

<h1 id="gradient-boosting-regression">Gradient Boosting Regression</h1>

<p>Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.</p>

<p>Like other boosting methods, gradient boosting combines weak &ldquo;learners&rdquo; into a single strong learner in an iterative fashion. It is easiest to explain in the least-squares regression setting, where the goal is to &ldquo;teach&rdquo; a model $F$ to predict values in the form $\hat{y} = F(\mathbf{x})$ by minimizing the mean squared error $(\hat{y} - y)^{2}$, averaged over some training set of actual values of the output variable $y$.</p>

<p>Given a training dataset $\{(\mathbf{x}_{1},y_{1}),(\mathbf{x}_{2},y_{2}),\dots,(\mathbf{x}_{n},y_{n})\}$, the goal is to find an approximation $\hat{F}(\mathbf{x})$ to a function $F(\mathbf{x})$ that minimizes the expected value of some specified loss function $\mathcal{L}(y, F(x))$:
$$
\hat{F}=\arg\min_{F}\mathbb{E}_{\mathbf{x},y}\big[\mathcal{L}(y, F(\mathbf{x}))\big]
$$
The gradient boosting method assumes a real-valued $y$ and seeks an approximation $\hat{F}(\mathbf{x})$ in the form of a weighted sum of functions $h_{i}(\mathbf{x})$ from some class $\mathcal{H}$, called base (or weak) learners:
$$
F(\mathbf{x})=\sum_{i=1}^{M}\gamma_{i}h_{i}(\mathbf{x})+const.
$$
In accordance with the empirical risk minimization principle, the method tries to find an approximation $\hat{F}(\mathbf{x})$ that minimizes the average value of the loss function on the training set. It does so by starting with a model, consisting of a constant function $F_{0}(\mathbf{x})$, and incrementally expanding it in a greedy fashion:
$$
\begin{aligned}
F_{0}(\mathbf{x}) &amp; =\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}(y_{i},\gamma)\newline
F_{m}(\mathbf{x}) &amp; =F_{m-1}(\mathbf{x})+\arg\min_{h\in\mathcal{H}}\sum_{i=1}^{n}\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i})+h(\mathbf{x}_{i}))
\end{aligned}
$$
where $h\in\mathcal{H}$ is a base learner function.</p>

<p>Unfortunately, choosing the best function $h$ at each step for an arbitrary loss function $\mathcal{L}$ is a computationally infeasible optimization problem in general. Therefore, we will restrict to a simplification.
The idea is to apply a steepest descent step to this minimization problem. If we considered the continuous case, i.e. $\mathcal{H}$ the set of arbitrary differentiable functions on $\mathbb{R}$, we would update the model in accordance with the following equations:
$$
\begin{aligned}
F_{m}(\mathbf{x}) &amp; =F_{m-1}(\mathbf{x})-\gamma_{m}\sum_{i=1}^{n}\nabla_{F_{m-1}}\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i}))\newline
\gamma_{m} &amp; =\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}\bigg(y_{i},F_{m-1}(\mathbf{x}_{i})-\gamma\frac{\partial\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i}))}{\partial F_{m-1}(\mathbf{x}_{i})}\bigg)
\end{aligned}
$$
where the derivatives are taken with respect to the functions $F_{i}$ for $i\in\{1,\dots,m\}$. In the discrete case, however, i.e. the set $\mathcal{H}$ is finite, we will choose the candidate function $h$ closest to the gradient of $\mathcal{L}$ for which the coefficient $\gamma$ may then be calculated with the aid of line search the above equations. Note that this approach is a heuristic and will therefore not yield an exact solution to the given problem, yet a satisfactory approximation.</p>

<p>In pseudocode, the generic gradient boosting method is:</p>

<ol>
<li>Input: training dataset $\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}$, a differentiable loss function $\mathcal{L}(y,F(\mathbf{x}))$, number of iterations $M$.</li>
<li>Initialize model with a constant value: $F_{0}(\mathbf{x})=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}(y_{i},\gamma)$.</li>
<li>For $m=1$ to $M$:

<ol>
<li>Compute so-called pseudo-residuals: $r_{im}=-\big[\frac{\partial\mathcal{L}(y_{i},F(\mathbf{x}_{i}))}{\partial F(\mathbf{x}_{i})}\big]_{F(\mathbf{x})=F_{m-1}(\mathbf{x})}$, for $i=1,\dots,n$.</li>
<li>Fit a base learner (e.g. tree) $h_{m}(\mathbf{x})$ to pseudo-residuals, i.e., train it using the training dataset $\{\mathbf{x}_{i},r_{im}\}_{i=1}^{n}$.</li>
<li>Compute multiplier $\gamma_{m}$ by solving <a href="https://en.wikipedia.org/wiki/Line_search" target="_blank">one-dimensional optimization</a> problem: $\gamma_{m}=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i})+\gamma h_{m}(\mathbf{x}_{i}))$.</li>
<li>Update the model: $F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+\gamma_{m}h_{m}(\mathbf{x})$.</li>
</ol></li>
<li>Output $F_{M}(\mathbf{x})$.</li>
</ol>

<p>For the following description, we assume that the dataset is already cleaned and prepared in <strong>House Prices Advanced Regression Techniques &ndash; Data Analysis</strong></p>

<h1 id="model-construction-training-and-prediction">Model Construction, Training and Prediction</h1>

<h2 id="shuffling-and-splitting-data">Shuffling and Splitting Data</h2>

<pre><code class="language-python"># Shuffling train sets
train_features_st, train_features, train_labels = shuffle(train_features_st, train_features, train_labels, random_state=5)
# Splitting
x_train, x_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.1, random_state=200)
x_train_st, x_test_st, y_train_st, y_test_st = train_test_split(train_features_st, train_labels, test_size=0.1, random_state=200)
</code></pre>

<p>where <code>**_features_st</code> dataset is used for training Elastic Net, while <code>**_features</code> dataset is used for training Gradient Boosting Regressor.</p>

<p><strong>Define two functions to show R2 and RMSE scores for train and validation sets</strong></p>

<pre><code class="language-python">from sklearn.metrics import r2_score, mean_squared_error
import numpy as np
# Prints R2 and RMSE scores
def get_score(prediction, labels):
    print('R2: {}'.format(r2_score(prediction, labels)))
    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction, labels))))
# Shows scores for train and validation sets
def train_test(estimator, x_trn, x_tst, y_trn, y_tst):
    prediction_train = estimator.predict(x_trn)
    # Printing estimator
    print(estimator)
    # Printing train scores
    get_score(prediction_train, y_trn)
    prediction_test = estimator.predict(x_tst)
    # Printing test scores
    print(&quot;Test&quot;)
    get_score(prediction_test, y_tst)
</code></pre>

<h2 id="model-construction">Model Construction</h2>

<p>For Elastic Net, we use cross validation method to select the best parameters group for a given parameters map.</p>

<pre><code class="language-python">ens_test = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000).fit(x_train_st, y_train_st)
train_test(ens_test, x_train_st, x_test_st, y_train_st, y_test_st)
# Average R2 score and standard deviation of 5-fold cross-validation
scores = cross_val_score(ens_test, train_features_st, train_labels, cv=5)
print(&quot;Accuracy: %0.2f (+/- %0.2f)&quot; % (scores.mean(), scores.std() * 2))
</code></pre>

<p>Here the $\alpha$ is given as <code>alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10]</code>, $\rho$ is given as <code>l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99]</code>, and the maximal iterations is set as $5000$, $K$ of cross validation is set as $5$.
For Gradient Boosting Regressor, the cross validation technique is also used, parameters here are fixed and $K$ of cross validation is set as $5$.</p>

<pre><code class="language-python">g_best = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=3, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber').fit(x_train, y_train)
train_test(g_best, x_train, x_test, y_train, y_test)
# Average R2 score and standard deviation of 5-fold cross-validation
scores = cross_val_score(g_best, train_features_st, train_labels, cv=5)
print(&quot;Accuracy: %0.2f (+/- %0.2f)&quot; % (scores.mean(), scores.std() * 2))
</code></pre>

<p>After cross validation and obtaining the best model of Elastic Net and Gradient Boosting Regressor, we need to retraining the model:</p>

<pre><code class="language-python"># Retraining models
gb_model = g_best.fit(train_features, train_labels)
enst_model = ens_test.fit(train_features_st, train_labels)
</code></pre>

<p>Then get the predictions and save to file</p>

<pre><code class="language-python"># Getting our SalePrice estimation
final_labels = (np.exp(gb_model.predict(test_features)) + np.exp(enst_model.predict(test_features_st))) / 2
# print result
output = pd.DataFrame({'Id': test.Id, 'SalePrice': final_labels})
print(output)
# Saving to CSV
output.to_csv('submission.csv', index=False)
</code></pre>

<p>Below shows some information in training process and the prediction results:</p>

<pre><code class="language-bash">ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], copy_X=True,
       cv=None, eps=0.001, fit_intercept=True,
       l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000, n_alphas=100,
       n_jobs=1, normalize=False, positive=False, precompute='auto',
       random_state=None, selection='cyclic', tol=0.0001, verbose=0)
R2: 0.9009282706669409
RMSE: 0.1192142029440696
Test
R2: 0.8967299864999421
RMSE: 0.11097041288345283
Accuracy: 0.88 (+/- 0.10)
GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.05, loss='huber', max_depth=3,
             max_features='sqrt', max_leaf_nodes=None,
             min_impurity_split=1e-07, min_samples_leaf=15,
             min_samples_split=10, min_weight_fraction_leaf=0.0,
             n_estimators=3000, presort='auto', random_state=None,
             subsample=1.0, verbose=0, warm_start=False)
R2: 0.9617959062813555
RMSE: 0.07593410094831428
Test
R2: 0.9062977017781593
RMSE: 0.10586499921275429
Accuracy: 0.90 (+/- 0.04)

Predictions:
        Id      SalePrice
0     1461  119290.186865
1     1462  152342.289928
2     1463  180487.561653
3     1464  201057.891220
...    ...            ...
1456  2917  157280.476185
1457  2918  119939.102469
1458  2919  221954.591126

[1459 rows x 2 columns]
</code></pre>

<h1 id="reference">Reference</h1>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Elastic_net_regularization" target="_blank">Elastic Net Regularization</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html" target="_blank">Elastic Net in sklearn</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html" target="_blank">Cross Validation of Elastic Net in sklearn</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank">Gradient Boosting</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" target="_blank">Gradient Boosting Regressor in sklearn</a></li>
</ul>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/machine-learning">machine-learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/python">python</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/house_price_data_analysis/">House Prices Advanced Regression Techniques -- Data Analysis</a></li>
        
        <li><a href="/post/ml_zzh_note_4/">Machine Learning Note (4): Ensemble Learning</a></li>
        
        <li><a href="/post/understand_lstm/">Understanding LSTM Networks</a></li>
        
        <li><a href="/post/loss_functions/">Loss Functions in Neural Networks</a></li>
        
        <li><a href="/post/parameters_update/">Parameter Update Methods in Neural Networks</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2018 &middot; 

      <strong>Isaac Changhau (Zhang Hao)</strong>
      

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/java.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/javascript.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/c.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/html.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/xml.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ 
          tex2jax: { 
            inlineMath: [['$','$'], ['\\(','\\)']] 
          } 
        });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-xAWI9i8WMRLdgksuhaMCYMTw9D+MEc2cYVBApWwGRJ0cdcywTjMovOfJnlGt9LlEQj6QzyMzpIZLMYujetPcQg==" crossorigin="anonymous"></script>
    
    
    

  </body>
</html>

