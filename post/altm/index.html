<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.45" />
  <meta name="author" content="Zhang Hao">

  
  
  
  
    
      
    
  
  <meta name="description" content="This short paper, published by Ahn et al. on ICCE, introduces a new tone mapping technique for HDR images based on the retinex theory. Here I summarize this paper and show the Java implementation with OpenCV to realize the proposed algorithm.
Center/Surround Retinex The retinex theory, initially defined by Land et al., explains how the realiable color information from the real world is extracted by human visual system, and Jobson et al.">

  
  <link rel="alternate" hreflang="en-us" href="https://isaacchanghau.github.io/post/altm/">

  


  

  
  
  <meta name="theme-color" content="#3f51b5">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  <link rel="feed" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://isaacchanghau.github.io/post/altm/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/IsaacChanghau">
  <meta property="twitter:creator" content="@https://twitter.com/IsaacChanghau">
  
  <meta property="og:site_name" content="Isaac Changhau">
  <meta property="og:url" content="https://isaacchanghau.github.io/post/altm/">
  <meta property="og:title" content="Adaptive Local Tone Mapping Technique for HDR Image and Java Implementation | Isaac Changhau">
  <meta property="og:description" content="This short paper, published by Ahn et al. on ICCE, introduces a new tone mapping technique for HDR images based on the retinex theory. Here I summarize this paper and show the Java implementation with OpenCV to realize the proposed algorithm.
Center/Surround Retinex The retinex theory, initially defined by Land et al., explains how the realiable color information from the real world is extracted by human visual system, and Jobson et al.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-04-11T00:00:00&#43;08:00">
  
  <meta property="article:modified_time" content="2017-04-11T00:00:00&#43;08:00">
  

  

  <title>Adaptive Local Tone Mapping Technique for HDR Image and Java Implementation | Isaac Changhau</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Isaac Changhau</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#tags">
            
            <span>Tags</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#achievements">
            
            <span>Achievements</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Adaptive Local Tone Mapping Technique for HDR Image and Java Implementation</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2017-04-11 00:00:00 &#43;0800 &#43;08" itemprop="datePublished dateModified">
      Tue, Apr 11, 2017
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhang Hao">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  

  

  
  

  

</div>

    
    <div class="article-style" itemprop="articleBody">
      

<p>This short paper, published by Ahn et al. on ICCE, introduces a new tone mapping technique for HDR images based on the retinex theory. Here I summarize this paper and show the Java implementation with OpenCV to realize the proposed algorithm.</p>

<h1 id="center-surround-retinex">Center/Surround Retinex</h1>

<p>The <a href="https://en.wikipedia.org/wiki/Color_constancy#Retinex_theory" target="_blank">retinex theory</a>, initially defined by <a href="https://www.osapublishing.org/josa/abstract.cfm?uri=josa-61-1-1" target="_blank">Land et al.</a>, explains how the realiable color information from the real world is extracted by human visual system, and <em>Jobson et al.</em> introduced the single-scale retinex (<a href="http://www.ipol.im/pub/art/2014/107/article_lr.pdf" target="_blank">SSR</a>) and multi-scale retinex (<a href="http://www.ipol.im/pub/art/2014/107/article_lr.pdf" target="_blank">MSR</a>) based on Land&rsquo;s surround retinex theory. Generally, SSR is computed by
$$
R_{i}(x,y)=\log\big(I_{i}(x,y)\big)-\log\big(F(x,y)*I_{i}(x,y)\big)
$$
where $x$, $y$ are the pixel coordinates in the image, $R_{i}(x,y)$ denotes the retinex output of $i^{th}$ spectral band, $I_{i}(x,y)$ is the image distribution in the $i^{th}$ spectral band, $*$ represents convolution operation, and $F(x,y)$ is the Gaussian surround function, defined by
$$
F(x,y)=K\cdot e^{-\frac{x^{2}+y^{2}}{c^{2}}}
$$
where $c$ is Gaussian surround space constant, $K$ is normalization factor. In Addition, MSR is described as
$$
R_{MSR_{i}}(x,y)=\sum_{n=1}^{N}\omega_{n}\cdot R_{n_{i}}(x,y)
$$
where $N$ is the number of scales, $R_{n_{i}}(x,y)$ denotes the $i^{th}$ component of the $n^{th}$ scale, $R_{MSR_{i}}(x,y)$ denotes $i^{th}$ spectral component of the MSR output, and $\omega_{n}$ is the weight associated with the $n^{th}$ scale.</p>

<p>The purpose of MSR is to reduce halo artifacts around high contrast edges and to keep balance with the dynamic range compression and the color rendition. MSR produces good dynamic range compression, but still suffer from halo artifacts. In addition, SSR with small space constant makes large uniform regions graying out and flat-looking in images.</p>

<h1 id="adaptive-local-tone-mapping">Adaptive Local Tone Mapping</h1>

<p>For ALTM, there are two processes, one is applying a global tone mapping, and another is a local tone mapping based on retinex algorithm.</p>

<h2 id="global-adaptation">Global Adaptation</h2>

<p>Global adaptation simulates the early stage of human visual system, which senses brightness as an approximate logarithmic function according to the <a href="https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law" target="_blank">Weber-Fechner law</a>. The global adaptation is defined by
$$
L_{g}(x,y)=\frac{\log\big(\frac{L_{w}(x,y)}{\bar{L}_{w}}+1\big)}{\log\big(\frac{L_{w\max}}{\bar{L}_{w}}+1\big)}
$$
where $L_{g}(x,y)$ is global adaptation output, $L_{w}(x,y)$ is input luminance values, $L_{w\max}$ is maximum luminance value, $\bar{L}_{w}$ is the logarithmic average luminance and given as
$$
\bar{L}_{w}=e^{\frac{1}{m}\sum_{x,y}\log\big(\delta+L_{w}(x,y)\big)}
$$
where $m$ is the total number of pixels of one channel in the image and $\delta$ is a small value to prevent the singularity that occurs if black pixels are present. Moreover, the input world luminance values $L_{w}(x,y)$ is derived by
$$
L_{w}(x,y)=0.299\cdot Red(x,y)+0.587\cdot Green(x,y)+0.114\cdot Blue(x,y)
$$
according to the paper, and $Red$, $Green$ and $Blue$ denote the three color channels of the input image respectivaly. Below shows the Java implementation with OpenCV to derive the global adaptation:</p>

<pre><code class="language-java">/**
 * Global Adaptation
 * @param b blue channel of input image
 * @param g green channel of input image
 * @param r red channel of input image
 * @param rows number of rows of the input image
 * @param cols number of cloumns of the input image
 * @return a Mat list contains global adaptation output (Lg) and input world luminance values (Lw)
 */
private static List&lt;Mat&gt; globalAdaptation(Mat b, Mat g, Mat r, int rows, int cols) {
    // Calculate Lw &amp; maximum of Lw
    Mat Lw = new Mat(rows, cols, r.type());
    Core.multiply(r, new Scalar(rParam), r); // rParam = 0.299
    Core.multiply(g, new Scalar(gParam), g); // gParam = 0.587
    Core.multiply(b, new Scalar(bParam), b); // bParam = 0.114
    Core.add(r, g, Lw);
    Core.add(Lw, b, Lw);
    double LwMax = Core.minMaxLoc(Lw).maxVal; // the maximum luminance value
    // Calculate log-average luminance and get global adaptation result
    Mat Lw_ = Lw.clone();
    Core.add(Lw_, new Scalar(0.001), Lw_);
    Core.log(Lw_, Lw_);
    double LwAver = Math.exp(Core.sumElems(Lw_).val[0] / (rows * cols));
    Mat Lg = Lw.clone();
    Core.divide(Lg, new Scalar(LwAver), Lg);
    Core.add(Lg, new Scalar(1.0), Lg);
    Core.log(Lg, Lg);
    Core.divide(Lg, new Scalar(Math.log(LwMax / LwAver + 1.0)), Lg); // Lg is the global adaptation
    List&lt;Mat&gt; list = new ArrayList&lt;&gt;();
    list.add(Lw);
    list.add(Lg);
    return list;
}
</code></pre>

<h2 id="local-adaptation">Local Adaptation</h2>

<p>Local adaptation based on the retinex theory is applied after the global adaptation process. The output of SSR (and MSR) suffers strong halo artifacts and looks unnatural. In order to deal with these drawbacks, an edge-preserving filter, <a href="http://kaiminghe.com/eccv10/" target="_blank">guided filter</a> is used in this paper, is intorduced to substitute the Gaussian filter of the retinex algorithm to reduce the halo artifact effects. The guided filter is an edge-preserving filter like the <a href="https://www.google.com.sg/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwivkrb477fUAhWFrY8KHVOPBl4QFgggMAA&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FBilateral_filter&amp;usg=AFQjCNGR1HnnDTVK6h4lLUSHtoOYCpEsAA" target="_blank">bilateral filter</a> whose weights depend not only on the Euclidean distances but also on the luminance differences. These filters behave similar, but the guided filter has better performance near the edges. Also, its computational complexity is linear-time without approximation and independent of the kernel size. The local adaptation equation can be written as
$$
L_{l}(x,y)=\log L_{g}(x,y)-\log H_{g}(x,y)
$$
where $L_{l}(x,y)$ denotes the local adaptation output, and $H_{g}(x,y)$ denotes the output of the guided filter global adaptation result. Mathematically, the guided filter is defined as
$$
H_{g}(x,y)=\frac{1}{|\omega|}\sum_{(\zeta_{x},\zeta_{y})\in\omega(x,y)}\big(a(\zeta_{x},\zeta_{y})\cdot L_{g}(x,y)+b(\zeta_{x},\zeta_{y})\big)
$$
where $\zeta_{x}$, $\zeta_{y}$ are the neighborhood pixel coordinates, $\omega(x,y)$ is a local square window of a radius $r$ centered at pixel $(x,y)$, $|\omega|$ represents the number of pixels in $\omega(x,y)$, while $a(\zeta_{x},\zeta_{y})$ and $b(\zeta_{x},\zeta_{y})$ are linear coefficients, which are computed by
$$
\begin{aligned}
a(\zeta_{x},\zeta_{y}) &amp; =\frac{\mu_{2}(\zeta_{x},\zeta_{y})-\mu^{2}(\zeta_{x},\zeta_{y})}{\sigma^{2}(\zeta_{x},\zeta_{y})+\varepsilon}\newline
b(\zeta_{x},\zeta_{y}) &amp; =\mu(\zeta_{x},\zeta_{y})-a(\zeta_{x},\zeta_{y})\cdot\mu(\zeta_{x},\zeta_{y})
\end{aligned}
$$
where $\mu(\zeta_{x},\zeta_{y})$ and $\sigma^{2}(\zeta_{x},\zeta_{y})$ are the mean and variance of $L_{g}$ in $\omega(\zeta_{x},\zeta_{y})$, $\mu_{2}(\zeta_{x},\zeta_{y})$ is the mean of $L_{g}^{2}$ in $\omega(\zeta_{x},\zeta_{y})$, and $\varepsilon$ is a regularization parameter. Below is the Java codes to obtain the $H_{g}$, and the implementation details of guided filter can be found in my GitHub repository: <a href="https://github.com/IsaacChanghau/ALTMRetinex/blob/master/src/main/java/com/isaac/utils/Filters.java" target="_blank">[link]</a>.</p>

<pre><code class="language-java">/**
 * Local Adaptation (Guided Image Filter Result)
 * @param Lg global adaptation output
 * @param rows number of rows of the input image
 * @param cols number of cloumns of the input image
 * @return local adaptation result
 */
private static Mat localAdaptation(Mat Lg, int rows, int cols) {
    int krnlSz = Arrays.asList(3.0, rows * krnlRatio, cols * krnlRatio).stream().max(Double::compare).get().intValue();
    // maximum filter: using dilate to extract the local maximum of a image block, which acts as the maximum filter
    // Meanwhile, minimum filter can be achieved by using erode function
    Mat Lg_ = new Mat();
    Mat kernel = Imgproc.getStructuringElement(Imgproc.MORPH_RECT, new Size(krnlSz, krnlSz), new Point(-1, -1));
    Imgproc.dilate(Lg, Lg_, kernel);
    // guided image filter
    Mat Hg = Filters.GuidedImageFilter(Lg, Lg_, r, eps);
    return Hg;
}
</code></pre>

<p>After obtaining the result of global aadaptation filtered by edge-preserving filter, instead of applying the local adaptation equation directly, two factors are intorduced to prevent the flat-looking appearance caused by the filter and improve the performance of the algorithm. One is the contrast enhancement factor and another is adaptive nonlinearity offset. The contrast enhancement factor is computed by
$$
\alpha(x,y)=1+\eta\cdot\frac{L_{g}(x,y)}{L_{g\max}}
$$
where $\eta$ is the contrast control parameter, and $L_{g\max}$ is the maximum luminance value of global adaptation output. And the adaptive nonlinearity offset is written as
$$
\beta=\lambda\cdot\bar{L}_{g}
$$
which varies in accordance with the scene contents, and $\lambda$ is the nonlinearity control parameter, $\bar{L}_{g}$ is the logarithmic average luminance of the global adaptation output. After deriving these two factors, the final local adaptation equation is established as
$$
L_{out}(x,y)=\alpha(x,y)\cdot\log\bigg(\frac{L_{g}(x,y)}{H_{g}(x,y)}+\beta\bigg)
$$
where $L_{out}(x,y)$ is the final local adaptation output. Below is the Java Codes to obtain the final local adaptation output:</p>

<pre><code class="language-java">Mat alpha = new Mat(m, n, rChannel.type());
Core.divide(Lg, new Scalar(Core.minMaxLoc(Lg).maxVal / eta), alpha);
Core.add(alpha, new Scalar(1.0), alpha);
Mat Lg_ = new Mat(m, n, rChannel.type());
Core.add(Lg, new Scalar(1.0 / 255.0), Lg_);
Core.log(Lg_, Lg_);
double beta = Math.exp(Core.sumElems(Lg_).val[0] / (m * n)) * lambda;
Mat Lout = new Mat(m, n, rChannel.type());
Core.divide(Lg, Hg, Lout);
Core.add(Lout, new Scalar(beta), Lout);
Core.log(Lout, Lout);
Core.normalize(alpha.mul(Lout), Lout, 0, 255, Core.NORM_MINMAX);
</code></pre>

<p>After the local adaptation, the processed luminance values are rescaled from 0 to 1. Finally, the tone mapped image is obtained from the luminance values of the local adaptation output and the input HDR image. Here is the Java codes to do the mapping process:</p>

<pre><code class="language-java">Mat gain = obtainGain(Lout, Lw, m, n);
// output
Core.divide(rChannel.mul(gain), new Scalar(Core.minMaxLoc(rChannel).maxVal / 255.0), rChannel); // Red Channel
Core.divide(gChannel.mul(gain), new Scalar(Core.minMaxLoc(gChannel).maxVal / 255.0), gChannel); // Green Channel
Core.divide(bChannel.mul(gain), new Scalar(Core.minMaxLoc(bChannel).maxVal / 255.0), bChannel); // Blue Channel
// merge three color channels to a image
Mat outval = new Mat();
Core.merge(new ArrayList&lt;&gt;(Arrays.asList(bChannel, gChannel, rChannel)), outval);
outval.convertTo(outval, CvType.CV_8UC1);
</code></pre>

<p>Where the <code>gain</code> is derived by</p>

<pre><code class="language-java">private static Mat obtainGain(Mat Lout, Mat Lw, int rows, int cols) {
    Mat gain = new Mat(rows, cols, Lout.type());
    for (int i = 0; i &lt; rows; i++) {
        for (int j = 0; j &lt; cols; j++) {
            if (Lw.get(i, j)[0] == 0)
                gain.put(i, j, Lout.get(i, j)[0]);
            else
                gain.put(i, j, Lout.get(i, j)[0] / Lw.get(i, j)[0]);
        }
    }
    return gain;
}
</code></pre>

<p>The full Java Codes are available in my GitHub repository: <a href="https://github.com/IsaacChanghau/OptimizedImageEnhance/blob/master/src/main/java/com/isaac/models/ALTMRetinex.java" target="_blank">ALTMRetinex</a>, while Matlab codes are here: <a href="https://github.com/IsaacChanghau/OptimizedImageEnhance/tree/master/matlab/ALTMRetinex" target="_blank">[link]</a></p>

<h1 id="experiment-result">Experiment Result</h1>

<p>Here we exhibit some results generated by ALTM methods. And the image display method used is provided by <a href="https://github.com/master-atul/ImShow-Java-OpenCV" target="_blank">ImShow-Java-OpenCV</a>.
<img src="/img/imageprocessing/altm/iris.png" alt="Iris" />
<img src="/img/imageprocessing/altm/whitehouse.png" alt="White House" />
<img src="/img/imageprocessing/altm/horses.png" alt="Horses" />
<img src="/img/imageprocessing/altm/liahthouse.png" alt="Liahthouse" /></p>

<h1 id="reference">Reference</h1>

<ul>
<li><a href="http://koasas.kaist.ac.kr/bitstream/10203/172985/1/73275.pdf" target="_blank">Adaptive Local Tone Mapping Based on Retinex for High Dynamic Range Images</a></li>
<li><a href="http://www.ipol.im/pub/art/2014/107/article_lr.pdf" target="_blank">Multiscale Retinex</a></li>
<li><a href="https://github.com/master-atul/ImShow-Java-OpenCV" target="_blank">ImShow-Java-OpenCV</a></li>
</ul>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/image-processing">image-processing</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/java">java</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/opencv">opencv</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/install_opencv_java/">Installation of OpenCV for Java</a></li>
        
        <li><a href="/post/skiing_in_singapore/">Skiing In Singapore</a></li>
        
        <li><a href="/project/backscatter/">Removing Backscatter to Enhance the Visibility of Underwater Object</a></li>
        
        <li><a href="/project/retinex/">Image Enhancement based on Retinex Theory and Dual-tree Complex Wavelet Transform</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2018 &middot; 

      <strong>Isaac Changhau (Zhang Hao)</strong>
      

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/java.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/javascript.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/c.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/html.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/xml.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ 
          tex2jax: { 
            inlineMath: [['$','$'], ['\\(','\\)']] 
          } 
        });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-xAWI9i8WMRLdgksuhaMCYMTw9D+MEc2cYVBApWwGRJ0cdcywTjMovOfJnlGt9LlEQj6QzyMzpIZLMYujetPcQg==" crossorigin="anonymous"></script>
    
    
    

  </body>
</html>

