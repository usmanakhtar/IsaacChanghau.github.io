<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.45" />
  <meta name="author" content="Zhang Hao">

  
  
  
  
    
      
    
  
  <meta name="description" content="摘自周志华《机器学习》第八章-集成学习。
个体与集成 集成学习 (ensemble learning) 通过构建并结合多个学习器来完成学习任务，也被称为多分类器系统 (multi-classifier system)、基于委员会的学习 (committee-based learning) 等。 下图展示了集成学习的一般结构：先产生一组“个体学习器” (individual learner)，再用某种策略将它们结合。若集成中只包含同种类型的个体学习器，这样的集成是“同质的” (homogeneous)，同质集成中的个体学习器称为“基学习器” (base learner)，相应的学习算法称为“基学习算法” (base learning algorithm)。集成也可以包含不同类型的个体学习器，即“异质的” (heterogenous)。异质集成中的个体学习器由不同的学习算法生成，此时不再有基学习算法，而个体学习器也常称为“组件学习器” (component learner)。 集成学习通过将多个学习器进行结合，通常可以获得比单一学习器显著优越的泛化性能。这对“弱学习器” (weak learner) 尤为明显。实际中，要获得好的集成，个体学习器通常应该“好二不同”，即个体学习器要有一定的“准确性”，并且要有“多样性”，即学习器间具有差异。如下图所示 举个例子，考虑二分类问题 $y\in\{-1,&#43;1\}$ 和真实函数 $\boldsymbol{f}$，假设基分类器的错误率为 $\epsilon$，即对每个基分类器 $h_{i}$ 有 $$ P(h_{i}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\epsilon\tag{1} $$ 假设集成通过简单投票法结合 $T$ 个基分类器，若有超过半数的基分类器正确，则集成分类就正确： $$ H(\boldsymbol{x})=sign\bigg(\sum_{i=1}^{T}h_{i}(\boldsymbol{x})\bigg)\tag{2} $$ 假设基分类器的错误率相互独立，则由 Hoeffding 不等式可知，集成的错误率为 $$ P(H(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\sum_{k=0}^{\lfloor T/2\rfloor}{T\choose k}(1-\epsilon)^{k}\epsilon^{T-k}\leq\exp\big(-\frac{1}{2}T(1-2\epsilon)^{2}\big)\tag{3} $$ 由上式可得，随着个体集成中个体分类器数目 $T$ 的增大，集成的错误率将指数级下降，最终趋向于零。上面的分析有一个关键假设：基学习器的误差相互独立。实际上，个体学习器是为解决同一个问题训练出来的，显然不能相互独立。而“准确性”和“多样性”本身就存在冲突，当准确性很高之后，增加多样性就需要牺牲准确性。
根据个体学习器的生成方式，集成学习大致分为两类：
 个体学习器间存在强依赖关系、必须串行生成的序列化方法，如 Boosting。 个体学习器不存在强依赖关系、可同时生成的并行化方法，如 Bagging 和“随即森林” (Random Forest)。  Boosting Boosting 是一族可将弱学习器提升为强学习器的算法，其工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本训练下一个基学习器，如此重复进行，直至基学习器达到事先指定的值 $T$，最终将这 $T$ 个基学习器进行加权结合。如 AdaBoost 算法，">

  
  <link rel="alternate" hreflang="en-us" href="https://isaacchanghau.github.io/post/ml_zzh_note_4/">

  


  

  
  
  <meta name="theme-color" content="#3f51b5">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  <link rel="feed" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://isaacchanghau.github.io/post/ml_zzh_note_4/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/IsaacChanghau">
  <meta property="twitter:creator" content="@https://twitter.com/IsaacChanghau">
  
  <meta property="og:site_name" content="Isaac Changhau">
  <meta property="og:url" content="https://isaacchanghau.github.io/post/ml_zzh_note_4/">
  <meta property="og:title" content="Machine Learning Note (4): Ensemble Learning | Isaac Changhau">
  <meta property="og:description" content="摘自周志华《机器学习》第八章-集成学习。
个体与集成 集成学习 (ensemble learning) 通过构建并结合多个学习器来完成学习任务，也被称为多分类器系统 (multi-classifier system)、基于委员会的学习 (committee-based learning) 等。 下图展示了集成学习的一般结构：先产生一组“个体学习器” (individual learner)，再用某种策略将它们结合。若集成中只包含同种类型的个体学习器，这样的集成是“同质的” (homogeneous)，同质集成中的个体学习器称为“基学习器” (base learner)，相应的学习算法称为“基学习算法” (base learning algorithm)。集成也可以包含不同类型的个体学习器，即“异质的” (heterogenous)。异质集成中的个体学习器由不同的学习算法生成，此时不再有基学习算法，而个体学习器也常称为“组件学习器” (component learner)。 集成学习通过将多个学习器进行结合，通常可以获得比单一学习器显著优越的泛化性能。这对“弱学习器” (weak learner) 尤为明显。实际中，要获得好的集成，个体学习器通常应该“好二不同”，即个体学习器要有一定的“准确性”，并且要有“多样性”，即学习器间具有差异。如下图所示 举个例子，考虑二分类问题 $y\in\{-1,&#43;1\}$ 和真实函数 $\boldsymbol{f}$，假设基分类器的错误率为 $\epsilon$，即对每个基分类器 $h_{i}$ 有 $$ P(h_{i}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\epsilon\tag{1} $$ 假设集成通过简单投票法结合 $T$ 个基分类器，若有超过半数的基分类器正确，则集成分类就正确： $$ H(\boldsymbol{x})=sign\bigg(\sum_{i=1}^{T}h_{i}(\boldsymbol{x})\bigg)\tag{2} $$ 假设基分类器的错误率相互独立，则由 Hoeffding 不等式可知，集成的错误率为 $$ P(H(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\sum_{k=0}^{\lfloor T/2\rfloor}{T\choose k}(1-\epsilon)^{k}\epsilon^{T-k}\leq\exp\big(-\frac{1}{2}T(1-2\epsilon)^{2}\big)\tag{3} $$ 由上式可得，随着个体集成中个体分类器数目 $T$ 的增大，集成的错误率将指数级下降，最终趋向于零。上面的分析有一个关键假设：基学习器的误差相互独立。实际上，个体学习器是为解决同一个问题训练出来的，显然不能相互独立。而“准确性”和“多样性”本身就存在冲突，当准确性很高之后，增加多样性就需要牺牲准确性。
根据个体学习器的生成方式，集成学习大致分为两类：
 个体学习器间存在强依赖关系、必须串行生成的序列化方法，如 Boosting。 个体学习器不存在强依赖关系、可同时生成的并行化方法，如 Bagging 和“随即森林” (Random Forest)。  Boosting Boosting 是一族可将弱学习器提升为强学习器的算法，其工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本训练下一个基学习器，如此重复进行，直至基学习器达到事先指定的值 $T$，最终将这 $T$ 个基学习器进行加权结合。如 AdaBoost 算法，">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-05-18T00:00:00&#43;08:00">
  
  <meta property="article:modified_time" content="2017-05-18T00:00:00&#43;08:00">
  

  

  <title>Machine Learning Note (4): Ensemble Learning | Isaac Changhau</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Isaac Changhau</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#tags">
            
            <span>Tags</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#achievements">
            
            <span>Achievements</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Machine Learning Note (4): Ensemble Learning</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2017-05-18 00:00:00 &#43;0800 &#43;08" itemprop="datePublished dateModified">
      Thu, May 18, 2017
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhang Hao">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  

  

  
  

  

</div>

    
    <div class="article-style" itemprop="articleBody">
      

<p>摘自周志华《机器学习》第八章-集成学习。</p>

<h1 id="个体与集成">个体与集成</h1>

<p>集成学习 (ensemble learning) 通过构建并结合多个学习器来完成学习任务，也被称为多分类器系统 (multi-classifier system)、基于委员会的学习 (committee-based learning) 等。 下图展示了集成学习的一般结构：先产生一组“个体学习器” (individual learner)，再用某种策略将它们结合。若集成中只包含同种类型的个体学习器，这样的集成是“同质的” (homogeneous)，同质集成中的个体学习器称为“基学习器” (base learner)，相应的学习算法称为“基学习算法” (base learning algorithm)。集成也可以包含不同类型的个体学习器，即“异质的” (heterogenous)。异质集成中的个体学习器由不同的学习算法生成，此时不再有基学习算法，而个体学习器也常称为“组件学习器” (component learner)。
<img src="/img/machinelearning/ensemble/1.png" alt="1.png" />
集成学习通过将多个学习器进行结合，通常可以获得比单一学习器显著优越的泛化性能。这对“弱学习器” (weak learner) 尤为明显。实际中，要获得好的集成，个体学习器通常应该“好二不同”，即个体学习器要有一定的“准确性”，并且要有“多样性”，即学习器间具有差异。如下图所示
<img src="/img/machinelearning/ensemble/2.png" alt="2.png" />
举个例子，考虑二分类问题 $y\in\{-1,+1\}$ 和真实函数 $\boldsymbol{f}$，假设基分类器的错误率为 $\epsilon$，即对每个基分类器 $h_{i}$ 有
$$
P(h_{i}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\epsilon\tag{1}
$$
假设集成通过简单投票法结合 $T$ 个基分类器，若有超过半数的基分类器正确，则集成分类就正确：
$$
H(\boldsymbol{x})=sign\bigg(\sum_{i=1}^{T}h_{i}(\boldsymbol{x})\bigg)\tag{2}
$$
假设基分类器的错误率相互独立，则由 <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality" target="_blank">Hoeffding</a> 不等式可知，集成的错误率为
$$
P(H(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\sum_{k=0}^{\lfloor T/2\rfloor}{T\choose k}(1-\epsilon)^{k}\epsilon^{T-k}\leq\exp\big(-\frac{1}{2}T(1-2\epsilon)^{2}\big)\tag{3}
$$
由上式可得，随着个体集成中个体分类器数目 $T$ 的增大，集成的错误率将指数级下降，最终趋向于零。上面的分析有一个关键假设：<strong><em>基学习器的误差相互独立</em></strong>。实际上，个体学习器是为解决同一个问题训练出来的，显然不能相互独立。而“准确性”和“多样性”本身就存在冲突，当准确性很高之后，增加多样性就需要牺牲准确性。</p>

<p>根据个体学习器的生成方式，集成学习大致分为两类：</p>

<ul>
<li>个体学习器间存在强依赖关系、必须串行生成的序列化方法，如 Boosting。</li>
<li>个体学习器不存在强依赖关系、可同时生成的并行化方法，如 Bagging 和“随即森林” (Random Forest)。</li>
</ul>

<h1 id="boosting">Boosting</h1>

<p>Boosting 是一族可将弱学习器提升为强学习器的算法，其工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本训练下一个基学习器，如此重复进行，直至基学习器达到事先指定的值 $T$，最终将这 $T$ 个基学习器进行加权结合。如 AdaBoost 算法，</p>

<p>AdaBoost 算法<br />
输入：训练集 $D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$，$y\in\{-1,+1\}$；基学习算法 $\mathfrak{L}$；训练轮数 $T$。<br />
过程：<br />
$\qquad$$\mathcal{D}_{1}(\boldsymbol{x})=\frac{1}{m}$<br />
$\qquad$<strong>for</strong> $t=1,2,\dots,T$ <strong>do</strong><br />
$\qquad$$\qquad$$h_{t}=\mathfrak{L}(D,\mathcal{D}_{t})$<br />
$\qquad$$\qquad$$\epsilon_{t}=P_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big(h_{t}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x})\big)$<br />
$\qquad$$\qquad$<strong>if</strong> $\epsilon_{t}&gt;0.5$ <strong>then break</strong>$\qquad$ (检测是否优于随机猜测)<br />
$\qquad$$\qquad$$\alpha_{t}=\frac{1}{2}\ln\big(\frac{1-\epsilon_{t}}{\epsilon_{t}}\big)$<br />
$\qquad$$\qquad$$\mathcal{D}_{t+1}(\boldsymbol{x})=\frac{\mathcal{D}_{t}(\boldsymbol{x})}{Z_{t}}\times\begin{cases}\exp(-\alpha_{t}), &amp; h_{t}(\boldsymbol{x})=\boldsymbol{f}(\boldsymbol{x})\newline\exp(\alpha_{t}), &amp; h_{t}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x})\end{cases}=\frac{\mathcal{D}_{t}(\boldsymbol{x})\exp\big(-\alpha_{t}\boldsymbol{f}(\boldsymbol{x})h_{t}(\boldsymbol{x})\big)}{Z_{t}}$ ($Z_{t}$是规范化因子，确保$\mathcal{D}_{t+1}$是一个分布)<br />
$\qquad$<strong>end for</strong><br />
输出：$H(\boldsymbol{x})=sign\big(\sum_{t=1}^{T}\alpha_{t}h_{t}(\boldsymbol{x})\big)$</p>

<p>AdaBoost 可以理解为一个“加性模型” (additive model)，即基学习器的线性组合
$$
H(\boldsymbol{x})=\sum_{t=1}^{T}\alpha_{t}h_{t}(\boldsymbol{x})\tag{4}
$$
来最小化指数损失函数 (exponential loss function)
$$
\ell_{\exp}(H|\mathcal{D})=\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}}\big[e^{-\boldsymbol{f}(\boldsymbol{x})H(\boldsymbol{x})}\big]\tag{5}
$$
若 $H(\boldsymbol{x})$ 能令指数损失函数最小化，则考虑式(5)对 $H(\boldsymbol{x})$ 的偏导，并令导数为零，可得
$$
H(\boldsymbol{x})=\frac{1}{2}\ln\frac{P\big(\boldsymbol{f}(\boldsymbol{x})=1|\boldsymbol{x}\big)}{P\big(\boldsymbol{f}(\boldsymbol{x})=-1|\boldsymbol{x}\big)}\tag{6}
$$
因此有
$$
sign\big(H(\boldsymbol{x})\big)
=sign\bigg(\frac{1}{2}\ln\frac{P\big(\boldsymbol{f}(\boldsymbol{x})=1|\boldsymbol{x}\big)}{P\big(\boldsymbol{f}(\boldsymbol{x})=-1|\boldsymbol{x}\big)}\bigg)
=\dots
=\arg\max_{y\in\{-1,1\}}P\big(\boldsymbol{f}(\boldsymbol{x})=y|\boldsymbol{x}\big)\tag{7}
$$
这意味着 $sign\big(H(\boldsymbol{x})\big)$ 达到零贝叶斯最优错误率。换言之，若指数损失函数最小化，则分类错误率也将最小化；说明指数损失函数式分类任务原本 $0/1$ 损失函数的一致 (consistent) 替代损失函数。</p>

<p>在AdaBoost算法中，第一个基分类器 $h_{1}$ 是通过直接将基学习算法用于初始数据分布而得；此后迭代生成 $h_{t}$ 和 $\alpha_{t}$，当基分类器 $h_{t}$ 基于分布 $\mathcal{D}_{t}$ 产生后，该基分类器的权重 $\alpha_{t}$应使得 $\alpha_{t}h_{t}$ 最小化指数损失函数
$$
\ell_{\exp}(\alpha_{t}h_{t}|\mathcal{D}_{t})=\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big[e^{-f(\boldsymbol{x})\alpha_{t}h_{t}(\boldsymbol{x})}\big]=\dots=e^{-\alpha_{t}}(1-\epsilon_{t})+e^{\alpha_{t}}\epsilon_{t}\tag{8}
$$
其中 $\epsilon_{t}=P_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big(h_{t}(\boldsymbol{x})\neq f(\boldsymbol{x})\big)$。考虑指数损失函数关于 $\alpha_{t}$ 的导数并令导数为零可得
$$
\alpha_{t}=\frac{1}{2}\ln\bigg(\frac{1-\epsilon_{t}}{\epsilon_{t}}\bigg)\tag{9}
$$
这就是分类器权重更新公式，而 AdaBoost 算法在获得 $H_{t-1}$ 之后样本分布将进行调整，使下一轮的基学习器 $h_{t}$ 能纠正 $H_{t-1}$ 的一些错误，即最小化 $\ell_{\exp}(H_{t-1}+h_{t}|\mathcal{D})$。通过泰勒展开，数学期望定义等一系列变化和相关关系 (具体请参照书中推导) 可得到理想基学习器
$$
h_{t}(\boldsymbol{x})=\arg\min_{h}\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big[\Pi(f(\boldsymbol{x})\neq h(\boldsymbol{x}))\big]\tag{10}
$$
由此可见，理想的 $h_{t}$ 将在分布 $\mathcal{D}_{t}$ 下最小化分类误差。因此，弱分类器将基于分布 $\mathcal{D}_{t}$ 来训练，且针对 $\mathcal{D}_{t}$ 的分类误差应小于0.5，这在一定程度上类似“残差逼近”的思想。考虑到 $\mathcal{D}_{t}$ 和 $\mathcal{D}_{t+1}$ 的关系，有
$$
\mathcal{D}_{t+1}(\boldsymbol{x})=\mathcal{D}_{t}(\boldsymbol{x})\cdot e^{-f(\boldsymbol{x})\alpha_{t}h_{t}(\boldsymbol{x})}\frac{\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}}\big[e^{-f(\boldsymbol{x})H_{t-1}(\boldsymbol{x})}\big]}{\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}}\big[e^{-f(\boldsymbol{x})H_{t}(\boldsymbol{x})}\big]}\tag{11}
$$
这便是样本分布更新公式。以上是从基于加性模型迭代式优化指数损失函数的角度推导出了AdaBoost算法。
Boosting算法要求基学习器能对特定的数据分布进行学习，这可通过“重赋权法” (re-weighting) 实施，即在训练过程的每一轮中，根据样本分布为每一个训练样本重新赋予一个权重。对无法接受带权样本的基学习算法，则可通过“重采样法” (re-sampling) 来处理，即每一轮学习中，根据样本分布对训练数据重新进行采样，在用重采样而得的样本集对基学习器进行训练。两种方法没有显著优劣差别。但是，若采用重采样法，则面对学习过程停止问题 (流程图中检测是否优于随机猜测，否抛弃当前基学习器，学习过程停止)，可获得“重启动”机会以避免训练过程过早停止。</p>

<p>从偏差-方差分解角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建很强的集成。</p>

<h1 id="bagging与随机森林">Bagging与随机森林</h1>

<p>欲得到泛化能力强的集成，集成中的个体学习器应尽可能相互独立，虽然在实际中无法做到，但可以设法使基学习器尽可能具有较大的差异。给定一个数据集，一种方法是对样本进行采样，产生若干不同的子集，再从每个子集中训练出一个基学习器。为了避免因为采样导致每个基学习器训练数据不足，常采用相互有交叠的采样子集。</p>

<h2 id="bagging">Bagging</h2>

<p>Bagging 是并行式集成学习的代表，它直接基于自助采样法 (<a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" target="_blank">bootstrap sampling</a>)。给定包含 $m$ 个样本的数据集，先随机取出一个样本放入采样集，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，经过 $m$ 次随机采样，得到含 $m$ 个样本的采样集，初始训练集中有的样本在采样集多次出现，有的则从未出现。极限状态下 ($m\mapsto\infty$)，初始训练集中约有 $63.2%$ 的样本出现在采样集中。这样可采样出 $T$ 个含 $m$ 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器结合。这就是 Bagging 的基本流程。在对预测输出进行结合时，Bagging 通常对分类任务使用简单投票法，对回归任务使用简单平均法。若分类预测时出现两个类收到同样票数的情形，则最简单的方法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。Bagging 算法流程如下：</p>

<p>输入：训练集 $D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$，$y\in\{-1,+1\}$；基学习算法 $\mathfrak{L}$；训练轮数 $T$。<br />
过程：</p>

<p>$\qquad$<strong>for</strong> $t=1,2,\dots,T$ <strong>do</strong><br />
$\qquad$$\qquad$$h_{t}=\mathfrak{L}(D,\mathcal{D}_{bs})$<br />
$\qquad$<strong>end for</strong>
输出：$H(\boldsymbol{x})=\arg\max_{y\in\mathcal{Y}}\sum_{t=1}^{T}\Pi\big(h_{t}(\boldsymbol{x})=y\big)$</p>

<p>假设基学习器的计算复杂度为 $\mathcal{O}(m)$，则 Bagging 的复杂度大致为 $T\big(\mathcal{O}(m)+\mathcal{O}(s)\big)$，考虑到采样与投票/平均过程的复杂度 $\mathcal{O}(s)$ 很小，而 $T$ 通常是一个不太大的常数，因此训练一个 Bagging 集成与直接使用基学习器训练一个学习器的复杂度同阶。此外，与标准 AdaBoost 只适用于二分类任务不同，Bagging能不经修改的用于多分类、回归任务。而自助采样过程还给 Bagging 带来了另一个优点：由于每个基学习器只使用了初始训练集中约 $63.2%$ 的样本，剩下的样本可用作验证集来对泛化性能进行“包外估计” (<a href="https://en.wikipedia.org/wiki/Out-of-bag_error" target="_blank">out-of-bag estimate</a>)。</p>

<p>令 $D_{t}$ 表示 $h_{t}$ 实际使用的训练集，令 $H^{oob}(\boldsymbol{x})$ 表示对样本 $\boldsymbol{x}$ 的包外预测，即仅考虑那些未使用 $\boldsymbol{x}$ 训练的基学习器在 $\boldsymbol{x}$ 上的预测，有
$$
H^{oob}(\boldsymbol{x})=\arg\max_{y\in\mathcal{Y}}\sum_{t=1}^{T}\Pi\big(h_{t}(\boldsymbol{x})=y\big)\cdot\Pi(\boldsymbol{x}\notin D_{t})
$$
则 Bagging 泛化误差的包外估计为
$$
\epsilon^{oob}=\frac{1}{\vert D\vert}\sum_{(\boldsymbol{x},y)\in D}\Pi\big(H^{oob}(\boldsymbol{x})\neq y\big)
$$
当基学习器是决策树时，包外样本还可以用于辅助剪枝，或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；当基学习器是神经网络时，可使用包外样本辅助早期停止以减小过拟合风险。从偏差-方差分解角度看， Bagging 主要关注<strong><em>降低方差</em></strong>，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器中效用更为明显。</p>

<h2 id="随机森林">随机森林</h2>

<p>随机森林 (Random Forset，RF) 是 Bagging 的一个扩展。RF 在以决策树为基学习器构建 Bagging 集成的基础上，在决策树的训练过程中引入随机属性选择。传统决策树在选择划分属性时是在当前节点的属性集合 (假定有 $d$ 个属性) 中选择一个最优属性；而在 RF 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $k$ 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。参数 $k$ 控制了随机性的引入程度：</p>

<ul>
<li>若令 $k=d$，则基决策树的构建与传统决策树相同；</li>
<li>若令 $k=1$，则是随机选择一个属性划分；</li>
<li>一般情况下，推荐 $k=\log_{2}d$。</li>
</ul>

<p>与 Bagging 中基学习器的“多样性”仅通过样本扰动而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这使得最终集成的泛化性能可通过个体学习器之间异度的增加而进一步提升。此外，随机森林的收敛性与 Bagging 相似，随机森林的起始性能往往相对较差，特别是在集成中只包含一个基学习器时，因为通过引入属性扰动，随机森林中个体学习器的性能往往有所降低。然而，随着个体学习器数目的增加，随机森林通常会收敛到更低的泛化误差。值得一提的是，随机森林的训练效率常优于 Bagging，因为在个体决策树的构建过程中，Bagging 使用的是“确定型“决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林shiyongde”随机型“决策树则只需考察一个属性子集。</p>

<h1 id="结合策略">结合策略</h1>

<p>学习器结合可能会从三个方面带来好处：</p>

<ul>
<li>从统计方面，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减小这一风险；</li>
<li>从计算方面，学习算法往往会陷入局部极小，有的局部极小点对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险；</li>
<li>从表示方面，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个学习器，由于假设空间有所扩大，有可能学得更好的近似。</li>
</ul>

<p><img src="/img/machinelearning/ensemble/3.png" alt="3.png" />
假定集成包含 $T$ 个基学习器 $\{h_{1},h_{2},\dots,h_{T}\}$，其中 $h_{i}$ 在示例 $\boldsymbol{x}$ 上的输出为 $h_{i}(\boldsymbol{x})$。</p>

<h2 id="平均法">平均法</h2>

<ul>
<li><strong><em>简单平均法</em></strong> (simple averaging)：$$H(\boldsymbol{x})=\frac{1}{T}\sum_{i=1}^{T}h_{i}(\boldsymbol{x})$$</li>
<li><strong><em>加权平均法</em></strong> (weighted averaging)：$$H(\boldsymbol{x})=\sum_{i=1}^{T}w_{i}h_{i}(\boldsymbol{x})$$其中 $w_{i}$ 是个体学习器 $h_{i}$ 的权重，通常要求 $w_{i}\geq 0$，$\sum_{i=1}^{T}w_{i}=1$。</li>
</ul>

<p>加权平均法的权重一般从训练数据中学习而得，但由于样本不充分或噪声，通常学习的权重并不完全可靠，对于规模较大的集成，由于权重较多，甚至可能导致过拟合。一般而言，<strong><em>在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法。</em></strong></p>

<h2 id="投票法">投票法</h2>

<p>对分类任务，学习器 $h_{i}$ 将从类别标记集合 $\{c_{1},\dots,c_{N}\}$ 中预测出一个标记。将 $h_{i}$ 的预测输出表示为一个 $N$ 维向量 $\big(h_{i}^{1}(\boldsymbol{x}),\dots,h_{i}^{N}(\boldsymbol{x})\big)$，其中 $h_{i}^{j}(\boldsymbol{x})$ 表示 $h_{i}$ 在类别标记 $c_{j}$ 上的输出。</p>

<ul>
<li><strong><em>绝对多数投票法</em></strong> (majority voting)：
$$
H(\boldsymbol{x})=\begin{cases}
c_{j}, &amp; \sum_{i=1}^{T}h_{i}^{j}(\boldsymbol{x})&gt;0.5\sum_{k=1}^{N}\sum_{i=1}^{T}h_{i}^{k}(\boldsymbol{x});\newline
reject, &amp; otherwise.
\end{cases}
$$
即若某标记得票过半数，则预测为该标记；否则拒绝预测。</li>
<li><strong><em>相对多数投票法</em></strong> (plurality voting)：
$$
H(\boldsymbol{x})=c_{\arg\max_{j}\sum_{i=1}^{T}h_{i}^{j}(\boldsymbol{x})}
$$
即预测为的票最多的标记，若同时有多个标记获最高票，则从中随机选择一个。</li>
<li><strong><em>加权投票法</em></strong> (weighted voting)：
$$
H(\boldsymbol{x})=c_{\arg\max_{j}\sum_{i=1}^{T}w_{i}h_{i}^{j}(\boldsymbol{x})}
$$
与加权平均法类似，$w_{i}$ 是 $h_{i}$ 的权重，通常 $w_{i}\geq 0$，$\sum_{i=1}^{T}w_{i}=1$。</li>
</ul>

<p>以上没有限制个体学习器输出值的类型，实际中，不同类型的个体学习器可能产生不同类型的输出值，常见的有：</p>

<ul>
<li>类标记：$h_{i}^{j}(\boldsymbol{x})\in\{0,1\}$，若 $h_{i}$ 将样本 $\boldsymbol{x}$ 预测为类别 $c_{j}$ 则取值为1，否则为0.使用类标记的投票亦称“硬投票” (hard voting)。</li>
<li>类概率：$h_{i}^{j}(\boldsymbol{x})\in[0,1]$，相当于对后验概率 $P(c_{j}|\boldsymbol{x})$ 的一个估计。使用概率的投票亦称“软投票” (soft voting)。</li>
</ul>

<p>注意：不同类型的 $h_{i}^{j}(\boldsymbol{x})$ 值不能混用。</p>

<h2 id="学习法">学习法</h2>

<p>当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。Stacking是学习法的典型代表，这里称个体学习器为初级学习器，用于结合的学习器称为次级学习器或元学习器 (meta-learner)。</p>

<p>Stacking先从初始数据集训练出初级学习器，然后生成一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。Stacking算法描述如下，这里假定初级学习器使用不同学习算法产生，即初级集成是异质的。</p>

<p>输入：训练集 $D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$，初级学习算法 $\mathfrak{L}_{1},\mathfrak{L}_{2},\dots,\mathfrak{L}_{T}$，次级学习算法 $\mathfrak{L}$。</p>

<p>过程：</p>

<p>$\qquad$<strong>for</strong> $t=1,2,\dots,T$ <strong>do</strong><br />
$\qquad\qquad h_{t}=\mathfrak{L}_{t}(D)$<br />
$\qquad$<strong>end for</strong><br />
$\qquad D&rsquo;=\emptyset$<br />
$\qquad$<strong>for</strong> $i=1,2,\dots,m$ <strong>do</strong><br />
$\qquad\qquad$<strong>for</strong> $t=1,2,\dots,T$ <strong>do</strong><br />
$\qquad\qquad\qquad z_{it}=h_{t}(\boldsymbol{x}_{i})$<br />
$\qquad\qquad$<strong>end for</strong><br />
$\qquad\qquad D&rsquo;=D&rsquo;\bigcup\big((z_{i1},\dots,z_{iT}),y_{i}\big)$<br />
$\qquad$<strong>end for</strong><br />
$\qquad h&rsquo;=\mathfrak{L}(D&rsquo;)$<br />
输出：$H(\boldsymbol{x})=h&rsquo;\big(h_{1}(\boldsymbol{x}),\dots,h_{T}(\boldsymbol{x})\big)$</p>

<p>在训练阶段，次级训练集释利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险比较大。因此一般通过交叉验证或留一法这样的方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。次级学习器的输入属性表示和次级学习算法对Stacking集成的泛化性能优很大影响。将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归 (Multi-response Linear Regression，MLR) 作为次级学习算法效果较好，在MLR中使用不同的属性集效果更佳。</p>

<p>此外，贝叶斯模型平均 (Bayes Model Averaging，BMA) 基于后验概率来为不同模型赋予权重，可视为加权平均法的一种特殊实现，理论上来说，若数据生成模型恰在当前考虑的模型中，切数据噪声很少，则BMA不差于Stacking，然而实际中很难确保这一要求。因此 Stacking 通常优于 BMA，因为其鲁棒性比 BMA好，且 BMA 对模型近似误差非常敏感。</p>

<h1 id="多样性">多样性</h1>

<h2 id="误差-分歧分解">误差-分歧分解</h2>

<p>设集成泛化误差为 $E$，令 $\bar{E}=\sum_{i=1}^{T}w_{i}E_{i}$ 表示个体学习器泛化误差的加权均值，$\bar{A}=\sum_{i=1}^{T}w_{i}A_{i}$ 表示个体学习器的加权分歧值，有
$$
E=\bar{E}-\bar{A}
$$
这个式子明确提示出：<strong><em>个体学习器准确性越高、多样性越大，则集成越好</em></strong>。(推导此处省略，书中推导过程只适用于回归学习，难以直接推广到分类学习任务。)</p>

<h2 id="多样性度量">多样性度量</h2>

<p>多样性度量 (diversity measure) 是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。典型做法是考虑个体分类器两两相似/不相似性。对二分类任务，分类器 $h_{i}$ 与 $h_{j}$ 的预测结果列联表 (contingency table) 为</p>

<p>| $h<em>{i}=+1$ | $h</em>{i}=-1$
:&mdash;: | :&mdash;: | :&mdash;:
$h_{j}=+1$ | a | c
$h_{j}=-1$ | b | d</p>

<p>其中，a表示 $h_{i}$ 与 $h_{j}$ 均预测为正类的样本数目；b、c、d 含义类推；$a+b+c+d=m$。基于此列联遍，给出以下常见多样性度量。</p>

<ul>
<li><p><strong><em>不合度量</em></strong> (disafreement measure)：
$$
dis_{ij}=\frac{b+c}{m}
$$
$dis_{ij}\in[0,1]$，值越大多样性越大。</p></li>

<li><p><strong><em>相关系数</em></strong> (correlation coefficient)：
$$
\rho_{ij}=\frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}
$$
$\rho_{ij}\in[-1,1]$，若 $h_{i}$ 与 $h_{j}$ 无关，则值为0；若 $h_{i}$ 与 $h_{j}$ 正相关则值为正，否则为负。</p></li>

<li><p><strong><em>$Q$-统计量</em></strong> ($Q$-statistic)：
$$
Q_{ij}=\frac{ad-bc}{ad+bc}
$$
$Q_{ij}$ 与相关系数 $\rho_{ij}$的符号相同，且 $\vert Q_{ij}\vert\leq\vert\rho_{ij}\vert$。</p></li>

<li><p><strong><em>$\kappa$-统计量</em></strong> ($\kappa$-statistic)：
$$
\kappa=\frac{p_{1}-p_{2}}{1-p_{2}}
$$
其中，$p_{1}$ 是两个分类器取得一致的概率；$p_{2}$ 是两个分类器偶然达成一致的概率，它们可由数据集 $D$ 估算：
$$p_{1}=\frac{a+d}{m}
$$
$$
p_{2}=\frac{(a+b)(a+c)+(c+d)(b+d)}{m^{2}}$$
若分类器 $h_{i}$ 与 $h_{j}$ 在 $D$ 上完全一致，则 $\kappa=1$；若它们仅是偶然达成一致，则 $\kappa=0$。$\kappa$ 通常非负，仅在 $h_{i}$ 与 $h_{j}$ 达成一致的概率甚至低于偶然性的情况下取负值。</p></li>
</ul>

<p>以上都是“成对型” (pairwise) 多样性度量，可以通过二维图绘制出来，如著名的“$\kappa$-误差图”，如下面的示例
<img src="/images/machinelearning/ensemble/4.png" alt="4.png" />
其中横坐标是这对分类器的 $\kappa$ 值，纵坐标是它们的平均误差，显然，数据点云的位置越高，则个体分类器准确性越低；点云的位置越靠右，则个体学习器的多样性越小。</p>

<h2 id="多样性增强">多样性增强</h2>

<p>集成学习中需有效地生成多样性大的个体学习器，一般方法是在学习过程中引入随机性，常见方法是对数据样本、输入属性、输出表示、算法参数进行扰动。</p>

<ul>
<li><strong><em>样本数据扰动</em></strong>：给定初始数据集，可从中残生不同数据子集，再利用不同的数据子集训练处不同的个体学习器。数据样本扰动通常基于采样法，简单高效。这种方法对“不稳定基学习器”，如决策树、神经网络等很有效，但对于对数据样本扰动不敏感的基学习器 (稳定基学习器)，如线性学习器、支持向量机、朴素贝叶斯、$k$-近邻学习器等效果不明显。</li>
<li><strong><em>输入属性扰动</em></strong>：训练样本通常由一组属性描述，不同的“子空间”提供了不同的数据观察视角。显然，从不同子空间训练出的个体学习器必然有所不同。随机子空间 (random subspace) 算法是一种代表性方法。</li>
<li><strong><em>输出表示扰动</em></strong>：通过对输出表示进行操纵以增强多样性。可对训练样本的类标记稍作变动，如“翻转法” (Flipping Output) 随机改变一些训练样本标记；也可对输出表示进行转化，如“输出调制法” (Output Smearing) 将分类输出转化为回归输出后构建个体学习器；还可将原任务拆解为多个可同时求解的子任务，如ECOC法利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基学习器。</li>
<li><strong><em>算法参数扰动</em></strong>：基学习算法一般都有参数需要进行设置，因此可以通过随机设置不同的参数，往往可产生差别较大的个体学习器，如“负相关法” (Negative Correlation) 显式地通过正则化项来强制个体神经网络使用不同的参数。对参数较少的算法，可通过将其学习过程中某些环节用其它类似方式代替，从而达到扰动目的。</li>
</ul>

<h1 id="习题">习题</h1>

<p><strong><em>8.1 假设抛硬币正面朝上的概率为 $p$，反面朝上的概率为 $1-p$，令 $H(n)$ 代表抛 $n$ 次硬币所得正面朝上的次数，则最多 $k$ 次正面朝上的概率为</em></strong>
$$
P(H(n)\leq k)=\sum_{i=0}^{k}{n\choose i}p^{i}(1-p)^{n-i}
$$
对 $\delta&gt;0$，$k=(p-\delta)n$，有 Hoeffding 不等式
$$
P(H(n)\leq(p-\delta)n)\leq e^{-2\delta^{2}n}
$$
<strong><em>试推导出式(3)</em></strong>。
Ans: 取$p-\delta=\frac{1}{2}$，则$\delta=p-\frac{1}{2}=\frac{1}{2}-\epsilon$，
$$
P(H(n)\leq\frac{n}{2})=\sum_{i=0}{\frac{n}{2}}{n \choose i}p^{i}(1-p)^{n-i}\leq e^{-2(\frac{1}{2}-\epsilon)^{2}n}=e^{-\frac{1}{2}(1-2\epsilon)^{2}n}
$$</p>

<p><strong><em>8.2 对于 $0/1$ 损失函数来说，指数损失函数并非仅有的一致替代函数。考虑式(5)，试证明：任意损失函数 $\ell\big(-\boldsymbol{f}(\boldsymbol{x})H(\boldsymbol{x})\big)$，若对于 $H(\boldsymbol{x})$ 在区间 $[-\infty,\delta]$ $(\delta&gt;0)$ 上单调递减，则 $\ell$ 是 $0/1$ 损失函数的一致替代函数。</em></strong><br />
Ans: 总损失
$$
\begin{aligned}
\mathcal{L} &amp; =\ell\big(-H(x)f(x)\big)P\big(f(x)|x\big)=\ell\big(-H(x)\big)\cdot P\big(f(x)\newline
&amp; =1|x\big)+\ell\big(H(x)\big)\cdot P\big(f(x)=0|x\big),\quad H(x)\in\{-1,1\}
\end{aligned}
$$
要使$\mathcal{L}$最小，当$P(f(x)=1|x)&gt;P(f(x)=0|x)$时，会希望$\ell(-H(x))&lt;\ell(H(x))$，由于$\ell$是递减的，得$H(x)&gt;H(-x)$，的$H(x)=1$。同理当$P(f(x)=1|x)&lt;P(f(x)=0|x)$时，$H(x)=−1$。$\ell(−H(x)f(x))$是对$H(x)$的单调递减函数，那么可以认为$\ell(−H(x)f(x))$是对$−H(x)$的单调递增函数。此时$H(x)=\arg\max_{y\in0,1}P(f(x)=y|x)$,即达到了贝叶斯最优错误率，说明$\ell$是$0/1$损失函数的一致替代函数。</p>

<p><strong><em>8.3 从网上下载或自己编程实现AdaBoost，以不剪枝决策树为基学习器，在西瓜数据集3.0$\alpha$上训练一个AdaBoost集成。</em></strong><br />
Ans: 由于西瓜数据集数据量太小，这里我们使用UCI数据集的Iris数据进行实验，并使用Sci-kit Learn机器学习包编程，具体细节见代码。</p>

<pre><code class="language-python">import pandas as pd
import warnings
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.utils import shuffle
warnings.filterwarnings('ignore')

data = pd.read_csv('iris.csv')
labels = data['class']  # labels
data.drop(['class'], axis=1, inplace=True)  # data
# shuffling data and labels
data, labels = shuffle(data, labels, random_state=5)
# split into train and test
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.1, random_state=200)
# create base learner
base_learner = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=2, random_state=100,
                                      max_leaf_nodes=5)
# create AdaBoost classifier and fit train data
ada_boost = AdaBoostClassifier(base_learner, n_estimators=10, algorithm='SAMME', learning_rate=0.5, random_state=200)
print(ada_boost, '\n')  # print AdaBoost configuration
# fit data
ada_boost.fit(train_data, train_labels)
print(ada_boost.base_estimator_, '\n')  # print base learner
print('Base Learner error and weight:')
for idx, err, weight in zip(range(1, 11), ada_boost.estimator_errors_, ada_boost.estimator_weights_):
    print('Base Learner-%d\t' % idx, err, '\t', weight)
print('\n')
print('Feature Importance:')
for feature, importance in zip(list(train_data), ada_boost.feature_importances_):
    print(feature, '\t', importance)
print('\n')
# Accuracy
scores = cross_val_score(ada_boost, train_data, train_labels, cv=5)
print(&quot;Accuracy: %0.2f (+/- %0.2f)\n&quot; % (scores.mean(), scores.std() * 2))
# predict
predict_labels = ada_boost.predict(test_data)
for predict, test_label in zip(predict_labels, test_labels):
    print(predict, '\t', test_label)
</code></pre>

<p>得到结果：</p>

<pre><code class="language-bash">AdaBoostClassifier(algorithm='SAMME',
          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,
            max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=100,
            splitter='best'),
          learning_rate=0.5, n_estimators=10, random_state=200) 

DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,
            max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=100,
            splitter='best') 

Base Learner error and weight:
Base Learner-1   0.0296296296296     2.09102507132
Base Learner-2   0.104626988622      1.41999302436
Base Learner-3   0.111889828638      1.38236413421
Base Learner-4   0.116849602012      1.3578775189
Base Learner-5   0.136963254896      1.26694588583
Base Learner-6   0.180878117849      1.1017783246
Base Learner-7   0.174611689277      1.12321827288
Base Learner-8   0.169743533665      1.14029657839
Base Learner-9   0.273137055613      0.835955706868
Base Learner-10  0.127787913186      1.30690391613

Feature Importance:
sepalLength      0.134583407492
sepalWidth   0.0584706598454
petalLength      0.382968813518
petalWidth   0.423977119144

Accuracy: 0.94 (+/- 0.04)

Predict              Actual
Iris-setosa          Iris-setosa
Iris-virginica       Iris-virginica
Iris-versicolor      Iris-versicolor
Iris-versicolor      Iris-versicolor
Iris-versicolor      Iris-versicolor
Iris-virginica       Iris-virginica
Iris-setosa          Iris-setosa
Iris-setosa          Iris-setosa
Iris-setosa          Iris-setosa
Iris-versicolor      Iris-versicolor
Iris-virginica       Iris-virginica
Iris-virginica       Iris-virginica
Iris-versicolor      Iris-versicolor
Iris-versicolor      Iris-versicolor
Iris-versicolor      Iris-versicolor
</code></pre>

<p><strong><em>8.4 <a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank">GradientBoosting</a>是一种常用的Boosting算法，试分析其与AdaBoost的异同。</em></strong><br />
Ans: GradientBoosting与AdaBoost相同的地方在于要生成多个分类器以及每个分类器都有一个权值，最后将所有分类器加权累加起来。不同在于：AdaBoost通过每个分类器的分类结果改变每个样本的权值用于新的分类器和生成权值，但不改变每个样本不会改变。GradientBoosting将每个分类器对样本的预测值与真实值的差值传入下一个分类器来生成新的分类器和权值(这个差值就是下降方向)，而每个样本的权值不变。</p>

<p><strong><em>8.5 试编程实现Bagging，以决策树桩为基学习器，在西瓜数据集3.0$\alpha$上训练一个Bagging集成。</em></strong><br />
Ans: 这里同样适用UCI数据集的Iris数据进行实验，具体细节见代码。</p>

<pre><code class="language-python">import pandas as pd
import warnings
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split, cross_val_score
warnings.filterwarnings('ignore')

data = pd.read_csv('iris.csv')
labels = data['class']  # labels
data.drop(['class'], axis=1, inplace=True)  # data
# shuffling data and labels
data, labels = shuffle(data, labels, random_state=5)
# split into train and test
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.1, random_state=200)
# create base learner
base_learner = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=2, random_state=100,
                                      max_leaf_nodes=5)
# create bagging classifier
bagging = BaggingClassifier(base_learner, n_estimators=10, bootstrap=True, bootstrap_features=True, oob_score=True,
                            max_samples=0.5, random_state=200, n_jobs=-1)
print(bagging, '\n')  # print bagging configuration
# fit data
bagging.fit(train_data, train_labels)
print(bagging.base_estimator_, '\n')  # print base learner
print('Base Learner Features:')
for idx, feature in zip(range(1, 11), bagging.estimators_features_):
    print('Base Learner-%d\t' % idx, feature)
print('\n')
scores = cross_val_score(bagging, train_data, train_labels, cv=5)
print(&quot;Accuracy: %0.2f (+/- %0.2f)\n&quot; % (scores.mean(), scores.std() * 2))
# predict
predict_labels = bagging.predict(test_data)
for predict, test_label in zip(predict_labels, test_labels):
    print(predict, '\t', test_label)
</code></pre>

<p>输出结果为：</p>

<pre><code class="language-bash">BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,
            max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=100,
            splitter='best'),
         bootstrap=True, bootstrap_features=True, max_features=1.0,
         max_samples=0.5, n_estimators=10, n_jobs=-1, oob_score=True,
         random_state=200, verbose=0, warm_start=False) 

DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,
            max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=100,
            splitter='best') 

Base Learner Features:
Base Learner-1   [1 1 1 0]
Base Learner-2   [1 2 3 0]
Base Learner-3   [3 3 1 1]
Base Learner-4   [2 2 1 2]
Base Learner-5   [3 1 0 2]
Base Learner-6   [2 2 1 2]
Base Learner-7   [3 2 0 2]
Base Learner-8   [3 0 2 0]
Base Learner-9   [1 1 3 3]
Base Learner-10  [1 0 1 3]

Accuracy: 0.93 (+/- 0.03)

Iris-setosa      Iris-setosa
Iris-virginica   Iris-virginica
Iris-versicolor      Iris-versicolor
Iris-versicolor      Iris-versicolor
Iris-versicolor      Iris-versicolor
Iris-virginica   Iris-virginica
Iris-setosa      Iris-setosa
Iris-setosa      Iris-setosa
Iris-setosa      Iris-setosa
Iris-versicolor      Iris-versicolor
Iris-virginica   Iris-virginica
Iris-virginica   Iris-virginica
Iris-versicolor      Iris-versicolor
Iris-versicolor      Iris-versicolor
Iris-versicolor      Iris-versicolor
</code></pre>

<p><strong><em>8.6 试析Bagging通常为何难以提升朴素贝叶斯分类器的性能。</em></strong><br />
Ans: Bagging主要是降低分类器的方差，而朴素贝叶斯分类器没有方差可以减小。对全训练样本生成的朴素贝叶斯分类器是最优的分类器，不能用随机抽样来提高泛化性能。</p>

<p><strong><em>8.7 试析随机森林为何比决策树Bagging集成的训练速度更快。</em></strong><br />
Ans: 随机森林不仅会随机样本，还会在所有样本属性中随机几种出来计算。这样每次生成分类器时都是对部分属性计算最优，速度会比Bagging计算全属性要快。</p>

<p><strong><em>8.8 <a href="http://www.jmlr.org/papers/volume13/benbouzid12a/benbouzid12a.pdf" target="_blank">MultiBoosting</a>算法将AdaBoost作为Bagging的基学习器，<a href="http://infochim.u-strasbg.fr/new/CS3_2010/Tutorial/Ensemble/EnsembleModeling.pdf" target="_blank">Iterative Bagging</a>算法则是将Bagging作为AdaBoost的基学习器。试比较二者的优缺点。</em></strong><br />
Ans: MultiBoosting由于集合了Bagging，Wagging，AdaBoost，可以有效的降低误差和方差，特别是误差。但是训练成本和预测成本都会显著增加。Iterative Bagging相比Bagging会降低误差，但是方差上升。由于Bagging本身就是一种降低方差的算法，所以Iterative Bagging相当于Bagging与单分类器的折中。</p>

<p><strong><em>8.9 试设计一种可视的多样性度量，对8.3和8.5中得到的集成进行评估，并与 $\kappa$-误差图比较。</em></strong><br />
Ans: <strong><em>TODO</em></strong></p>

<p><strong><em>8.10 试设计一种能提升$k$近邻分类器性能的集成学习方法。</em></strong><br />
Ans: 可以使用Bagging来提升k近邻分类器的性能，每次随机抽样出一个子样本，并训练一个k近邻分类器，对测试样本进行分类。最终取最多的一种分类。</p>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/machine-learning">machine-learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/python">python</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/ensemble-learning">ensemble-learning</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/ml_zzh_note_3/">Machine Learning Note (3): Support Vector Machine</a></li>
        
        <li><a href="/post/ml_zzh_note_2/">Machine Learning Note (2): Linear Regression</a></li>
        
        <li><a href="/post/ml_zzh_note_1/">Machine Learning Note (1): Model Evaluation and Selection</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2018 &middot; 

      <strong>Isaac Changhau (Zhang Hao)</strong>
      

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/java.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/javascript.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/c.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/html.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/xml.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ 
          tex2jax: { 
            inlineMath: [['$','$'], ['\\(','\\)']] 
          } 
        });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-xAWI9i8WMRLdgksuhaMCYMTw9D+MEc2cYVBApWwGRJ0cdcywTjMovOfJnlGt9LlEQj6QzyMzpIZLMYujetPcQg==" crossorigin="anonymous"></script>
    
    
    

  </body>
</html>

