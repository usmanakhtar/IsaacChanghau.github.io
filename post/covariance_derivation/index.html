<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.41" />
  <meta name="author" content="Zhang Hao">

  
  
  
  
    
      
    
  
  <meta name="description" content="In probability theory and statistics, covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, i.">

  
  <link rel="alternate" hreflang="en-us" href="https://isaacchanghau.github.io/post/covariance_derivation/">

  


  

  
  
  <meta name="theme-color" content="#3f51b5">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  <link rel="feed" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://isaacchanghau.github.io/post/covariance_derivation/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/IsaacChanghau">
  <meta property="twitter:creator" content="@https://twitter.com/IsaacChanghau">
  
  <meta property="og:site_name" content="Isaac Changhau">
  <meta property="og:url" content="https://isaacchanghau.github.io/post/covariance_derivation/">
  <meta property="og:title" content="Mathematical Derivation of Covariance Matrix | Isaac Changhau">
  <meta property="og:description" content="In probability theory and statistics, covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, i.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-06-17T00:00:00&#43;08:00">
  
  <meta property="article:modified_time" content="2017-06-17T00:00:00&#43;08:00">
  

  

  <title>Mathematical Derivation of Covariance Matrix | Isaac Changhau</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Isaac Changhau</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#tags">
            
            <span>Tags</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#achievements">
            
            <span>Achievements</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Mathematical Derivation of Covariance Matrix</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2017-06-17 00:00:00 &#43;0800 &#43;08" itemprop="datePublished dateModified">
      Sat, Jun 17, 2017
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhang Hao">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  

  

  
  

  

</div>

    
    <div class="article-style" itemprop="articleBody">
      

<p>In probability theory and statistics, <a href="https://en.wikipedia.org/wiki/Covariance" target="_blank">covariance</a> is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, i.e., the variables tend to show opposite behavior, the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. While the normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation.</p>

<h1 id="notation-and-definition">Notation and Definition</h1>

<p>Suppose a set of random variables, $X_{1},X_{2},\dots,X_{n}$, where each random variable has $m$ samples, say, $X_{i}=(x_{i,1},\dots,x_{i,m})$, and denote $\mathbf{X}=(X_{1},X_{2},\dots,X_{n})^{T}$, thus, $\mathbf{X}$ is the random sample matrix:
$$
\mathbf{X}=\begin{bmatrix}
x_{1,1} &amp; x_{1,2} &amp; \dots &amp; x_{1,m} \newline
x_{2,1} &amp; x_{2,2} &amp; \dots &amp; x_{2,m} \newline
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \newline
x_{n,1} &amp; x_{n,2} &amp; \dots &amp; x_{n,m}
\end{bmatrix}_{n\times m}\tag{1}
$$
where the elements in $i$-th row of $\mathbf{X}$, i.e., $x_{i,1},\dots,x_{i,m}$, represent the $m$ samples of $i$-th random variable $X_{i}$. Moreover, let&rsquo;s denote
$$
\alpha_{i}:=(x_{i,1},x_{i,2},\dots,x_{i,m})^{T},i=1,2,\dots,n\tag{2}
$$
$$
\beta_{j}:=(x_{1,j},x_{2,j},\dots,x_{n,j})^{T},j=1,2,\dots,m\tag{3}
$$
so, $\alpha_{i}$ represents the $i$-th <strong>row</strong> of $\mathbf{X}$, while $\beta_{j}$ corresponds to the $j$-th <strong>column</strong> of $\mathbf{X}$, then we will have:
$$
\mathbf{X}=(\beta_{1},\beta_{2},\dots,\beta_{m})=(\alpha_{1},\alpha_{2},\dots,\alpha_{n})^{T}\tag{4}
$$
Below gives some basic concepts which will be used to deduce the covariance matrix:</p>

<p><strong>Mathematical expectation</strong>, $E[\xi]$, is the summation or integration of a possible values from a random variable $\xi$. It is computed by the sum of the product of each posibale value $x_{i}$ and the probability $P(\xi=x_{i})$ of the value occurring.
$$
E[\xi]=\sum_{i}x_{i}P(\xi=x_{i})\tag{5}
$$
Here the mathematical expectation reflects the average of all possible values of the random variable. Simply, assuming all the possible values of $\xi$ are $x_{1},x_{2},\dots,x_{n}$, and they are equiprobable, i.e, $P(\xi=x_{i})=\frac{1}{n}$, $i=1,2,\dots,n$, then we have
$$
E[\xi]=\sum_{i=1}^{n}x_{i}P(\xi=x_{i})=\frac{1}{n}\sum_{i=1}^{n}x_{i}\tag{6}
$$
Besides, mathematical expectation operator is <strong>linear</strong> in the sense that
$$
E[a\xi+b\eta]=aE[\xi]+bE[\eta]\tag{7}
$$
and $\xi-E[\xi]$ is called <strong>dispersion</strong> of random variable $\xi$.</p>

<p><strong>Variance</strong>, denote as $D[\xi]$, or $var[\xi]$, is the expectation of the squared deviation of a random variable $\xi$ from its mean
$$
var[\xi]=E\big[(\xi-E[\xi])^{2}\big]\tag{8}
$$
It is non-negative, and informally measures how far a set of (random) numbers are spread out from their mean. Thus, the variance can be used to infer the rate of divergence of distribution of random varibale.</p>

<p><strong>Covariance</strong> is a measure of the joint variability of two random variables. Considering two random variables $X_{i}$ and $X_{j}$, their covariance is defined as
$$
cov(X_{i},X_{j})=E\big[(X_{i}-E[X_{i}])\cdot(X_{j}-E[X_{j}])\big]\tag{9}
$$
and we have</p>

<ul>
<li>$cov(X_{i},X_{j})=cov(X_{j},X_{i})$</li>
<li>$cov(X_{i},X_{i})=var(X_{i})$</li>
</ul>

<p>Futher, accoding to the linearity of expectation, we have
$$
\begin{aligned}
cov(X_{i},X_{j})
&amp; = E\big[(X_{i}-E[X_{i}])\cdot(X_{j}-E[X_{j}])\big]\newline
&amp; = E\big[X_{i}X_{j}-E[X_{j}]X_{i}-E[X_{i}]X_{j}+E[X_{i}]E[X_{j}]\big]\newline
&amp; = E[X_{i}X_{j}]-E[X_{j}]E[X_{i}]-E[X_{i}]E[X_{j}]+E[X_{i}]E[X_{j}]\newline
&amp; = E[X_{i}X_{j}]-E[X_{i}]E[X_{j}]
\end{aligned}\tag{10}
$$
Thus, $cov(X_{i},X_{j})=E[X_{i}X_{j}]-E[X_{i}]E[X_{j}]$.</p>

<p><strong>Independence</strong>: If $X_{i}$ and $X_{j}$ are independent, then their covariance is zero. This follows because under independence, we have
$$
E[X_{i},X_{j}]=E[X_{i}]E[X_{j}]
$$</p>

<p><strong>Coherence</strong>: assume that the random variables $X_{i}$ and $X_{j}$ santisfy $var[X_{i}]var[X_{j}]\neq 0$, then the correlation coefficient is defined as
$$
\rho_{X_{i},X_{j}}=\frac{cov[X_{i},X_{j}]}{\sqrt{var[X_{i}]}\cdot\sqrt{var[X_{j}]}}\tag{11}
$$
if $\rho_{X_{i},X_{j}}=0$, $X_{i}$ and $X_{j}$ areuncorrelated, otherwise, they are correlated. Correlation coefficient, also named as linearly correlation coefficient, since the correlation coefficient describes the degree of &ldquo;linear&rdquo; relation between $X_{i}$ and $X_{j}$.</p>

<p>Meanwhile, it is worth to mention that for two random variables $X_{i}$ and $X_{j}$, $X_{i}$ and $X_{j}$ are independent is the sufficient but not necessary condition of uncorrelated, say, if $X_{i}$ and $X_{j}$ are independent, they must be uncorrelated, conversely, it does not always stand up. Thus, we can derive the relationship: <strong>independent</strong> $\Rightarrow$ <strong>uncorrelated</strong> $\Leftrightarrow$ $cov(X_{i},X_{j})=0$.</p>

<p>For a $n$ dimensional random variable $\mathbf{X}=(X_{1},X_{2},\dots,X_{n})^{T}$, its covariance matrix is defined as:
$$
C:=C(\mathbf{X})=(c_{i,j})_{n\times n}=\begin{bmatrix}
cov(X_{1},X_{1}) &amp; cov(X_{1},X_{2}) &amp; \dots &amp; cov(X_{1},X_{n}) \newline
cov(X_{2},X_{1}) &amp; cov(X_{2},X_{2}) &amp; \dots &amp; cov(X_{2},X_{n}) \newline
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \newline
cov(X_{n},X_{1}) &amp; cov(X_{n},X_{2}) &amp; \dots &amp; cov(X_{n},X_{n})
\end{bmatrix}_{n\times n}\tag{12}
$$
where $c_{i,j}=cov(X_{i},X_{j})$, obviously, $C$ is a symmetric matrix, and its diagonal elements represent variance, non-diagonal elements represent the covariance among different components of $\mathbf{X}$. If the coherence of two components are small, then the corresponding value in $C$ is small either. Specifically, if all the components are uncorrelated with each other, $C$ becomes a diagonal matrix.</p>

<p>Note that, we cannot derive the actual value of $C(\mathbf{X})$, but approximately estimate its value according to the sample data provided in $\mathbf{X}$, which means the computed covariance matrix relys on the sample data, normally, the larger the sample data ($m$ is larger), the broader th coverage, and the covariance metrix is more reliable.</p>

<h1 id="mathematical-derivation">Mathematical Derivation</h1>

<p>According to (6), $E[X_{i}]$ and $E[X_{j}]$ can be approximately written as
$$
E[X_{i}]=\frac{1}{m}\sum_{k=1}^{m}x_{i,k},\qquad
E[X_{j}]=\frac{1}{m}\sum_{k=1}^{m}x_{j,k}\tag{13}
$$
Let $Y=(X_{i}-E[X_{i}])\cdot(X_{j}-E[X_{j}]):=(Y_{1},Y_{2},\dots,Y_{m})$, where $Y_{k}=(x_{i,k}-E[X_{i}])\cdot(x_{j,k}-E[X_{j}])$, $k=1,2,\dots,m$, with (13), we can derive:
$$
\begin{aligned}
c_{i,j}
&amp; =E[Y]=\frac{1}{m}\sum_{k=1}^{m}Y_{k}\newline
&amp; =\frac{1}{m}\sum_{k=1}^{m}(x_{i,k}-E[X_{i}])\cdot(x_{j,k}-E[X_{j}])\newline
&amp; =\frac{1}{m}\sum_{k=1}^{m}\bigg[\big(x_{i,k}-\frac{1}{m}\sum_{p=1}^{m}x_{i,p}\big)\cdot\big(x_{j,k}-\frac{1}{m}\sum_{q=1}^{m}x_{j,q}\big)\bigg]
\end{aligned}\tag{14}
$$
Then we need to simply the formula (14), as describe below:
$$
\begin{aligned}
c_{i,j}
&amp; = \frac{1}{m}\sum_{k=1}^{m}\bigg[\big(x_{i,k}-\frac{1}{m}\sum_{p=1}^{m}x_{i,p}\big)\cdot\big(x_{j,k}-\frac{1}{m}\sum_{q=1}^{m}x_{j,q}\big)\bigg]\newline
&amp; = \frac{1}{m}\sum_{k=1}^{m}\bigg[x_{i,k}\cdot x_{j,k}-x_{i,k}\cdot\frac{1}{m}\sum_{q=1}^{m}x_{j,q}-x_{j,k}\cdot\frac{1}{m}\sum_{p=1}^{m}x_{i,p}+\frac{1}{m}\sum_{p=1}^{m}x_{i,p}\cdot\frac{1}{m}\sum_{q=1}^{m}x_{j,q}\bigg]\newline
&amp; = \frac{1}{m}\sum_{k=1}^{m}x_{i,k}\cdot x_{j,k}-\frac{1}{m}\sum_{k=1}^{m}x_{i,k}\cdot\frac{1}{m}\sum_{q=1}^{m}x_{j,q}-\frac{1}{m}\sum_{k=1}^{m}x_{j,k}\cdot\frac{1}{m}\sum_{p=1}^{m}x_{i,p}+\frac{1}{m}\sum_{k=1}^{m}\frac{1}{m}\sum_{p=1}^{m}x_{i,p}\cdot\frac{1}{m}\sum_{q=1}^{m}x_{j,q}\newline
&amp; =\frac{1}{m}\sum_{k=1}^{m}x_{i,k}\cdot x_{j,k}-\frac{1}{m^{2}}\sum_{k=1}^{m}x_{i,k}\cdot\sum_{k=1}^{m}x_{j,k}-\frac{1}{m^{2}}\sum_{k=1}^{m}x_{j,k}\cdot\sum_{k=1}^{m}x_{i,k}+\frac{1}{m^{3}}\sum_{k=1}^{m}1\cdot\sum_{p=1}^{m}x_{i,p}\cdot\sum_{q=1}^{m}x_{j,q}\newline
&amp; = \frac{1}{m}\sum_{k=1}^{m}x_{i,k}\cdot x_{j,k}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\cdot\big(\sum_{k=1}^{m}x_{j,k}\big)-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{j,k}\big)\cdot\big(\sum_{k=1}^{m}x_{i,k}\big)+\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\cdot\big(\sum_{k=1}^{m}x_{j,k}\big)\newline
&amp; = \frac{1}{m}\sum_{k=1}^{m}x_{i,k}\cdot x_{j,k}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\cdot\big(\sum_{k=1}^{m}x_{j,k}\big)\newline
&amp; =\frac{1}{m}\alpha_{i}^{T}\alpha_{j}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\cdot\big(\sum_{k=1}^{m}x_{j,k}\big)
\end{aligned}\tag{15}
$$
Here we derive the representation of element $c_{i,j}$ in $C$, in order to obtain a more terse expression of $C$, we first introduce two temporary matrix $A$ and $B$, make $C=A-B$, and simply $A$ and $B$ seperately.</p>

<p>For matrix $A=(a_{i,j})_{n\times n}$, where
$$
a_{i,j}=\frac{1}{m}\alpha_{i}^{T}\alpha_{j}\tag{16}
$$
according to (4), we have
$$\begin{aligned}
A &amp; =\frac{1}{m}
\begin{pmatrix}\alpha_{1}^{T}\newline
\alpha_{2}^{T}\newline
\vdots\newline
\alpha_{n}^{T}
\end{pmatrix}
(\alpha_{1},\alpha_{2},\dots,\alpha_{n})\newline
&amp; =\frac{1}{m}\mathbf{X}\mathbf{X}^{T}\newline
&amp; =\frac{1}{m}(\beta_{1},\beta_{2},\dots,\beta_{m})(\beta_{1},\beta_{2},\dots,\beta_{m})^{T}\newline
&amp; =\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}
\end{aligned}\tag{17}
$$
Then for matrix $B=(b_{i,j})_{n\times n}$, where
$$
b_{i,j}=\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\cdot\big(\sum_{k=1}^{m}x_{j,k}\big)\tag{18}
$$
we have
$$\begin{aligned}
B &amp; =\frac{1}{m^{2}}
\begin{pmatrix}
\sum_{k=1}^{m}x_{1,k}\newline
\sum_{k=1}^{m}x_{2,k}\newline
\vdots\newline
\sum_{k=1}^{m}x_{n,k}
\end{pmatrix}
(\sum_{k=1}^{m}x_{1,k},\sum_{k=1}^{m}x_{2,k},\dots,\sum_{k=1}^{m}x_{n,k})\newline
&amp; =\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{i=1}^{m}\beta_{i}\big)^{T}
\end{aligned}\tag{19}
$$
Since we suppose that $C=A-B$, so according to (17) and(19), we can write $C$ as
$$
C=\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}
-\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{i=1}^{m}\beta_{i}\big)^{T}\tag{20}
$$
Actually, the formula (20) still can be simplified to merge the two terms as one term:
$$\begin{aligned}
C
&amp; =\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}-\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{k=1}^{m}\beta_{k}\big)^{T}\newline
&amp; =\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}-\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{k=1}^{m}\beta_{k}\big)^{T}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\sum_{i=1}^{m}\beta_{i}\big)^{T}+\frac{1}{m^{2}}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\sum_{i=k}^{m}\beta_{k}\big)^{T}\newline
&amp; =\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}-\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{k=1}^{m}\beta_{k}\big)^{T}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\sum_{i=1}^{m}\beta_{i}\big)^{T}+\frac{1}{m^{3}}\sum_{i=1}^{m}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\sum_{i=k}^{m}\beta_{k}\big)^{T}\newline
&amp; = \frac{1}{m}\bigg\{\beta_{i}\beta_{i}^{T}-\frac{1}{m}\beta_{i}\big(\sum_{k=1}^{m}\beta_{k}\big)^{T}-\frac{1}{m}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\beta_{i}^{T}\big)+\big(\frac{1}{m}\sum_{k=1}^{m}\beta_{k}\big)\big(\frac{1}{m}\sum_{i=k}^{m}\beta_{k}\big)^{T}\bigg\}\newline
&amp; =\frac{1}{m}\sum_{i=1}^{m}\big(\beta_{i}-\frac{1}{m}\sum_{k=1}^{m}\beta_{k}\big)\big(\beta_{i}^{T}-\frac{1}{m}\sum_{k=1}^{m}\beta_{k}^{T}\big)\newline
&amp; =\frac{1}{m}\sum_{i=1}^{m}\big(\beta_{i}-\beta_{0}\big)\big(\beta_{i}-\beta_{0}\big)^{T}
\end{aligned}\tag{21}
$$
where $\beta_{0}=\frac{1}{m}\sum_{k=1}^{m}\beta_{k}$, specifically, if the average of $m$ samples are zero for every random variable in $\mathbf{X}$, i.e., $\sum_{k=1}^{m}x_{i,k}=0$, then according to (18), we have $b_{i,j}=0$, further, $B=0$. In this case, we have $c_{i,j}=a_{i,j}=\frac{1}{m}\alpha_{i}^{T}\alpha_{j}$,
$$
C=A=\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}=\frac{1}{m}\mathbf{X}\mathbf{X}^{T}\tag{22}
$$
This is the overall mathematical process to deduce the covariance matrix.</p>

<h1 id="reference">Reference</h1>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Covariance" target="_blank">Covariance</a></li>
<li><a href="https://en.wikipedia.org/wiki/Covariance_matrix" target="_blank">Covariance Matrix</a></li>
<li><a href="http://www.statisticssolutions.com/directory-of-statistical-analyses-mathematical-expectation/" target="_blank">Mathematical expectation</a></li>
</ul>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/algorithms">algorithms</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/covariance">covariance</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/skiing_in_singapore/">Skiing In Singapore</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2018 &middot; 

      <strong>Isaac Changhau (Zhang Hao)</strong>
      

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/java.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/javascript.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/c.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/html.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/xml.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ 
          tex2jax: { 
            inlineMath: [['$','$'], ['\\(','\\)']] 
          } 
        });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-xAWI9i8WMRLdgksuhaMCYMTw9D+MEc2cYVBApWwGRJ0cdcywTjMovOfJnlGt9LlEQj6QzyMzpIZLMYujetPcQg==" crossorigin="anonymous"></script>
    
    
    

  </body>
</html>

