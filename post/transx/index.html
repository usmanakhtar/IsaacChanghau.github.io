<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.45" />
  <meta name="author" content="Zhang Hao">

  
  
  
  
    
      
    
  
  <meta name="description" content="It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of methods are called TransX. Here, I only describe the general idea and mathematical process of each method, for more information about those methods in parameters setting, experiment results and discussion, you can directly read the original papers through the links I provide.">

  
  <link rel="alternate" hreflang="en-us" href="https://isaacchanghau.github.io/post/transx/">

  


  

  
  
  <meta name="theme-color" content="#3f51b5">
  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Playfair&#43;Display:400,700%7cFauna&#43;One">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  <link rel="feed" href="https://isaacchanghau.github.io/index.xml" type="application/rss+xml" title="Isaac Changhau">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://isaacchanghau.github.io/post/transx/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@https://twitter.com/IsaacChanghau">
  <meta property="twitter:creator" content="@https://twitter.com/IsaacChanghau">
  
  <meta property="og:site_name" content="Isaac Changhau">
  <meta property="og:url" content="https://isaacchanghau.github.io/post/transx/">
  <meta property="og:title" content="TransX: Embedding Entities and Relationships of Multi-relational Data | Isaac Changhau">
  <meta property="og:description" content="It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of methods are called TransX. Here, I only describe the general idea and mathematical process of each method, for more information about those methods in parameters setting, experiment results and discussion, you can directly read the original papers through the links I provide.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-09-14T00:00:00&#43;08:00">
  
  <meta property="article:modified_time" content="2017-09-14T00:00:00&#43;08:00">
  

  

  <title>TransX: Embedding Entities and Relationships of Multi-relational Data | Isaac Changhau</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Isaac Changhau</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#tags">
            
            <span>Tags</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#achievements">
            
            <span>Achievements</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">TransX: Embedding Entities and Relationships of Multi-relational Data</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2017-09-14 00:00:00 &#43;0800 &#43;08" itemprop="datePublished dateModified">
      Thu, Sep 14, 2017
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhang Hao">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    14 min read
  </span>
  

  
  

  

  
  

  

</div>

    
    <div class="article-style" itemprop="articleBody">
      

<p>It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of methods are called TransX. Here, I only describe the general idea and mathematical process of each method, for more information about those methods in parameters setting, experiment results and discussion, you can directly read the original papers through the links I provide.</p>

<h1 id="multi-relational-data-overview">Multi-relational Data Overview</h1>

<p>There are a lot of relational knowledge database available nowadays, like:</p>

<ul>
<li><a href="http://conceptnet.io" target="_blank">ConceptNet</a>, which is a freely-available semantic network, designed to help computers understand the meanings of words that people use.</li>
<li><a href="https://wordnet.princeton.edu" target="_blank">WordNet</a>, which is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. WordNet&rsquo;s structure makes it a useful tool for computational linguistics and natural language processing.</li>
<li><a href="https://verbs.colorado.edu/verbnet/" target="_blank">VerbNet</a>, which is the largest on-line verb lexicon currently available for English. It is a hierarchical domain-independent, broad-coverage verb lexicon with mappings to other lexical resources such as WordNet, <a href="http://www.cis.upenn.edu/~xtag/" target="_blank">Xtag</a> and <a href="https://framenet.icsi.berkeley.edu/fndrupal/" target="_blank">FrameNet</a>.</li>
<li><a href="https://en.wikipedia.org/wiki/Freebase" target="_blank">FreeBase</a>, which is a large collaborative knowledge base consisting of data composed mainly by its community members. It was an online collection of structured data harvested from many sources, including individual, user-submitted wiki contributions. Freebase aimed to create a global resource that allowed people (and machines) to access common information more effectively.</li>
<li><a href="https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/" target="_blank">YaGo</a>, which is a huge semantic knowledge base, derived from Wikipedia, WordNet and GeoNames. It has knowledge of more than 10 million entities and contains more than 120 million facts about these entities.</li>
</ul>

<p>Actually, there are still a lot of different relational knowledge database exist, here we only show some representative databases above. Before we discuss those algorithms, let&rsquo;s see some examples of multi-relational knowledge database to get the idea about how the database looks like.
<img src="/img/nlp/transx/conceptnet-wordnet.png" alt="conceptnet and wordnet" />
In the graph above, I show an example of two knowledge DB. For ConceptNet, each entity (or, node) is a concept, and entities are connected to each other through some specific relations, like, <code>oven--UsedFor--&gt;cook</code>, <code>cake--IsA--&gt;dessert</code>, here <code>oven</code>, <code>cook</code>, <code>cake</code> and <code>dessert</code> are entities, while <code>UsedFor</code> and <code>IsA</code> are relations. For WordNet, there are two different entities, one is called Synset (blue node), another is Word (green one). Each synset in WordNet is connected with other synsets through <strong>hypernym</strong> and <strong>hyponym</strong> relations, while Word only connect to its own synset and has some links with other words in the same synset, but never connected to words outside.</p>

<p>Although there are amount of relations and connections within a knowledge database, generally, all of the relations are in the several certain forms, like <strong>one-to-many relation</strong>, <strong>many-to-one relation</strong>, <strong>one-to-one relation</strong>, <strong>co-relation</strong>, <strong>reflexive relation</strong> and so on.
<img src="/img/nlp/transx/relation-forms.png" alt="relation-forms" />
For example, one-to-many relation, which means one entity with one relation, links to several different entities, say, <code>apple--is_a--&gt;fruit</code>, <code>apple--is_a--&gt;computer_brand</code>, <code>apple--is_a--&gt;computer_manufacturer</code> and so on. For many-to-one relation, it is similar. However, the reflexive relation is unique, since two entities connect to each other with same relation, like <code>plate--is_a-&gt;dish</code>, while <code>dish--is_a--&gt;plate</code> too.</p>

<p>Dispite the scale of relational knowledge databases and how many different relation forms they have, all of them are able to be decomposed to the <strong>basic component</strong> (triple), i.e., an entity connect to another entity with a certain relation, as shown below
<img src="/img/nlp/transx/basic-component.png" alt="basic-component" />
In order to convert those relational data into embeddings, which are convenient and easy to access via statistical approach, researchers proposed several methods to handle this issue, like <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Nickel_438.pdf" target="_blank">RESCAL</a>, <a href="https://pdfs.semanticscholar.org/057a/c29c84084a576da56247bdfd63bf17b5a891.pdf" target="_blank">SE</a>, <a href="http://www.thespermwhale.com/jaseweston/papers/ebrm_mlj.pdf" target="_blank">SME(linear)</a>, <a href="http://www.thespermwhale.com/jaseweston/papers/ebrm_mlj.pdf" target="_blank">SME(bilinear)</a>, <a href="https://hal.inria.fr/hal-00776335/document" target="_blank">LFM</a>, <a href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank">TransE</a>, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.486.2800&amp;rep=rep1&amp;type=pdf" target="_blank">TransH</a>, <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571/9523" target="_blank">TransR</a>, <a href="http://www.aclweb.org/anthology/P15-1067" target="_blank">TransD</a> and so on, while the TransE, TransH, TransR, TransD are a group of similar methods, thus, we put them together and named as <strong>TransX</strong>. So, TransX is a set of methods to create embeddings for entities and relations while remembering their connection information.</p>

<h1 id="transe">TransE</h1>

<p>TransE &ndash; <a href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank">Translating Embeddings for Modeling Multi-relational Data</a>.</p>

<p>This paper considers the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces, its objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. So the TransE is proposed, which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities.</p>

<p>TransE, an energy-based model for learning low-dimensional embeddings of entities.  In TransE, relationships are represented as translations in the embedding space:</p>

<blockquote>
<p>if $(h,r,t)$ holds, then the embedding of the tail entity $t$ should be <strong>close to</strong> the embedding of the head entity $h$ <strong>plus</strong> some vector that depends on the relationship $r$, while it is also the general idea of the training process of TransE.</p>
</blockquote>

<p>The main motivation behind this translation-based parameterization is that hierarchical relationships are extremely common in KBs and translations are the natural transformations for representing them. Another motivation comes from <a href="https://arxiv.org/abs/1310.4546" target="_blank">Distributed Representations of Words and Phrases and Their Compositionality</a>, in which the authors learn word embeddings from free text, and some <em>1-to-1</em> relationships between entities of different types are (coincidentally rather than willingly) represented by the model as translations in the embedding space. This suggests that there may exist embedding spaces in which <em>1-to-1</em> relationships between entities of different types may, as well, be represented by translations.</p>

<p><strong>Translation-based model</strong>:
Given a training set $S$ of triplets $(h,r,t)$ composed of two entities $h,t\in E$ (the set of entities) and a relationship $r\in R$ (the set of relationships), TransE learns vector embeddings of the entities and the relationships. The embeddings take values in $\mathbb{R}^{k}$ ($k$ is a model hyperparameter) and are denoted with the same letters, in boldface characters.</p>

<p>The basic idea behind the model is that the functional relation induced by the $r$-labeled edges corresponds to a translation of the embeddings, i.e. <strong>$h+r\approx t$ when $(h,r,t)$ holds ($t$ should be a nearest neighbor of $h+r$), while $h+r$ should be far away from $t$ otherwise</strong>, as the graph below shows. Following an energy-based framework, the energy of a triplet is equal to $d(h+r,t)$ for some dissimilarity measure $d$, which we take to be either the $L_{1}$ or the $L_{2}$-norm.
<img src="/img/nlp/transx/transe-translation-graph.png" alt="TransE Translation" />
To learn such embeddings, we minimize a margin-based ranking criterion over the training set:
$$
\mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h&rsquo;,r,t&rsquo;)\in S&rsquo;_{(h,r,t)}}\big[\gamma+d(h+r,t)-d(h&rsquo;+r,t&rsquo;)\big]_{+}\tag{1}
$$
where $[x]_{+}$ denotes the positive part of $x$, $\gamma&gt;0$ is a margin hyperparameter, and the dissimilarity measure $d$ is the squared euclidean distance, which computed by
$$
d(h+r,t)=\lVert h\rVert^{2}_{2}+\lVert r\rVert^{2}_{2}+\lVert t\rVert^{2}_{2}-2\big(h^{T}t+r^{T}(t-h)\big)\tag{2}
$$
with the norm constraints that $\lVert h\rVert^{2}_{2}=\lVert t\rVert^{2}_{2}=1$, and the set of corrupted triplets, constructed according to the equation (3) below, is composed of training triplets with either the head or tail replaced by a random entity (but not both at the same time)
$$
S&rsquo;_{(h,r,t)}=\big\{(h&rsquo;,r,t)\vert h&rsquo;\in E\big\}\bigcup\big\{(h,r,t&rsquo;)\vert t&rsquo;\in E\big\}\tag{3}
$$
The loss function (1) favors lower values of the energy for training triplets than for corrupted triplets, and is thus a natural implementation of the intended criterion.</p>

<p>The optimization is carried out by stochastic gradient descent (in minibatch mode), over the possible $h$, $r$, and $t$, with the additional constraints that the $L_{2}$-norm of the embeddings of the entities is 1 (no regularization or norm constraints are given to the label embeddings $r$). This constraint is important, because it prevents the training process to trivially minimize $\mathcal{L}$ by artificially increasing entity embeddings norms. The detailed optimization procedure is described in the graph below.
<img src="/img/nlp/transx/process.png" alt="Process Graph" /></p>

<h1 id="transh">TransH</h1>

<p>TransH &ndash; <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.486.2800&amp;rep=rep1&amp;type=pdf" target="_blank">Knowledge Graph Embedding by Translating on Hyperplanes</a>.</p>

<p>Before talking about TransH, let&rsquo;s see some problems that TransE holds. TransE models a relation $r$ as a translation vector $r\in\mathbb{R}^{k}$, and assumes the error $\Vert h+r-t\Vert_{l_{1}/l_{2}}$ is low if $(h,r,t)$ is a golden triple. It applies well to irreflexive and one-to-one relations but has problems when dealing with reflexive or many-to-one, one-to-many, many-to-many relations. Considering the ideal case of no-error embedding where $h+r-t=0$, if $(h,r,t)\in\Delta$, we can get the following consequences directly from TransE model.</p>

<ul>
<li>If $(h,r,t)\in\Delta$ and $(t,r,h)\in\Delta$, i.e., $r$ is a reflexive map, then $r=0$ and $h=t$.</li>
<li>If $\forall i\in\{0,1,\dots,m\}$, $(h_{i},r,t)\in\Delta$, i.e., $r$ is a many-to-one map, then $h_{0}=\dots=h_{m}$. Similarly, if $(h,r,t_{i})\in\Delta$, i.e., $r$ is a one-to-many map, then $t_{0}=\dots=t_{m}$.</li>
</ul>

<p>The reason leading to the above consequences is, in TransE, the representation of an entity is the same when involved in any relations, ignoring <em>distributed representations of entities when involved in different relations</em>. Hence, TransH is proposed to handle the problems of TransE in modeling reflexive, one-to-many, many-to-one and many-to-many relations. The general idea of TransH is shown below, which introduces a <strong>relation-specific hyperplane</strong> to project the entities to the hyperplane, and the translation process is done in such hyperplane too. For a relation $r$, the authors position the relation-specific translation vector $d_{r}$ in the relation-specific hyperplane $w_{r}$ (the normal vector) rather than in the same space of entity embeddings.
<img src="/img/nlp/transx/transh-graph.png" alt="TransH Graph" />
Specifically, for a triplet $(h,r,t)$, the embedding $h$ and $t$ are first projected to the hyperplane $w_{r}$. The projections are denoted as $h_{\bot}$ and $t_{\bot}$, respectively, while
$$
\begin{aligned}
h_{\bot} &amp;=h-w_{r}^{T}hw_{r}\newline
t_{\bot} &amp;=t-w_{r}^{T}tw_{r}
\end{aligned}
$$
The authors expect $h_{\bot}$ and $t_{\bot}$ can be connected by a translation vector $d_{r}$ on the hyperplane with low error if $(h,r,t)$ is a golden triplet. And the graph below shows how the TransH sloves the problems in TransE.
<img src="/img/nlp/transx/transh-solve-problem.png" alt="TransH Solve Problems" />
Thus, in TransH, by introducing the mechanism of projecting to the relation-specific hyperplane, it enables different roles of an entity in different relations/triplets. Generally, the training process of TransH is similar to TransE, its cost function is also a margin-based ranking loss
$$
\mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h&rsquo;,r,t&rsquo;)\in S&rsquo;_{(h,r,t)}}\big[f_{r}(h,t)+\gamma-f_{r&rsquo;}(h&rsquo;,t&rsquo;)\big]_{+}\tag{4}
$$
where $[x]_{+}=\max(0,x)$, $S$ is a set of golden triples, $S&rsquo;_{(h,r,t)}$ denotes the set of negative triplets constructed by corrupting $(h,r,t)$, $\gamma$ is the margin separating positive and negative triplets, and $f_{r}(h,t)$ is a score function and derived by
$$
f_{r}(h,t)=\Vert h_{\bot}+d_{r}-t_{\bot}\Vert_{2}^{2}=\Vert(h-w_{r}^{T}hw_{r})+d_{r}-(t-w_{r}^{T}tw_{r})\Vert_{2}^{2}\tag{5}
$$
When minimizing the loss $\mathcal{L}$, the following constraints are considered:</p>

<ul>
<li>$\forall e\in E$, $\Vert e\Vert_{2}\leq 1$</li>
<li>$\forall r\in R$, $\vert w_{r}^{T}d_{r}\vert/\Vert d_{r}\Vert_{2}\leq\epsilon$</li>
<li>$\forall r\in R$, $\Vert w_{r}\Vert_{2}=1$</li>
</ul>

<p><strong>Reducing False Negative Labels</strong>:
Here shows a new method to sample the corrupted triples to reduce the false.</p>

<ol>
<li>Give more chance to replacing the head entity if the relation is one-to-many and give more chance to replacing the tail entity if the relation is many-to-one.</li>
<li>Among all the triplets of a relation r, we first get the following two statistics:

<ol>
<li>the average number of tail entities per head entity, denoted as $tph$</li>
<li>the average number of head entities per tail entity, denoted as $hpt$</li>
</ol></li>
</ol>

<p>Then defining a Bernoulli distribution with parameter $\frac{thp}{thp+hpt}$ for sampling: given a golden triplet $(h,r,t)$ of the relation $r$, with probability $\frac{thp}{thp+hpt}$ to corrupt the triplet by replacing the head, and with probability $\frac{pht}{thp+hpt}$ to corrupt the triplet by replacing the tail.</p>

<h1 id="transr-and-ctransr">TransR and CTransR</h1>

<p>TransR &ndash; <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571/9523" target="_blank">Learning Entity and Relation Embeddings for Knowledge Graph Completion</a>.</p>

<p>TransR is proposed since the authors realize that TransH still have some limitations, since both TransE and TransH assume embeddings of entities and relations being in the same space. However, an entity may have multiple aspects, and various relations focus on different aspects of entities. Hence, it is intuitive that some entities are similar and thus close to each other in the entity space, but are <strong>comparably different in some specific aspects</strong> and thus <strong>far away from each other in the corresponding relation spaces</strong>.
<img src="/img/nlp/transx/limitation-transh.png" alt="TransH Limitations" />
Thus, TransR models entities and relations in distinct spaces, i.e., entity space and multiple relation spaces (i.e., relation-specific entity spaces), and performs translation in the corresponding relation space. In TransR, for each triple $(h,r,t)$, entities embeddings are set as $h,t\in\mathbb{R}^{k}$, and relation embedding is set as $r\in\mathbb{R}^{d}$, while the dimensions of entity embeddings and relation embeddings are not necessarily identical. For each relation $r$, set a projection matrix $M_{r}\in\mathbb{R}^{k\times d}$ maps entities from entity space to relation space.
<img src="/img/nlp/transx/transr.png" alt="TransR Limitations" />
With the mapping matrix, the projected vectors of entities are computed by
$$
h_{r}=hM_{r},\quad t_{r}=tM_{r}
$$
And the score function is correspondingly defined as
$$
f_{r}(h,t)=\Vert h_{r}+r-t_{r}\Vert_{2}^{2}\tag{6}
$$
with the constraints that $\forall h,r,t$, $\Vert h\Vert_{2}\leq 1$, $\Vert r\Vert_{2}\leq 1$, $\Vert t\Vert_{2}\leq 1$, $\Vert hM_{r}\Vert_{2}\leq 1$, $\Vert tM_{r}\Vert_{2}\leq 1$. And the following margin-based score function as objective for training:
$$
\mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h&rsquo;,r,t&rsquo;)\in S&rsquo;_{(h,r,t)}}\max\big[0, f_{r}(h,t)+\gamma-f_{r}(h&rsquo;,t&rsquo;)\big]\tag{7}
$$
where $\max(x, y)$ aims to get the maximum between $x$ and $y$, $\gamma$ is the margin, $S$ is the set of correct triples and $S&rsquo;$ is the set of incorrect triples.</p>

<p><strong>Cluster-based TransR (CTransR)</strong>:</p>

<p>The above mentioned models, including TransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse. In order to better model these relations, the authors incorporate the idea of piecewise linear regression to extend TransR.</p>

<p>The basic idea is that first segment input instances into several groups. Formally, for a specific relation $r$, all entity pairs $(h,t)$ in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar $r$ relation. All entity pairs $(h,t)$ are represented with their vector offsets $(h−t)$ for clustering, where $h$ and $t$ are obtained with TransE. Afterwards, learn a separate relation vector $r_{c}$ for each cluster and matrix $M_{r}$ for each relation, respectively. The authors define the projected vectors of entities as $h_{r,c}=hM_{r}$, $t_{r,c}=tM_{r}$ and the score function is defined as
$$
f_{r}(h,t)=\Vert h_{r,c}+r_{c}-t_{r,c}\Vert_{2}^{2}+\alpha\Vert r_{c}-r\Vert_{2}^{2}\tag{8}
$$
where $\Vert r_{c}-r\Vert_{2}^{2}$ aims to ensure cluster-specific relation vector $r_{c}$ not too far away from the original relation vector $r$, and $\alpha$ controls the effect of this constraint. Besides, same to TransR, CTransR also enforce constraints on norm of embeddings $h$, $r$, $t$ and mapping matrices.</p>

<h1 id="transd">TransD</h1>

<p>TransD &ndash; <a href="http://www.aclweb.org/anthology/P15-1067" target="_blank">Knowledge Graph Embedding via Dynamic Mapping Matrix</a></p>

<p>Despite that TransR/CTransR has significant improvements compared with previous state-of-the-art models. However, it also has several flaws:</p>

<ol>
<li>For a typical relation $r$, all entities share the same mapping matrix $M_{r}$. However, the entities linked by a relation always contains various types and attributes.</li>
<li>The projection operation is an interactive process between an entity and a relation, it is unreasonable that the mapping matrices are determined only by relations.</li>
<li>Matrix-vector multiplication makes it has large amount of calculation, and when relation number is large, it also has much more parameters than TransE and TransH.</li>
</ol>

<p>To solve these flaws, TransD is proposed and its basic idea is shown in the graph below. In TransD, <strong>two vectors</strong> are defined for each entity and relation. The first vector represents the meaning of an entity or a relation, the other one (called <strong>projection vector</strong>) represents the way that how to project a entity embedding into a relation vector space and it will be used to construct mapping matrices. Therefore, every entity-relation pair has an unique mapping matrix. In addition, TransD has no matrix-by-vector operations which can be replaced by vectors operations.
<img src="/img/nlp/transx/transd-graph.png" alt="TransD Graph" />
In the graph above, each shape represents an entity pair appearing in a triplet of relation $r$. $M_{rh}$ and $M_{rt}$ are mapping matrices of $h$ and $t$, respectively. $h_{ip}$, $t_{ip}$ ($i=1,2,3$) and $r_{p}$ are projection vectors. $h_{i\bot}$ and $t_{i\bot}$ ($i=1,2,3$) are projected vectors of entities. The projected vectors satisfy $h_{i\bot}+r\approx t_{i\bot}$ ($i=1,2,3$).</p>

<p><strong>TransD Model</strong>: each named symbol object (entities and relations) is represented by <em>two</em> vectors. The first one captures the meaning of entity (relation), the other one is used to construct mapping matrices. For example, given a triplet $(h,r,t)$, its vectors are $h$, $h_{p}$, $r$, $r_{p}$, $t$, $t_{p}$, where subscript $p$ marks the projection vectors, $h,h_{p},t,t_{p}\in\mathbb{R}^{n}$ and $r,r_{p}\in\mathbb{R}^{m}$. For each triplet $(h,r,t)$ the authors set two mapping matrices $M_{rh},M_{rt}\in\mathbb{R}^{m\times n}$ to project entities from entity space to relation space:
$$
\begin{aligned}
M_{rh}&amp;=r_{p}h_{p}^{T}+I^{m\times n}\newline
M_{rt}&amp;=r_{p}t_{p}^{T}+I^{m\times n}
\end{aligned}\tag{9}
$$
Therefore, the mapping matrices are determined by both entities and relations, and this kind of operation makes the two projection vectors interact sufficiently because each element of them can meet every entry comes from another vector. As the authors initialize each mapping matrix with an identity matrix, thus, the $I^{m\times n}$ is added to $M_{rh}$ and $M_{rt}$. With the mapping matrices, the projected vectors are defined as follows:
$$
\begin{aligned}
h_{\bot}&amp;=M_{rh}h\newline
t_{\bot}&amp;=M_{rt}t
\end{aligned}\tag{10}
$$
and the score function is set as
$$
f_{r}(h,t)=-\Vert h_{\bot}+r-t_{\bot}\Vert_{2}^{2}\tag{11}
$$
with the constraints that $\Vert h\Vert_{2}\leq 1$, $\Vert t\Vert_{2}\leq 1$, $\Vert r\Vert_{2}\leq 1$, $\Vert h_{\bot}\Vert_{2}\leq 1$ and $\Vert t_{\bot}\Vert_{2}\leq 1$.</p>

<p>For training process, the authors first denote that $S=\{(h_{i},r_{i},t_{i})\vert y_{i}=1\}$ as the golden triples, and $S&rsquo;=\{(h_{i},r_{i},t_{i})\vert y_{i}=0\}$ as the negative triples, and the negative triples is derived by
$$
S&rsquo;=\{(h_{l},r_{k},t_{k})\vert h_{l}\neq h_{k}\land y_{k}=1\}\cup\{(h_{k},r_{k},t_{l})\vert t_{l}\neq t_{k}\land y_{k}=1\}
$$
while the authors also use two strategies &ldquo;unif&rdquo; and &ldquo;bern&rdquo; described in TransH to replace the head or tail entity. using $\xi$ and $\xi&rsquo;$ to denote a golden triplet and a corresponding negative triplet, respectively. The training objective is
$$
\mathcal{L}=\sum_{\xi\in S}\sum_{\xi&rsquo;\in S&rsquo;}\big[\gamma+f_{r}(\xi&rsquo;)-f_{r}(\xi)\big]_{+}\tag{12}
$$
<strong>Connections with TransE/H/R and CTransR</strong>:</p>

<ul>
<li>TransE is a special case of TransD when the dimension of vectors satisfies $m=n$ and all projection vectors are set zero.</li>
<li>TransH is related to TransD when we set $m=n$. Under the setting, projected vectors of entities can be rewritten as follows:
$$\begin{aligned}
h_{\bot}&amp;=M_{rh}h=h+h_{p}^{T}hr_{p}\newline
t_{\bot}&amp;=M_{rt}t=t+t_{p}^{T}tr_{p}
\end{aligned}
$$
Hence, when $m = n$, the difference between TransD and TransH is that projection vectors are determinded only by relations in TransH, but TransD’s projection vectors are determinded by both entities and relations.</li>
<li>As to TransR/CTransR, TransD is an improvement of it. TransR/CTransR directly defines a mapping matrix for each relation, TransD consturcts two mapping matrices dynamically for each triplet by setting a projection vector for each entity and relation. In addition, TransD has no matrix-vector multiplication operation which can be replaced by vector operations. Without loss of generality, assuming $m\geq n$, the projected vectors can be computed as follows:
$$
\begin{aligned}
h_{\bot}&amp;=M_{rh}h=h_{p}^{T}hr_{p}+\big[h^{T},0^{T}\big]^{T}\newline
t_{\bot}&amp;=M_{rt}t=t_{p}^{T}tr_{p}+\big[t^{T},0^{T}\big]^{T}
\end{aligned}
$$
Therefore, TransD has less calculation than TransR/CTransR, which makes it train faster and can be applied on large-scale knowledge graphs.</li>
</ul>

<h1 id="source-codes">Source Codes</h1>

<p>The Python and C++ codes of the methods above are available in the GitHub page of Natural Language Processing Lab at Tsinghua University (<a href="https://github.com/thunlp" target="_blank">THUNLP</a>).<br />
Python version: <a href="https://github.com/thunlp/TensorFlow-TransX" target="_blank">TensorFlow-TransX</a>.<br />
Converted TensorFlow-TransX for Python 2 and Tensorflow 0.12.0: <a href="https://github.com/IsaacChanghau/AmusingPythonCodes/tree/master/transx" target="_blank">link</a>.<br />
C++ version: <a href="https://github.com/thunlp/Fast-TransX" target="_blank">Fast-TransX</a>, <a href="https://github.com/thunlp/KB2E" target="_blank">KB2E</a>.</p>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/natural-language-processing">natural-language-processing</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/deep-learning">deep-learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/python">python</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/c&#43;&#43;">c&#43;&#43;</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/relation-representation">relation-representation</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/wolf_warrior/">Analyzing Wolf Warrior II Movie Comments using Python</a></li>
        
        <li><a href="/post/beam_search/">Beam Search Algorithms in Sequence to Sequence</a></li>
        
        <li><a href="/post/seq2seq_conversation/">Seq2Seq Learning and Neural Conversational Model</a></li>
        
        <li><a href="/post/neural_responding_machine/">Neural Responding Machine for Short-Text Conversation -- Summary</a></li>
        
        <li><a href="/post/word2vecf/">Word2Vecf -- Dependency-Based Word Embeddings and Lexical Substitute</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017-2018 &middot; 

      <strong>Isaac Changhau (Zhang Hao)</strong>
      

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/java.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/javascript.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/c.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/html.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/xml.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ 
          tex2jax: { 
            inlineMath: [['$','$'], ['\\(','\\)']] 
          } 
        });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-xAWI9i8WMRLdgksuhaMCYMTw9D+MEc2cYVBApWwGRJ0cdcywTjMovOfJnlGt9LlEQj6QzyMzpIZLMYujetPcQg==" crossorigin="anonymous"></script>
    
    
    

  </body>
</html>

